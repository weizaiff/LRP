{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a66f8a83-e87d-4da4-8d01-6f689f7cb24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    20251124:\\n        把一些神经元mask掉，再看看generation\\n    20251125:\\n        峰度的top1% ---> bottom 1%\\n        gap rate top & bottom 1% test\\n    20251202:\\n        open ended generation test\\n        calc LRP_based method\\n    20251204:\\n        进行多组实验\\n    20251210:\\n        llama2 base model generation\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    20251124:\n",
    "        把一些神经元mask掉，再看看generation\n",
    "    20251125:\n",
    "        峰度的top1% ---> bottom 1%\n",
    "        gap rate top & bottom 1% test\n",
    "    20251202:\n",
    "        open ended generation test\n",
    "        calc LRP_based method\n",
    "    20251204:\n",
    "        进行多组实验\n",
    "    20251210:\n",
    "        llama2 base model generation\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d39be3a-bc98-4e3f-948a-bc56e3654a41",
   "metadata": {},
   "source": [
    "## open-ended generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3ec4d08-fa6a-4ae2-a5a4-00aedec2f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import transformers\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "import torch\n",
    "import os\n",
    "# download checkpoint\n",
    "from accelerate import load_checkpoint_and_dispatch\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To prevent long warnings :)\n",
    "\n",
    "#from accelerate import load_checkpoint_and_dispatch\n",
    "\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ccdc958-9ef5-4565-91ce-91c3de3b7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "352d7f56-4ec1-4edd-a2bd-5d2ee384bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0279e997-1e63-4fdf-b360-5816aa1575b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import importlib\n",
    "\n",
    "importlib.reload(util)      # 只能 reload 模块本身\n",
    "from util import calc_ppl, get_test_data, get_open_ended_answer_vllm   # reload 后再重新 import 函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bdba13-ef65-4432-9908-ccf3d995518d",
   "metadata": {},
   "source": [
    "# function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6ed64-7229-4ff7-9c7a-39ad1719b236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d03ac7d9-4627-4ede-b09e-c485db6065f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint):\n",
    "    config = AutoConfig.from_pretrained(checkpoint,trust_remote_code=True)\n",
    "    print('checkpoint:', checkpoint)\n",
    "    \n",
    "    \n",
    "    if True:\n",
    "    \n",
    "        device_map='auto'\n",
    "        model= AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True, torch_dtype= torch.bfloat16,device_map=device_map,weights_only=False ) # for download model weight\n",
    "    \n",
    "    else:\n",
    "        with init_empty_weights():\n",
    "            model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n",
    "    \n",
    "        model = load_checkpoint_and_dispatch(\n",
    "            model, checkpoint, device_map=\"auto\", dtype=torch.bfloat16#, no_split_module_classes=[\"GPTJBlock\"]\n",
    "        )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)#AutoTokenizer.from_pretrained(\"/home/work/lyftri/projects/model_zoo/compass_sea_13b_s4_merge2HF_org_convert_TP_1_PP_2\",  trust_remote_code=True)#AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def set_neuron_zero(model, neuron_list):\n",
    "    '''\n",
    "    将指定的neuron设置为zero\n",
    "\n",
    "    '''\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    for ikey,_ in neuron_list:\n",
    "        w_name = ikey.split('_index_')[0]\n",
    "        neuron_index = int(ikey.split('_index_')[1])\n",
    "\n",
    "        #set zero\n",
    "        state_dict[w_name][neuron_index,:] =0 #1 #0 #-1  #-63.75#0\n",
    "    return model\n",
    "    \n",
    "\n",
    "\n",
    "def generate(model, tokenizer, textlist):\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    #textlist=[\"The garden blooms with vibrant colors, a gentle breeze carries the scent of roses, and the sun casts a warm glow.\"] # maxlen 64\n",
    "    #textlist=['智慧殿堂中，书籍如海，知识无尽，探索永恒。艾拉追寻历史，灵感无限']# maxlen 64\n",
    "    #textlist=['你是']# maxlen 64\n",
    "    \n",
    "    #textlist=['The garden blooms with vibrant colors, a gentle breeze carries the scent of roses, and the sun casts a warm glow.']\n",
    "    #textlist=['China is']\n",
    "    bar = tqdm(total=len(textlist))\n",
    "    \n",
    "    \n",
    "    device='cuda' # cpu cuda\n",
    "    for itext in textlist:\n",
    "        print('*'*20)\n",
    "        input_text = itext\n",
    "        print('input:', input_text)\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "        print('input_ids:', input_ids)\n",
    "    \n",
    "        outputs = model.generate(**input_ids, do_sample=False, num_beams=1, max_new_tokens=128) #max_length \n",
    "        print('outputs:', outputs)\n",
    "        print('output:',tokenizer.batch_decode(outputs, skip_special_tokens=False))\n",
    "        bar.update(1)\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "410cf28a-e1f7-4bbf-8160-ff9d0766eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_neuron_model_vllm_LRP(model, activation_mask_dict):\n",
    "\n",
    "\n",
    "        \n",
    "        if activation_mask_dict:\n",
    "            def factory(mask):\n",
    "                def llama_forward_lrp(self, x):\n",
    "                    '''\n",
    "                        mask: {'up_proj':[...], 'gate_proj':[...], 'down_proj':[...]}\n",
    "                        \n",
    "                    '''\n",
    "                    gate_up, _ = self.gate_up_proj(x)  # b, l, 2i\n",
    "                    i = gate_up.size(-1)\n",
    "                    activation = F.silu(gate_up[:, :, : i // 2])\n",
    "                    if 'gate_proj' in mask:\n",
    "                        activation.index_fill_(2, mask['gate_proj'], 0)\n",
    "\n",
    "                    if 'up_proj' in mask:\n",
    "                        x = activation * gate_up[:, :, i // 2 :].index_fill_(2, mask['up_proj'], 0)\n",
    "                    else:\n",
    "                        x = activation * gate_up[:, :, i // 2 :]\n",
    "                    x, _ = self.down_proj(x)\n",
    "                    if 'down_proj' in mask:\n",
    "                        x.index_fill_(2, mask['down_proj'], 0)\n",
    "                    return x\n",
    "                def llama_forward(self, x):\n",
    "                    gate_up, _ = self.gate_up_proj(x)  # b, l, 2i\n",
    "                    i = gate_up.size(-1)\n",
    "                    activation = F.silu(gate_up[:, :, : i // 2])\n",
    "                    activation.index_fill_(2, mask, 0)\n",
    "                    x = activation * gate_up[:, :, i // 2 :]\n",
    "                    x, _ = self.down_proj(x)\n",
    "                    return x\n",
    "                def llama_forward_split(self, x):\n",
    "                    gate_ = self.gate_proj(x)  # b, l, 2i\n",
    "                    i = gate_.size(-1)\n",
    "                    activation = F.silu(gate_)\n",
    "                    #activation.index_fill_(2, mask, 0)\n",
    "\n",
    "                    # test\n",
    "                    print(activation.shape)\n",
    "                    activation.index_fill_(2, torch.tensor(list(range(activation.shape[1]))), 0 )\n",
    "                    x = activation * self.up_proj(x)\n",
    "                    x = self.down_proj(x)\n",
    "                    return x\n",
    "    \n",
    "                def bloom_forward(self, x: torch.Tensor):\n",
    "                    x, _ = self.dense_h_to_4h(x)\n",
    "                    x = self.gelu_impl(x)\n",
    "                    x.index_fill_(2, mask, 0)\n",
    "                    x, _ = self.dense_4h_to_h(x)\n",
    "                    return x\n",
    "    \n",
    "                if is_llama:\n",
    "                    return llama_forward_lrp\n",
    "                else:\n",
    "                    return bloom_forward\n",
    "    \n",
    "            for i, ilayer_mask_dict in enumerate(activation_mask_dict): \n",
    "                #print('ilayer:',i, layer_mask)\n",
    "                #if is_llama:\n",
    "                    # latest\n",
    "                    #obj = model.llm_engine.model_executor.driver_worker.model_runner.model.model.layers[i].mlp\n",
    "                #else:\n",
    "                    # latest\n",
    "                    #obj = model.llm_engine.model_executor.driver_worker.model_runner.model.transformer.h[i].mlp\n",
    "\n",
    "                if is_llama:\n",
    "                    obj = model.llm_engine.driver_worker.model_runner.model.model.layers[i].mlp\n",
    "                else:\n",
    "                    obj = model.llm_engine.driver_worker.model_runner.model.transformer.h[i].mlp\n",
    "\n",
    "                ilayer_mask_dict_cuda = {}\n",
    "                for ikey ,ival in ilayer_mask_dict.items():\n",
    "                    if len(ival)>0:\n",
    "                        ilayer_mask_dict_cuda[ikey] = torch.LongTensor(ival).to('cuda')\n",
    "                \n",
    "                \n",
    "                obj.forward = MethodType(factory(ilayer_mask_dict_cuda), obj)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbc9cd06-e6e2-421f-a83b-3995c38d508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    convert LRP_based neuron to LAPE format\n",
    "\n",
    "'''\n",
    "import re\n",
    "\n",
    "def convert_LAPE_format(neuron_en):\n",
    "\n",
    "    layer_list = [{'up_proj':[], 'gate_proj':[], 'down_proj':[]} for _ in range(config.num_hidden_layers)]\n",
    "\n",
    "    if isinstance(neuron_en, dict):\n",
    "        new_tmp = []\n",
    "        for ineuron, iscore in neuron_en.items():\n",
    "            new_tmp.append((ineuron, iscore))\n",
    "\n",
    "        neuron_en = new_tmp\n",
    "        \n",
    "    \n",
    "    for ineuron, iscore in neuron_en:\n",
    "    \n",
    "        \n",
    "        m = re.search(r'layers?\\.(\\d+).mlp.(.+).weight_index_([0-9]+)', ineuron)\n",
    "        \n",
    "        layer_index = int(m.group(1))\n",
    "        layer_type = m.group(2)\n",
    "        layer_neuron_index = int(m.group(3))\n",
    "        assert layer_type in ['up_proj', 'gate_proj', 'down_proj']\n",
    "    \n",
    "        layer_list[layer_index][layer_type].append(int(layer_neuron_index))\n",
    "\n",
    "    return layer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3d04f43-78e8-4fc1-a551-f879f28958b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-10 14:40:52 llm_engine.py:70] Initializing an LLM engine with config: model='/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-hf', tokenizer='/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=True, seed=0)\n",
      "INFO 12-10 14:40:57 llm_engine.py:275] # GPU blocks: 1911, # CPU blocks: 512\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from types import MethodType\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "#model_path ='/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "model_path ='/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "is_llama = bool(model_path.find('llama') >= 0)\n",
    "#model, tokenizer = load_model(model_path)\n",
    "\n",
    "# 初始化 vLLM\n",
    "llm =LLM(model=model_path, tensor_parallel_size=torch.cuda.device_count(), enforce_eager=True) #LLM(model=model_path, tensor_parallel_size=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d598d0e-9eda-4d4e-979a-3bc48c639e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c64992-d1cf-4c6d-9506-22ccc6dc39e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e69de76-f7b8-482e-a726-ee4a34055e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f77c1f94-e075-4300-899d-bdb9bc496731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# llama chat\n",
    "#BASE = \"/root/autodl-fs/LRP_kur_res/20251204_5000samples_cal_llama2_7b_chat\"\n",
    "# llama base\n",
    "BASE = \"/root/autodl-fs/LRP_kur_res/20251210_5000samples_cal_llama2_7b_base\"\n",
    "\n",
    "def build_model_list(llm, base=BASE):\n",
    "    model_list = [(llm, \"org_model\")]\n",
    "\n",
    "    for fname in sorted(os.listdir(base)):\n",
    "        if not fname.endswith(\".pt\"):\n",
    "            continue\n",
    "        if fname.startswith(\"all_mlp\"):\n",
    "            continue\n",
    "\n",
    "        fpath = f\"{base}/{fname}\"\n",
    "        model_name = fname.replace(\".pt\", \"\")\n",
    "        model_list.append((fpath, model_name))\n",
    "    \n",
    "    return model_list\n",
    "\n",
    "model_list = build_model_list(llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a17a821-6d8c-4ef0-9f11-2b98170a0142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecd638b3-5aca-46c6-9581-092fcb03f28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([('th_1_selected_LRP_kur_res_zscore', 0.19038895836479783),\n",
    " ('th_3_selected_LRP_kur_res_zscore', 0.13874257247146074),\n",
    " ('top_1perc_LRP_kur_res', 0.1351612402431789),\n",
    " ('th_0.5_selected_LRP_kur_res_zscore_margin_selected', 0.12974181253685116),\n",
    " ('gap_rate_top_0_0_1perc_LRP_kur_res', 0.12962774079757058),\n",
    " ('top_0_5perc_LRP_kur_res', 0.12617015898537512),\n",
    " ('gap_rate_bottom_0_1_perc_LRP_kur_res_zscore', 0.12280716273186916),\n",
    " ('soft_preference_th_0.99_selected_LRP_kur_res_Not_zscore_margin_selected',\n",
    "  0.11640363654684978)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33e4384a-b359-48cc-a144-688cdd65c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_need_exp(model_list):\n",
    "    target_list= [('th_1_selected_LRP_kur_res_zscore', np.float64(0.19038895836479783)),\n",
    " ('th_3_selected_LRP_kur_res_zscore', np.float64(0.13874257247146074)),\n",
    " ('top_1perc_LRP_kur_res', np.float64(0.1351612402431789)),\n",
    " ('th_0.5_selected_LRP_kur_res_zscore_margin_selected',\n",
    "  np.float64(0.12974181253685116)),\n",
    " ('gap_rate_top_0_0_1perc_LRP_kur_res', np.float64(0.12962774079757058)),\n",
    " ('top_0_5perc_LRP_kur_res', np.float64(0.12617015898537512)),\n",
    " ('gap_rate_bottom_0_1_perc_LRP_kur_res_zscore',\n",
    "  np.float64(0.12280716273186916)),\n",
    " ('soft_preference_th_0.99_selected_LRP_kur_res_Not_zscore_margin_selected',\n",
    "  np.float64(0.11640363654684978)) ]\n",
    "\n",
    "    target_list=[('th_1_selected_LRP_kur_res_', np.float64(0.19038895836479783))]\n",
    "\n",
    "\n",
    "    new_model_List = []\n",
    "    for ipath, iname in model_list:\n",
    "        for name, _ in target_list:\n",
    "            if name in iname or iname=='org_model':\n",
    "                new_model_List.append((ipath, iname))\n",
    "    return new_model_List\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "026fa2e9-510a-433a-ab62-71648321953d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_list = (get_need_exp(model_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36c6cc59-38dd-4e11-9df9-721307915ae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<vllm.entrypoints.llm.LLM at 0x7fb66f045270>, 'org_model'),\n",
       " ('/root/autodl-fs/LRP_kur_res/20251210_5000samples_cal_llama2_7b_base/th_1_selected_LRP_kur_res_en_zscore.pt',\n",
       "  'th_1_selected_LRP_kur_res_en_zscore'),\n",
       " ('/root/autodl-fs/LRP_kur_res/20251210_5000samples_cal_llama2_7b_base/th_1_selected_LRP_kur_res_en_zscore_margin_selected.pt',\n",
       "  'th_1_selected_LRP_kur_res_en_zscore_margin_selected'),\n",
       " ('/root/autodl-fs/LRP_kur_res/20251210_5000samples_cal_llama2_7b_base/th_1_selected_LRP_kur_res_vi_zscore.pt',\n",
       "  'th_1_selected_LRP_kur_res_vi_zscore'),\n",
       " ('/root/autodl-fs/LRP_kur_res/20251210_5000samples_cal_llama2_7b_base/th_1_selected_LRP_kur_res_vi_zscore_margin_selected.pt',\n",
       "  'th_1_selected_LRP_kur_res_vi_zscore_margin_selected'),\n",
       " ('/root/autodl-fs/LRP_kur_res/20251210_5000samples_cal_llama2_7b_base/th_1_selected_LRP_kur_res_zh_zscore.pt',\n",
       "  'th_1_selected_LRP_kur_res_zh_zscore'),\n",
       " ('/root/autodl-fs/LRP_kur_res/20251210_5000samples_cal_llama2_7b_base/th_1_selected_LRP_kur_res_zh_zscore_margin_selected.pt',\n",
       "  'th_1_selected_LRP_kur_res_zh_zscore_margin_selected')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36448025-997d-48cb-8794-02d7e82dc448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama2 7b base\n",
    "#save_dir = \"/root/autodl-fs/LRP/open_ended_data_generation/20251204_all_exp\"\n",
    "# ======== Build model list (auto) ========\n",
    "#BASE = \"/root/autodl-fs/LRP_kur_res/20251204_5000samples_cal_llama2_7b_chat\"\n",
    "\n",
    "# llama2 7b chat\n",
    "save_dir = \"/root/autodl-fs/LRP/open_ended_data_generation/20251210_llama2_7b_base\"\n",
    "BASE = \"/root/autodl-fs/LRP_kur_res/20251210_5000samples_cal_llama2_7b_base\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9493996-2103-4b82-ab48-a828a6cde1bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total models: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "Model: org_model\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/210 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts:   0%|          | 1/210 [00:03<11:39,  3.35s/it]\u001b[A\n",
      "Processed prompts:   1%|          | 2/210 [00:03<05:29,  1.58s/it]\u001b[A\n",
      "Processed prompts:   1%|▏         | 3/210 [00:04<03:31,  1.02s/it]\u001b[A\n",
      "Processed prompts:   2%|▏         | 5/210 [00:04<01:38,  2.07it/s]\u001b[A\n",
      "Processed prompts:   3%|▎         | 6/210 [00:04<01:26,  2.35it/s]\u001b[A\n",
      "Processed prompts:   4%|▍         | 8/210 [00:04<00:53,  3.76it/s]\u001b[A\n",
      "Processed prompts:   5%|▍         | 10/210 [00:04<00:43,  4.60it/s]\u001b[A\n",
      "Processed prompts:   5%|▌         | 11/210 [00:05<00:40,  4.93it/s]\u001b[A\n",
      "Processed prompts:   6%|▌         | 12/210 [00:05<00:47,  4.20it/s]\u001b[A\n",
      "Processed prompts:   6%|▌         | 13/210 [00:05<00:56,  3.47it/s]\u001b[A\n",
      "Processed prompts:   7%|▋         | 14/210 [00:06<01:27,  2.24it/s]\u001b[A\n",
      "Processed prompts:   7%|▋         | 15/210 [00:08<02:28,  1.31it/s]\u001b[A\n",
      "Processed prompts:   8%|▊         | 16/210 [00:09<02:33,  1.26it/s]\u001b[A\n",
      "Processed prompts:   8%|▊         | 17/210 [00:09<02:02,  1.58it/s]\u001b[A\n",
      "Processed prompts:   9%|▉         | 19/210 [00:09<01:20,  2.39it/s]\u001b[A\n",
      "Processed prompts:  10%|█         | 21/210 [00:10<01:29,  2.11it/s]\u001b[A\n",
      "Processed prompts:  10%|█         | 22/210 [00:12<02:26,  1.29it/s]\u001b[A\n",
      "Processed prompts:  11%|█         | 23/210 [00:13<02:10,  1.43it/s]\u001b[A\n",
      "Processed prompts:  11%|█▏        | 24/210 [00:13<01:44,  1.79it/s]\u001b[A\n",
      "Processed prompts:  12%|█▏        | 25/210 [00:13<01:30,  2.04it/s]\u001b[A\n",
      "Processed prompts:  12%|█▏        | 26/210 [00:14<01:49,  1.68it/s]\u001b[A\n",
      "Processed prompts:  13%|█▎        | 27/210 [00:15<01:51,  1.63it/s]\u001b[A\n",
      "Processed prompts:  13%|█▎        | 28/210 [00:15<01:30,  2.00it/s]\u001b[A\n",
      "Processed prompts:  14%|█▍        | 29/210 [00:15<01:19,  2.28it/s]\u001b[A\n",
      "Processed prompts:  14%|█▍        | 30/210 [00:15<01:03,  2.84it/s]\u001b[A\n",
      "Processed prompts:  15%|█▍        | 31/210 [00:16<01:26,  2.07it/s]\u001b[A\n",
      "Processed prompts:  15%|█▌        | 32/210 [00:16<01:07,  2.64it/s]\u001b[A\n",
      "Processed prompts:  16%|█▌        | 33/210 [00:17<01:35,  1.86it/s]\u001b[A\n",
      "Processed prompts:  16%|█▌        | 34/210 [00:17<01:23,  2.11it/s]\u001b[A\n",
      "Processed prompts:  17%|█▋        | 36/210 [00:18<00:54,  3.22it/s]\u001b[A\n",
      "Processed prompts:  18%|█▊        | 37/210 [00:18<00:45,  3.76it/s]\u001b[A\n",
      "Processed prompts:  18%|█▊        | 38/210 [00:18<00:44,  3.87it/s]\u001b[A\n",
      "Processed prompts:  19%|█▊        | 39/210 [00:18<00:44,  3.87it/s]\u001b[A\n",
      "Processed prompts:  19%|█▉        | 40/210 [00:19<00:41,  4.15it/s]\u001b[A\n",
      "Processed prompts:  20%|█▉        | 41/210 [00:19<00:53,  3.15it/s]\u001b[A\n",
      "Processed prompts:  20%|██        | 42/210 [00:19<00:55,  3.02it/s]\u001b[A\n",
      "Processed prompts:  20%|██        | 43/210 [00:20<01:09,  2.39it/s]\u001b[A\n",
      "Processed prompts:  21%|██▏       | 45/210 [00:21<01:24,  1.96it/s]\u001b[A\n",
      "Processed prompts:  22%|██▏       | 46/210 [00:24<02:55,  1.07s/it]\u001b[A\n",
      "Processed prompts:  22%|██▏       | 47/210 [00:25<02:55,  1.08s/it]\u001b[A\n",
      "Processed prompts:  23%|██▎       | 48/210 [00:27<03:44,  1.39s/it]\u001b[A\n",
      "Processed prompts:  23%|██▎       | 49/210 [00:31<05:32,  2.06s/it]\u001b[A\n",
      "Processed prompts:  24%|██▍       | 50/210 [00:34<06:00,  2.25s/it]\u001b[A\n",
      "Processed prompts:  24%|██▍       | 51/210 [00:35<04:58,  1.88s/it]\u001b[A\n",
      "Processed prompts:  25%|██▍       | 52/210 [00:36<04:07,  1.57s/it]\u001b[A\n",
      "Processed prompts:  25%|██▌       | 53/210 [00:36<03:06,  1.19s/it]\u001b[A\n",
      "Processed prompts:  26%|██▌       | 54/210 [00:37<02:51,  1.10s/it]\u001b[A\n",
      "Processed prompts:  26%|██▌       | 55/210 [00:39<03:22,  1.30s/it]\u001b[A\n",
      "Processed prompts:  27%|██▋       | 56/210 [00:39<02:28,  1.03it/s]\u001b[A\n",
      "Processed prompts:  27%|██▋       | 57/210 [00:40<02:49,  1.11s/it]\u001b[A\n",
      "Processed prompts:  28%|██▊       | 58/210 [00:41<02:15,  1.12it/s]\u001b[A\n",
      "Processed prompts:  28%|██▊       | 59/210 [00:48<06:46,  2.69s/it]\u001b[A\n",
      "Processed prompts:  29%|██▊       | 60/210 [00:51<07:25,  2.97s/it]\u001b[A\n",
      "Processed prompts:  29%|██▉       | 61/210 [00:57<09:08,  3.68s/it]\u001b[A\n",
      "Processed prompts:  30%|██▉       | 62/210 [01:00<08:53,  3.61s/it]\u001b[A\n",
      "Processed prompts:  30%|███       | 63/210 [01:33<30:49, 12.58s/it]\u001b[A\n",
      "Processed prompts:  30%|███       | 64/210 [01:56<38:08, 15.67s/it]\u001b[A\n",
      "Processed prompts:  37%|███▋      | 78/210 [02:01<05:31,  2.51s/it]\u001b[A\n",
      "Processed prompts:  38%|███▊      | 79/210 [02:03<05:23,  2.47s/it]\u001b[A\n",
      "Processed prompts:  38%|███▊      | 80/210 [02:05<05:12,  2.40s/it]\u001b[A\n",
      "Processed prompts:  39%|███▊      | 81/210 [02:10<05:59,  2.78s/it]\u001b[A\n",
      "Processed prompts:  39%|███▉      | 82/210 [02:16<07:09,  3.35s/it]\u001b[A\n",
      "Processed prompts:  40%|███▉      | 83/210 [02:22<07:55,  3.75s/it]\u001b[A\n",
      "Processed prompts:  40%|████      | 84/210 [02:31<10:13,  4.87s/it]\u001b[A\n",
      "Processed prompts:  40%|████      | 85/210 [02:35<09:43,  4.67s/it]\u001b[A\n",
      "Processed prompts:  41%|████      | 86/210 [02:36<07:40,  3.71s/it]\u001b[A\n",
      "Processed prompts:  41%|████▏     | 87/210 [02:39<07:24,  3.61s/it]\u001b[A\n",
      "Processed prompts:  42%|████▏     | 88/210 [02:43<07:16,  3.58s/it]\u001b[A\n",
      "Processed prompts:  42%|████▏     | 89/210 [02:49<08:34,  4.25s/it]\u001b[A\n",
      "Processed prompts:  43%|████▎     | 90/210 [02:52<07:41,  3.85s/it]\u001b[A\n",
      "Processed prompts:  43%|████▎     | 91/210 [02:53<06:05,  3.07s/it]\u001b[A\n",
      "Processed prompts:  44%|████▍     | 92/210 [02:54<05:09,  2.62s/it]\u001b[A\n",
      "Processed prompts:  44%|████▍     | 93/210 [02:57<05:04,  2.60s/it]\u001b[A\n",
      "Processed prompts:  45%|████▍     | 94/210 [02:59<04:49,  2.50s/it]\u001b[A\n",
      "Processed prompts:  45%|████▌     | 95/210 [03:01<04:32,  2.37s/it]\u001b[A\n",
      "Processed prompts:  46%|████▌     | 96/210 [03:04<04:43,  2.48s/it]\u001b[A\n",
      "Processed prompts:  46%|████▌     | 97/210 [03:10<06:28,  3.43s/it]\u001b[A\n",
      "Processed prompts:  47%|████▋     | 98/210 [03:11<05:19,  2.85s/it]\u001b[A\n",
      "Processed prompts:  47%|████▋     | 99/210 [03:11<03:49,  2.07s/it]\u001b[A\n",
      "Processed prompts:  48%|████▊     | 100/210 [03:13<03:42,  2.02s/it]\u001b[A\n",
      "Processed prompts:  48%|████▊     | 101/210 [03:15<03:47,  2.09s/it]\u001b[A\n",
      "Processed prompts:  49%|████▊     | 102/210 [03:19<04:28,  2.49s/it]\u001b[A\n",
      "Processed prompts:  49%|████▉     | 103/210 [03:20<03:48,  2.13s/it]\u001b[A\n",
      "Processed prompts:  50%|████▉     | 104/210 [03:24<04:29,  2.54s/it]\u001b[A\n",
      "Processed prompts:  50%|█████     | 105/210 [03:25<03:35,  2.05s/it]\u001b[A\n",
      "Processed prompts:  50%|█████     | 106/210 [03:26<03:14,  1.87s/it]\u001b[A\n",
      "Processed prompts:  51%|█████     | 107/210 [03:27<02:44,  1.60s/it]\u001b[A\n",
      "Processed prompts:  51%|█████▏    | 108/210 [03:28<02:15,  1.32s/it]\u001b[A\n",
      "Processed prompts:  52%|█████▏    | 109/210 [03:29<02:18,  1.37s/it]\u001b[A\n",
      "Processed prompts:  52%|█████▏    | 110/210 [03:36<04:53,  2.93s/it]\u001b[A\n",
      "Processed prompts:  53%|█████▎    | 111/210 [03:37<04:01,  2.44s/it]\u001b[A\n",
      "Processed prompts:  53%|█████▎    | 112/210 [03:46<07:04,  4.33s/it]\u001b[A\n",
      "Processed prompts:  54%|█████▍    | 113/210 [03:53<08:35,  5.32s/it]\u001b[A\n",
      "Processed prompts:  54%|█████▍    | 114/210 [03:58<08:24,  5.25s/it]\u001b[A\n",
      "Processed prompts:  55%|█████▍    | 115/210 [04:08<10:17,  6.50s/it]\u001b[A\n",
      "Processed prompts:  55%|█████▌    | 116/210 [04:15<10:29,  6.70s/it]\u001b[A\n",
      "Processed prompts:  56%|█████▌    | 117/210 [04:22<10:16,  6.63s/it]\u001b[A\n",
      "Processed prompts:  56%|█████▌    | 118/210 [04:22<07:26,  4.85s/it]\u001b[A\n",
      "Processed prompts:  57%|█████▋    | 119/210 [04:24<05:57,  3.93s/it]\u001b[A\n",
      "Processed prompts:  57%|█████▋    | 120/210 [04:25<04:39,  3.11s/it]\u001b[A\n",
      "Processed prompts:  58%|█████▊    | 121/210 [04:26<03:43,  2.52s/it]\u001b[A\n",
      "Processed prompts:  58%|█████▊    | 122/210 [04:27<02:44,  1.87s/it]\u001b[A\n",
      "Processed prompts:  59%|█████▊    | 123/210 [04:28<02:30,  1.73s/it]\u001b[A\n",
      "Processed prompts:  59%|█████▉    | 124/210 [04:29<02:07,  1.48s/it]\u001b[A\n",
      "Processed prompts:  60%|█████▉    | 125/210 [04:29<01:41,  1.19s/it]\u001b[A\n",
      "Processed prompts:  60%|██████    | 126/210 [04:31<01:46,  1.27s/it]\u001b[A\n",
      "Processed prompts:  60%|██████    | 127/210 [04:31<01:18,  1.05it/s]\u001b[A\n",
      "Processed prompts:  61%|██████    | 128/210 [04:33<01:38,  1.20s/it]\u001b[A\n",
      "Processed prompts:  61%|██████▏   | 129/210 [04:33<01:17,  1.04it/s]\u001b[A\n",
      "Processed prompts:  62%|██████▏   | 130/210 [04:34<01:13,  1.09it/s]\u001b[A\n",
      "Processed prompts:  62%|██████▏   | 131/210 [04:34<00:53,  1.47it/s]\u001b[A\n",
      "Processed prompts:  63%|██████▎   | 132/210 [04:35<00:51,  1.53it/s]\u001b[A\n",
      "Processed prompts:  63%|██████▎   | 133/210 [04:35<00:47,  1.61it/s]\u001b[A\n",
      "Processed prompts:  64%|██████▍   | 134/210 [04:37<01:02,  1.21it/s]\u001b[A\n",
      "Processed prompts:  64%|██████▍   | 135/210 [04:38<01:14,  1.00it/s]\u001b[A\n",
      "Processed prompts:  65%|██████▍   | 136/210 [04:39<01:00,  1.23it/s]\u001b[A\n",
      "Processed prompts:  65%|██████▌   | 137/210 [04:40<01:14,  1.01s/it]\u001b[A\n",
      "Processed prompts:  66%|██████▌   | 138/210 [04:42<01:26,  1.20s/it]\u001b[A\n",
      "Processed prompts:  66%|██████▌   | 139/210 [04:52<04:43,  3.99s/it]\u001b[A\n",
      "Processed prompts:  67%|██████▋   | 140/210 [04:55<04:06,  3.52s/it]\u001b[A\n",
      "Processed prompts:  68%|██████▊   | 142/210 [04:56<02:31,  2.22s/it]\u001b[A\n",
      "Processed prompts:  68%|██████▊   | 143/210 [04:59<02:39,  2.38s/it]\u001b[A\n",
      "Processed prompts:  69%|██████▊   | 144/210 [05:05<03:45,  3.42s/it]\u001b[A\n",
      "Processed prompts:  69%|██████▉   | 145/210 [05:06<02:57,  2.72s/it]\u001b[A\n",
      "Processed prompts:  70%|██████▉   | 146/210 [05:06<02:13,  2.08s/it]\u001b[A\n",
      "Processed prompts:  70%|███████   | 147/210 [05:16<04:24,  4.21s/it]\u001b[A\n",
      "Processed prompts:  70%|███████   | 148/210 [05:18<03:33,  3.44s/it]\u001b[A\n",
      "Processed prompts:  71%|███████   | 149/210 [05:23<04:06,  4.04s/it]\u001b[A\n",
      "Processed prompts:  71%|███████▏  | 150/210 [05:46<09:43,  9.72s/it]\u001b[A\n",
      "Processed prompts:  72%|███████▏  | 151/210 [05:55<09:07,  9.28s/it]\u001b[A\n",
      "Processed prompts:  72%|███████▏  | 152/210 [06:00<07:51,  8.12s/it]\u001b[A\n",
      "Processed prompts:  73%|███████▎  | 153/210 [06:05<06:50,  7.21s/it]\u001b[A\n",
      "Processed prompts:  73%|███████▎  | 154/210 [06:09<05:48,  6.22s/it]\u001b[A\n",
      "Processed prompts:  74%|███████▍  | 155/210 [06:11<04:26,  4.85s/it]\u001b[A\n",
      "Processed prompts:  74%|███████▍  | 156/210 [06:11<03:09,  3.50s/it]\u001b[A\n",
      "Processed prompts:  75%|███████▍  | 157/210 [06:11<02:18,  2.61s/it]\u001b[A\n",
      "Processed prompts:  75%|███████▌  | 158/210 [06:12<01:45,  2.02s/it]\u001b[A\n",
      "Processed prompts:  76%|███████▌  | 159/210 [06:13<01:23,  1.64s/it]\u001b[A\n",
      "Processed prompts:  76%|███████▌  | 160/210 [06:14<01:10,  1.42s/it]\u001b[A\n",
      "Processed prompts:  77%|███████▋  | 161/210 [06:14<00:59,  1.22s/it]\u001b[A\n",
      "Processed prompts:  77%|███████▋  | 162/210 [06:15<00:47,  1.01it/s]\u001b[A\n",
      "Processed prompts:  78%|███████▊  | 163/210 [06:15<00:38,  1.22it/s]\u001b[A\n",
      "Processed prompts:  78%|███████▊  | 164/210 [06:16<00:39,  1.16it/s]\u001b[A\n",
      "Processed prompts:  79%|███████▊  | 165/210 [06:18<00:44,  1.02it/s]\u001b[A\n",
      "Processed prompts:  80%|███████▉  | 167/210 [06:18<00:26,  1.60it/s]\u001b[A\n",
      "Processed prompts:  80%|████████  | 168/210 [06:18<00:22,  1.89it/s]\u001b[A\n",
      "Processed prompts:  80%|████████  | 169/210 [06:19<00:20,  1.99it/s]\u001b[A\n",
      "Processed prompts:  81%|████████  | 170/210 [06:20<00:27,  1.45it/s]\u001b[A\n",
      "Processed prompts:  81%|████████▏ | 171/210 [06:21<00:36,  1.07it/s]\u001b[A\n",
      "Processed prompts:  82%|████████▏ | 172/210 [06:23<00:40,  1.07s/it]\u001b[A\n",
      "Processed prompts:  82%|████████▏ | 173/210 [06:23<00:30,  1.21it/s]\u001b[A\n",
      "Processed prompts:  83%|████████▎ | 174/210 [06:24<00:27,  1.29it/s]\u001b[A\n",
      "Processed prompts:  83%|████████▎ | 175/210 [06:25<00:33,  1.04it/s]\u001b[A\n",
      "Processed prompts:  84%|████████▍ | 176/210 [06:28<00:47,  1.39s/it]\u001b[A\n",
      "Processed prompts:  84%|████████▍ | 177/210 [06:30<01:00,  1.84s/it]\u001b[A\n",
      "Processed prompts:  85%|████████▍ | 178/210 [06:31<00:46,  1.46s/it]\u001b[A\n",
      "Processed prompts:  85%|████████▌ | 179/210 [06:32<00:38,  1.24s/it]\u001b[A\n",
      "Processed prompts:  86%|████████▌ | 180/210 [06:50<03:07,  6.25s/it]\u001b[A\n",
      "Processed prompts:  86%|████████▌ | 181/210 [06:52<02:30,  5.18s/it]\u001b[A\n",
      "Processed prompts:  87%|████████▋ | 182/210 [06:55<02:07,  4.54s/it]\u001b[A\n",
      "Processed prompts:  87%|████████▋ | 183/210 [07:00<01:59,  4.42s/it]\u001b[A\n",
      "Processed prompts:  88%|████████▊ | 184/210 [07:03<01:47,  4.14s/it]\u001b[A\n",
      "Processed prompts:  88%|████████▊ | 185/210 [07:07<01:41,  4.06s/it]\u001b[A\n",
      "Processed prompts:  89%|████████▊ | 186/210 [07:12<01:41,  4.23s/it]\u001b[A\n",
      "Processed prompts:  89%|████████▉ | 187/210 [07:13<01:17,  3.38s/it]\u001b[A\n",
      "Processed prompts:  90%|████████▉ | 188/210 [07:20<01:40,  4.58s/it]\u001b[A\n",
      "Processed prompts:  90%|█████████ | 189/210 [07:24<01:31,  4.37s/it]\u001b[A\n",
      "Processed prompts:  90%|█████████ | 190/210 [07:26<01:14,  3.74s/it]\u001b[A\n",
      "Processed prompts:  91%|█████████ | 191/210 [07:30<01:08,  3.62s/it]\u001b[A\n",
      "Processed prompts:  91%|█████████▏| 192/210 [07:36<01:18,  4.34s/it]\u001b[A\n",
      "Processed prompts:  92%|█████████▏| 193/210 [07:40<01:12,  4.29s/it]\u001b[A\n",
      "Processed prompts:  92%|█████████▏| 194/210 [07:45<01:13,  4.59s/it]\u001b[A\n",
      "Processed prompts:  93%|█████████▎| 195/210 [07:47<00:54,  3.63s/it]\u001b[A\n",
      "Processed prompts:  93%|█████████▎| 196/210 [07:49<00:46,  3.36s/it]\u001b[A\n",
      "Processed prompts:  94%|█████████▍| 197/210 [07:50<00:33,  2.55s/it]\u001b[A\n",
      "Processed prompts:  94%|█████████▍| 198/210 [07:51<00:23,  1.92s/it]\u001b[A\n",
      "Processed prompts:  95%|█████████▍| 199/210 [07:51<00:15,  1.44s/it]\u001b[A\n",
      "Processed prompts:  95%|█████████▌| 200/210 [07:52<00:12,  1.25s/it]\u001b[A\n",
      "Processed prompts:  96%|█████████▌| 201/210 [07:52<00:09,  1.06s/it]\u001b[A\n",
      "Processed prompts:  96%|█████████▌| 202/210 [07:53<00:06,  1.18it/s]\u001b[A\n",
      "Processed prompts:  97%|█████████▋| 203/210 [07:53<00:05,  1.28it/s]\u001b[A\n",
      "Processed prompts:  97%|█████████▋| 204/210 [07:56<00:08,  1.35s/it]\u001b[A\n",
      "Processed prompts:  98%|█████████▊| 205/210 [08:00<00:10,  2.15s/it]\u001b[A\n",
      "Processed prompts:  98%|█████████▊| 206/210 [08:07<00:14,  3.54s/it]\u001b[A\n",
      "Processed prompts:  99%|█████████▊| 207/210 [08:11<00:11,  3.86s/it]\u001b[A\n",
      "Processed prompts:  99%|█████████▉| 208/210 [08:15<00:07,  3.91s/it]\u001b[A\n",
      "Processed prompts: 100%|█████████▉| 209/210 [08:18<00:03,  3.66s/it]\u001b[A\n",
      "Processed prompts: 100%|██████████| 210/210 [08:20<00:00,  2.39s/it]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2443c78f07284b9b92b02d8350416be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  14%|█▍        | 1/7 [08:21<50:06, 501.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] /root/autodl-fs/LRP/open_ended_data_generation/20251210_llama2_7b_base/20251210_LRP_generation_org_model_open_ended.json\n",
      "****************************************\n",
      "Model: th_1_selected_LRP_kur_res_en_zscore\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/210 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts:   0%|          | 1/210 [00:04<15:21,  4.41s/it]\u001b[A\n",
      "Processed prompts:   1%|▏         | 3/210 [00:04<04:07,  1.20s/it]\u001b[A\n",
      "Processed prompts:   3%|▎         | 7/210 [00:04<01:23,  2.44it/s]\u001b[A\n",
      "Processed prompts:   4%|▍         | 9/210 [00:04<01:04,  3.10it/s]\u001b[A\n",
      "Processed prompts:   5%|▌         | 11/210 [00:05<00:52,  3.76it/s]\u001b[A\n",
      "Processed prompts:   6%|▌         | 13/210 [00:05<00:53,  3.65it/s]\u001b[A\n",
      "Processed prompts:   7%|▋         | 14/210 [00:06<01:18,  2.51it/s]\u001b[A\n",
      "Processed prompts:   8%|▊         | 16/210 [00:08<01:41,  1.91it/s]\u001b[A\n",
      "Processed prompts:   8%|▊         | 17/210 [00:08<01:34,  2.03it/s]\u001b[A\n",
      "Processed prompts:   9%|▊         | 18/210 [00:08<01:23,  2.29it/s]\u001b[A\n",
      "Processed prompts:  10%|▉         | 20/210 [00:10<01:39,  1.91it/s]\u001b[A\n",
      "Processed prompts:  10%|█         | 21/210 [00:10<01:27,  2.17it/s]\u001b[A\n",
      "Processed prompts:  10%|█         | 22/210 [00:11<02:07,  1.47it/s]\u001b[A\n",
      "Processed prompts:  11%|█▏        | 24/210 [00:13<02:01,  1.52it/s]\u001b[A\n",
      "Processed prompts:  12%|█▏        | 25/210 [00:15<02:59,  1.03it/s]\u001b[A\n",
      "Processed prompts:  13%|█▎        | 27/210 [00:15<02:02,  1.49it/s]\u001b[A\n",
      "Processed prompts:  13%|█▎        | 28/210 [00:15<01:49,  1.66it/s]\u001b[A\n",
      "Processed prompts:  14%|█▍        | 29/210 [00:17<02:24,  1.25it/s]\u001b[A\n",
      "Processed prompts:  14%|█▍        | 30/210 [00:19<03:04,  1.02s/it]\u001b[A\n",
      "Processed prompts:  15%|█▍        | 31/210 [00:21<04:20,  1.46s/it]\u001b[A\n",
      "Processed prompts:  15%|█▌        | 32/210 [00:25<06:02,  2.03s/it]\u001b[A\n",
      "Processed prompts:  16%|█▌        | 33/210 [00:25<04:39,  1.58s/it]\u001b[A\n",
      "Processed prompts:  16%|█▌        | 34/210 [00:26<03:43,  1.27s/it]\u001b[A\n",
      "Processed prompts:  17%|█▋        | 35/210 [00:28<04:36,  1.58s/it]\u001b[A\n",
      "Processed prompts:  17%|█▋        | 36/210 [00:31<05:35,  1.93s/it]\u001b[A\n",
      "Processed prompts:  18%|█▊        | 37/210 [00:31<04:21,  1.51s/it]\u001b[A\n",
      "Processed prompts:  18%|█▊        | 38/210 [00:49<17:51,  6.23s/it]\u001b[A\n",
      "Processed prompts:  19%|█▊        | 39/210 [01:56<1:09:23, 24.35s/it]\u001b[A\n",
      "Processed prompts:  25%|██▌       | 53/210 [02:02<10:07,  3.87s/it]  \u001b[A\n",
      "Processed prompts:  26%|██▌       | 54/210 [02:09<10:46,  4.14s/it]\u001b[A\n",
      "Processed prompts:  26%|██▌       | 55/210 [02:16<11:18,  4.38s/it]\u001b[A\n",
      "Processed prompts:  27%|██▋       | 56/210 [02:21<11:41,  4.55s/it]\u001b[A\n",
      "Processed prompts:  27%|██▋       | 57/210 [02:26<11:44,  4.61s/it]\u001b[A\n",
      "Processed prompts:  28%|██▊       | 58/210 [02:31<11:38,  4.59s/it]\u001b[A\n",
      "Processed prompts:  28%|██▊       | 59/210 [02:35<11:24,  4.53s/it]\u001b[A\n",
      "Processed prompts:  29%|██▊       | 60/210 [02:39<10:51,  4.34s/it]\u001b[A\n",
      "Processed prompts:  29%|██▉       | 61/210 [02:42<10:18,  4.15s/it]\u001b[A\n",
      "Processed prompts:  30%|██▉       | 62/210 [02:45<09:33,  3.88s/it]\u001b[A\n",
      "Processed prompts:  30%|███       | 63/210 [02:48<08:59,  3.67s/it]\u001b[A\n",
      "Processed prompts:  30%|███       | 64/210 [02:51<08:18,  3.42s/it]\u001b[A\n",
      "Processed prompts:  31%|███       | 65/210 [02:54<07:39,  3.17s/it]\u001b[A\n",
      "Processed prompts:  31%|███▏      | 66/210 [02:56<07:13,  3.01s/it]\u001b[A\n",
      "Processed prompts:  32%|███▏      | 67/210 [02:59<06:34,  2.76s/it]\u001b[A\n",
      "Processed prompts:  32%|███▏      | 68/210 [03:01<06:04,  2.57s/it]\u001b[A\n",
      "Processed prompts:  33%|███▎      | 69/210 [03:03<05:45,  2.45s/it]\u001b[A\n",
      "Processed prompts:  33%|███▎      | 70/210 [03:05<05:23,  2.31s/it]\u001b[A\n",
      "Processed prompts:  34%|███▍      | 71/210 [03:07<05:02,  2.18s/it]\u001b[A\n",
      "Processed prompts:  34%|███▍      | 72/210 [03:09<04:46,  2.07s/it]\u001b[A\n",
      "Processed prompts:  35%|███▍      | 73/210 [03:10<04:30,  1.98s/it]\u001b[A\n",
      "Processed prompts:  35%|███▌      | 74/210 [03:12<04:10,  1.84s/it]\u001b[A\n",
      "Processed prompts:  36%|███▌      | 75/210 [03:16<05:38,  2.50s/it]\u001b[A\n",
      "Processed prompts:  36%|███▌      | 76/210 [03:29<12:56,  5.79s/it]\u001b[A\n",
      "Processed prompts:  37%|███▋      | 77/210 [03:45<19:20,  8.73s/it]\u001b[A\n",
      "Processed prompts:  37%|███▋      | 78/210 [03:53<18:36,  8.46s/it]\u001b[A\n",
      "Processed prompts:  38%|███▊      | 79/210 [03:58<16:04,  7.36s/it]\u001b[A\n",
      "Processed prompts:  38%|███▊      | 80/210 [04:01<13:38,  6.29s/it]\u001b[A\n",
      "Processed prompts:  39%|███▊      | 81/210 [04:04<11:19,  5.26s/it]\u001b[A\n",
      "Processed prompts:  39%|███▉      | 82/210 [04:07<09:40,  4.54s/it]\u001b[A\n",
      "Processed prompts:  40%|███▉      | 83/210 [04:10<08:45,  4.14s/it]\u001b[A\n",
      "Processed prompts:  40%|████      | 84/210 [04:12<07:30,  3.57s/it]\u001b[A\n",
      "Processed prompts:  40%|████      | 85/210 [04:15<06:38,  3.19s/it]\u001b[A\n",
      "Processed prompts:  41%|████      | 86/210 [04:17<06:03,  2.93s/it]\u001b[A\n",
      "Processed prompts:  41%|████▏     | 87/210 [04:19<05:24,  2.64s/it]\u001b[A\n",
      "Processed prompts:  42%|████▏     | 88/210 [04:22<05:14,  2.58s/it]\u001b[A\n",
      "Processed prompts:  42%|████▏     | 89/210 [04:23<04:40,  2.32s/it]\u001b[A\n",
      "Processed prompts:  43%|████▎     | 90/210 [04:25<04:30,  2.26s/it]\u001b[A\n",
      "Processed prompts:  43%|████▎     | 91/210 [04:27<04:10,  2.11s/it]\u001b[A\n",
      "Processed prompts:  44%|████▍     | 92/210 [04:27<03:07,  1.59s/it]\u001b[A\n",
      "Processed prompts:  44%|████▍     | 93/210 [04:29<02:54,  1.49s/it]\u001b[A\n",
      "Processed prompts:  45%|████▍     | 94/210 [04:31<03:09,  1.63s/it]\u001b[A\n",
      "Processed prompts:  45%|████▌     | 95/210 [04:31<02:34,  1.34s/it]\u001b[A\n",
      "Processed prompts:  46%|████▌     | 96/210 [04:32<02:18,  1.22s/it]\u001b[A\n",
      "Processed prompts:  46%|████▌     | 97/210 [04:34<02:32,  1.35s/it]\u001b[A\n",
      "Processed prompts:  47%|████▋     | 98/210 [04:34<02:03,  1.10s/it]\u001b[A\n",
      "Processed prompts:  47%|████▋     | 99/210 [04:41<05:10,  2.80s/it]\u001b[A\n",
      "Processed prompts:  48%|████▊     | 100/210 [04:42<03:57,  2.16s/it]\u001b[A\n",
      "Processed prompts:  48%|████▊     | 101/210 [04:56<10:13,  5.62s/it]\u001b[A\n",
      "Processed prompts:  49%|████▊     | 102/210 [05:01<09:46,  5.43s/it]\u001b[A\n",
      "Processed prompts:  49%|████▉     | 103/210 [05:04<08:29,  4.77s/it]\u001b[A\n",
      "Processed prompts:  50%|████▉     | 104/210 [05:10<08:59,  5.09s/it]\u001b[A\n",
      "Processed prompts:  50%|█████     | 105/210 [05:16<09:46,  5.59s/it]\u001b[A\n",
      "Processed prompts:  50%|█████     | 106/210 [05:24<10:33,  6.09s/it]\u001b[A\n",
      "Processed prompts:  51%|█████     | 107/210 [05:30<10:49,  6.31s/it]\u001b[A\n",
      "Processed prompts:  51%|█████▏    | 108/210 [05:34<09:03,  5.33s/it]\u001b[A\n",
      "Processed prompts:  52%|█████▏    | 109/210 [05:37<08:08,  4.83s/it]\u001b[A\n",
      "Processed prompts:  52%|█████▏    | 110/210 [05:41<07:29,  4.49s/it]\u001b[A\n",
      "Processed prompts:  53%|█████▎    | 111/210 [05:44<06:37,  4.02s/it]\u001b[A\n",
      "Processed prompts:  53%|█████▎    | 112/210 [05:56<10:35,  6.49s/it]\u001b[A\n",
      "Processed prompts:  54%|█████▍    | 113/210 [05:58<08:10,  5.05s/it]\u001b[A\n",
      "Processed prompts:  54%|█████▍    | 114/210 [05:59<06:24,  4.01s/it]\u001b[A\n",
      "Processed prompts:  55%|█████▍    | 115/210 [06:00<04:51,  3.06s/it]\u001b[A\n",
      "Processed prompts:  55%|█████▌    | 116/210 [06:01<03:48,  2.43s/it]\u001b[A\n",
      "Processed prompts:  56%|█████▌    | 117/210 [06:02<03:02,  1.97s/it]\u001b[A\n",
      "Processed prompts:  56%|█████▌    | 118/210 [06:03<02:41,  1.75s/it]\u001b[A\n",
      "Processed prompts:  57%|█████▋    | 119/210 [06:05<02:42,  1.78s/it]\u001b[A\n",
      "Processed prompts:  57%|█████▋    | 120/210 [06:07<02:35,  1.73s/it]\u001b[A\n",
      "Processed prompts:  58%|█████▊    | 121/210 [06:07<02:01,  1.37s/it]\u001b[A\n",
      "Processed prompts:  58%|█████▊    | 122/210 [06:09<02:02,  1.39s/it]\u001b[A\n",
      "Processed prompts:  59%|█████▊    | 123/210 [06:10<02:05,  1.44s/it]\u001b[A\n",
      "Processed prompts:  59%|█████▉    | 124/210 [06:11<01:38,  1.15s/it]\u001b[A\n",
      "Processed prompts:  60%|█████▉    | 125/210 [06:11<01:20,  1.06it/s]\u001b[A\n",
      "Processed prompts:  60%|██████    | 126/210 [06:11<01:01,  1.37it/s]\u001b[A\n",
      "Processed prompts:  60%|██████    | 127/210 [06:12<01:07,  1.23it/s]\u001b[A\n",
      "Processed prompts:  61%|██████    | 128/210 [06:13<01:05,  1.25it/s]\u001b[A\n",
      "Processed prompts:  62%|██████▏   | 130/210 [06:14<00:42,  1.87it/s]\u001b[A\n",
      "Processed prompts:  62%|██████▏   | 131/210 [06:16<01:13,  1.07it/s]\u001b[A\n",
      "Processed prompts:  63%|██████▎   | 132/210 [06:21<02:36,  2.01s/it]\u001b[A\n",
      "Processed prompts:  63%|██████▎   | 133/210 [06:22<02:23,  1.86s/it]\u001b[A\n",
      "Processed prompts:  64%|██████▍   | 134/210 [06:32<05:15,  4.15s/it]\u001b[A\n",
      "Processed prompts:  64%|██████▍   | 135/210 [06:44<07:45,  6.21s/it]\u001b[A\n",
      "Processed prompts:  65%|██████▍   | 136/210 [06:59<11:03,  8.96s/it]\u001b[A\n",
      "Processed prompts:  65%|██████▌   | 137/210 [07:10<11:20,  9.32s/it]\u001b[A\n",
      "Processed prompts:  66%|██████▌   | 138/210 [07:18<10:53,  9.07s/it]\u001b[A\n",
      "Processed prompts:  66%|██████▌   | 139/210 [07:24<09:44,  8.23s/it]\u001b[A\n",
      "Processed prompts:  67%|██████▋   | 140/210 [07:27<07:46,  6.66s/it]\u001b[A\n",
      "Processed prompts:  67%|██████▋   | 141/210 [07:32<07:07,  6.20s/it]\u001b[A\n",
      "Processed prompts:  68%|██████▊   | 142/210 [07:35<05:45,  5.08s/it]\u001b[A\n",
      "Processed prompts:  68%|██████▊   | 143/210 [07:37<04:35,  4.11s/it]\u001b[A\n",
      "Processed prompts:  69%|██████▊   | 144/210 [07:39<03:56,  3.58s/it]\u001b[A\n",
      "Processed prompts:  69%|██████▉   | 145/210 [07:41<03:24,  3.14s/it]\u001b[A\n",
      "Processed prompts:  70%|██████▉   | 146/210 [07:44<03:06,  2.92s/it]\u001b[A\n",
      "Processed prompts:  70%|███████   | 147/210 [07:46<02:59,  2.84s/it]\u001b[A\n",
      "Processed prompts:  70%|███████   | 148/210 [07:47<02:24,  2.32s/it]\u001b[A\n",
      "Processed prompts:  71%|███████   | 149/210 [07:50<02:22,  2.33s/it]\u001b[A\n",
      "Processed prompts:  71%|███████▏  | 150/210 [07:51<02:00,  2.01s/it]\u001b[A\n",
      "Processed prompts:  72%|███████▏  | 151/210 [07:51<01:30,  1.53s/it]\u001b[A\n",
      "Processed prompts:  72%|███████▏  | 152/210 [07:52<01:09,  1.20s/it]\u001b[A\n",
      "Processed prompts:  73%|███████▎  | 153/210 [07:52<00:54,  1.06it/s]\u001b[A\n",
      "Processed prompts:  73%|███████▎  | 154/210 [07:53<00:50,  1.11it/s]\u001b[A\n",
      "Processed prompts:  74%|███████▍  | 155/210 [07:55<01:08,  1.25s/it]\u001b[A\n",
      "Processed prompts:  74%|███████▍  | 156/210 [07:55<00:50,  1.06it/s]\u001b[A\n",
      "Processed prompts:  75%|███████▍  | 157/210 [08:08<03:56,  4.45s/it]\u001b[A\n",
      "Processed prompts:  75%|███████▌  | 158/210 [08:23<06:43,  7.76s/it]\u001b[A\n",
      "Processed prompts:  76%|███████▌  | 159/210 [08:33<07:01,  8.27s/it]\u001b[A\n",
      "Processed prompts:  76%|███████▌  | 160/210 [08:38<06:05,  7.32s/it]\u001b[A\n",
      "Processed prompts:  77%|███████▋  | 161/210 [08:42<05:16,  6.46s/it]\u001b[A\n",
      "Processed prompts:  77%|███████▋  | 162/210 [08:46<04:24,  5.51s/it]\u001b[A\n",
      "Processed prompts:  78%|███████▊  | 163/210 [08:51<04:19,  5.52s/it]\u001b[A\n",
      "Processed prompts:  78%|███████▊  | 164/210 [08:57<04:15,  5.55s/it]\u001b[A\n",
      "Processed prompts:  79%|███████▊  | 165/210 [09:00<03:36,  4.81s/it]\u001b[A\n",
      "Processed prompts:  79%|███████▉  | 166/210 [09:04<03:28,  4.75s/it]\u001b[A\n",
      "Processed prompts:  80%|███████▉  | 167/210 [09:07<02:55,  4.08s/it]\u001b[A\n",
      "Processed prompts:  80%|████████  | 168/210 [09:11<02:48,  4.02s/it]\u001b[A\n",
      "Processed prompts:  80%|████████  | 169/210 [09:12<02:14,  3.27s/it]\u001b[A\n",
      "Processed prompts:  81%|████████  | 170/210 [09:15<01:56,  2.92s/it]\u001b[A\n",
      "Processed prompts:  81%|████████▏ | 171/210 [09:17<01:45,  2.71s/it]\u001b[A\n",
      "Processed prompts:  82%|████████▏ | 172/210 [09:18<01:29,  2.35s/it]\u001b[A\n",
      "Processed prompts:  82%|████████▏ | 173/210 [09:21<01:30,  2.43s/it]\u001b[A\n",
      "Processed prompts:  83%|████████▎ | 174/210 [09:23<01:22,  2.30s/it]\u001b[A\n",
      "Processed prompts:  83%|████████▎ | 175/210 [09:24<01:07,  1.94s/it]\u001b[A\n",
      "Processed prompts:  84%|████████▍ | 176/210 [09:25<01:00,  1.78s/it]\u001b[A\n",
      "Processed prompts:  84%|████████▍ | 177/210 [09:26<00:51,  1.55s/it]\u001b[A\n",
      "Processed prompts:  85%|████████▍ | 178/210 [09:27<00:44,  1.38s/it]\u001b[A\n",
      "Processed prompts:  85%|████████▌ | 179/210 [09:28<00:36,  1.19s/it]\u001b[A\n",
      "Processed prompts:  86%|████████▌ | 180/210 [09:32<01:03,  2.12s/it]\u001b[A\n",
      "Processed prompts:  86%|████████▌ | 181/210 [09:41<01:59,  4.13s/it]\u001b[A\n",
      "Processed prompts:  87%|████████▋ | 182/210 [09:56<03:22,  7.22s/it]\u001b[A\n",
      "Processed prompts:  87%|████████▋ | 183/210 [10:07<03:44,  8.31s/it]\u001b[A\n",
      "Processed prompts:  88%|████████▊ | 184/210 [10:16<03:42,  8.57s/it]\u001b[A\n",
      "Processed prompts:  88%|████████▊ | 185/210 [10:21<03:11,  7.65s/it]\u001b[A\n",
      "Processed prompts:  89%|████████▊ | 186/210 [10:25<02:37,  6.58s/it]\u001b[A\n",
      "Processed prompts:  89%|████████▉ | 187/210 [10:29<02:12,  5.76s/it]\u001b[A\n",
      "Processed prompts:  90%|████████▉ | 188/210 [10:33<01:54,  5.20s/it]\u001b[A\n",
      "Processed prompts:  90%|█████████ | 189/210 [10:37<01:40,  4.80s/it]\u001b[A\n",
      "Processed prompts:  90%|█████████ | 190/210 [10:40<01:24,  4.21s/it]\u001b[A\n",
      "Processed prompts:  91%|█████████ | 191/210 [10:43<01:13,  3.88s/it]\u001b[A\n",
      "Processed prompts:  91%|█████████▏| 192/210 [10:45<01:01,  3.40s/it]\u001b[A\n",
      "Processed prompts:  92%|█████████▏| 193/210 [10:46<00:46,  2.73s/it]\u001b[A\n",
      "Processed prompts:  92%|█████████▏| 194/210 [10:48<00:40,  2.51s/it]\u001b[A\n",
      "Processed prompts:  93%|█████████▎| 195/210 [10:49<00:29,  1.99s/it]\u001b[A\n",
      "Processed prompts:  93%|█████████▎| 196/210 [10:50<00:23,  1.70s/it]\u001b[A\n",
      "Processed prompts:  94%|█████████▍| 197/210 [10:51<00:20,  1.54s/it]\u001b[A\n",
      "Processed prompts:  94%|█████████▍| 198/210 [10:53<00:17,  1.48s/it]\u001b[A\n",
      "Processed prompts:  95%|█████████▍| 199/210 [10:53<00:12,  1.16s/it]\u001b[A\n",
      "Processed prompts:  95%|█████████▌| 200/210 [10:54<00:09,  1.03it/s]\u001b[A\n",
      "Processed prompts:  96%|█████████▌| 201/210 [10:54<00:08,  1.03it/s]\u001b[A\n",
      "Processed prompts:  96%|█████████▌| 202/210 [10:56<00:07,  1.01it/s]\u001b[A\n",
      "Processed prompts:  97%|█████████▋| 203/210 [11:02<00:17,  2.53s/it]\u001b[A\n",
      "Processed prompts:  97%|█████████▋| 204/210 [11:10<00:25,  4.18s/it]\u001b[A\n",
      "Processed prompts:  98%|█████████▊| 205/210 [11:15<00:22,  4.55s/it]\u001b[A\n",
      "Processed prompts:  98%|█████████▊| 206/210 [11:20<00:18,  4.65s/it]\u001b[A\n",
      "Processed prompts:  99%|█████████▊| 207/210 [11:24<00:12,  4.33s/it]\u001b[A\n",
      "Processed prompts:  99%|█████████▉| 208/210 [11:26<00:07,  3.91s/it]\u001b[A\n",
      "Processed prompts: 100%|█████████▉| 209/210 [11:28<00:03,  3.20s/it]\u001b[A\n",
      "Processed prompts: 100%|██████████| 210/210 [11:30<00:00,  3.29s/it]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148e44e4005d47f1a67db43719f03d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  29%|██▊       | 2/7 [19:52<51:04, 612.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] /root/autodl-fs/LRP/open_ended_data_generation/20251210_llama2_7b_base/20251210_LRP_generation_th_1_selected_LRP_kur_res_en_zscore_open_ended.json\n",
      "****************************************\n",
      "Model: th_1_selected_LRP_kur_res_en_zscore_margin_selected\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/210 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts:   0%|          | 1/210 [00:04<14:53,  4.27s/it]\u001b[A\n",
      "Processed prompts:   1%|          | 2/210 [00:04<06:32,  1.89s/it]\u001b[A\n",
      "Processed prompts:   2%|▏         | 4/210 [00:04<02:34,  1.33it/s]\u001b[A\n",
      "Processed prompts:   4%|▍         | 8/210 [00:04<00:59,  3.42it/s]\u001b[A\n",
      "Processed prompts:   5%|▍         | 10/210 [00:05<00:51,  3.89it/s]\u001b[A\n",
      "Processed prompts:   7%|▋         | 14/210 [00:05<00:38,  5.11it/s]\u001b[A\n",
      "Processed prompts:   8%|▊         | 16/210 [00:05<00:33,  5.75it/s]\u001b[A\n",
      "Processed prompts:   9%|▊         | 18/210 [00:08<01:19,  2.42it/s]\u001b[A\n",
      "Processed prompts:   9%|▉         | 19/210 [00:08<01:13,  2.60it/s]\u001b[A\n",
      "Processed prompts:  10%|▉         | 20/210 [00:08<01:15,  2.51it/s]\u001b[A\n",
      "Processed prompts:  10%|█         | 22/210 [00:09<01:03,  2.94it/s]\u001b[A\n",
      "Processed prompts:  11%|█         | 23/210 [00:09<01:19,  2.34it/s]\u001b[A\n",
      "Processed prompts:  11%|█▏        | 24/210 [00:10<01:29,  2.07it/s]\u001b[A\n",
      "Processed prompts:  12%|█▏        | 25/210 [00:11<01:25,  2.17it/s]\u001b[A\n",
      "Processed prompts:  12%|█▏        | 26/210 [00:14<03:41,  1.20s/it]\u001b[A\n",
      "Processed prompts:  13%|█▎        | 28/210 [00:16<03:09,  1.04s/it]\u001b[A\n",
      "Processed prompts:  14%|█▍        | 29/210 [00:18<03:57,  1.31s/it]\u001b[A\n",
      "Processed prompts:  14%|█▍        | 30/210 [00:20<04:28,  1.49s/it]\u001b[A\n",
      "Processed prompts:  15%|█▍        | 31/210 [00:21<04:21,  1.46s/it]\u001b[A\n",
      "Processed prompts:  15%|█▌        | 32/210 [00:21<03:25,  1.15s/it]\u001b[A\n",
      "Processed prompts:  16%|█▌        | 33/210 [00:25<05:23,  1.83s/it]\u001b[A\n",
      "Processed prompts:  16%|█▌        | 34/210 [00:25<04:05,  1.39s/it]\u001b[A\n",
      "Processed prompts:  17%|█▋        | 35/210 [00:26<03:37,  1.24s/it]\u001b[A\n",
      "Processed prompts:  17%|█▋        | 36/210 [00:27<02:49,  1.03it/s]\u001b[A\n",
      "Processed prompts:  18%|█▊        | 37/210 [00:27<02:05,  1.37it/s]\u001b[A\n",
      "Processed prompts:  18%|█▊        | 38/210 [00:31<04:57,  1.73s/it]\u001b[A\n",
      "Processed prompts:  19%|█▊        | 39/210 [00:32<04:29,  1.58s/it]\u001b[A\n",
      "Processed prompts:  19%|█▉        | 40/210 [00:33<04:05,  1.45s/it]\u001b[A\n",
      "Processed prompts:  20%|█▉        | 41/210 [00:33<02:58,  1.06s/it]\u001b[A\n",
      "Processed prompts:  20%|██        | 42/210 [00:34<03:02,  1.09s/it]\u001b[A\n",
      "Processed prompts:  20%|██        | 43/210 [00:35<02:41,  1.03it/s]\u001b[A\n",
      "Processed prompts:  21%|██        | 44/210 [00:41<06:26,  2.33s/it]\u001b[A\n",
      "Processed prompts:  21%|██▏       | 45/210 [00:53<14:24,  5.24s/it]\u001b[A\n",
      "Processed prompts:  22%|██▏       | 46/210 [01:56<1:02:04, 22.71s/it]\u001b[A\n",
      "Processed prompts:  29%|██▊       | 60/210 [02:02<09:02,  3.61s/it]  \u001b[A\n",
      "Processed prompts:  29%|██▉       | 61/210 [02:10<09:42,  3.91s/it]\u001b[A\n",
      "Processed prompts:  30%|██▉       | 62/210 [02:16<10:19,  4.18s/it]\u001b[A\n",
      "Processed prompts:  30%|███       | 63/210 [02:22<10:41,  4.36s/it]\u001b[A\n",
      "Processed prompts:  30%|███       | 64/210 [02:27<10:48,  4.44s/it]\u001b[A\n",
      "Processed prompts:  31%|███       | 65/210 [02:31<10:47,  4.46s/it]\u001b[A\n",
      "Processed prompts:  31%|███▏      | 66/210 [02:35<10:37,  4.43s/it]\u001b[A\n",
      "Processed prompts:  32%|███▏      | 67/210 [02:39<10:07,  4.25s/it]\u001b[A\n",
      "Processed prompts:  32%|███▏      | 68/210 [02:43<09:39,  4.08s/it]\u001b[A\n",
      "Processed prompts:  33%|███▎      | 69/210 [02:46<09:00,  3.84s/it]\u001b[A\n",
      "Processed prompts:  33%|███▎      | 70/210 [02:49<08:29,  3.64s/it]\u001b[A\n",
      "Processed prompts:  34%|███▍      | 71/210 [02:52<07:56,  3.43s/it]\u001b[A\n",
      "Processed prompts:  34%|███▍      | 72/210 [02:54<07:11,  3.13s/it]\u001b[A\n",
      "Processed prompts:  35%|███▍      | 73/210 [02:57<06:51,  3.00s/it]\u001b[A\n",
      "Processed prompts:  35%|███▌      | 74/210 [02:59<06:14,  2.75s/it]\u001b[A\n",
      "Processed prompts:  36%|███▌      | 75/210 [03:01<05:50,  2.60s/it]\u001b[A\n",
      "Processed prompts:  36%|███▌      | 76/210 [03:03<05:30,  2.47s/it]\u001b[A\n",
      "Processed prompts:  37%|███▋      | 77/210 [03:04<04:29,  2.03s/it]\u001b[A\n",
      "Processed prompts:  37%|███▋      | 78/210 [03:05<03:46,  1.71s/it]\u001b[A\n",
      "Processed prompts:  38%|███▊      | 79/210 [03:07<03:47,  1.74s/it]\u001b[A\n",
      "Processed prompts:  38%|███▊      | 80/210 [03:09<03:51,  1.78s/it]\u001b[A\n",
      "Processed prompts:  39%|███▊      | 81/210 [03:11<03:51,  1.79s/it]\u001b[A\n",
      "Processed prompts:  39%|███▉      | 82/210 [03:14<04:31,  2.12s/it]\u001b[A\n",
      "Processed prompts:  40%|███▉      | 83/210 [03:17<05:32,  2.62s/it]\u001b[A\n",
      "Processed prompts:  40%|████      | 84/210 [03:30<11:50,  5.64s/it]\u001b[A\n",
      "Processed prompts:  40%|████      | 85/210 [03:45<17:39,  8.48s/it]\u001b[A\n",
      "Processed prompts:  41%|████      | 86/210 [03:53<17:00,  8.23s/it]\u001b[A\n",
      "Processed prompts:  41%|████▏     | 87/210 [03:58<15:01,  7.33s/it]\u001b[A\n",
      "Processed prompts:  42%|████▏     | 88/210 [04:02<12:52,  6.33s/it]\u001b[A\n",
      "Processed prompts:  42%|████▏     | 89/210 [04:05<10:31,  5.22s/it]\u001b[A\n",
      "Processed prompts:  43%|████▎     | 90/210 [04:08<09:09,  4.58s/it]\u001b[A\n",
      "Processed prompts:  43%|████▎     | 91/210 [04:11<08:20,  4.20s/it]\u001b[A\n",
      "Processed prompts:  44%|████▍     | 92/210 [04:13<07:04,  3.60s/it]\u001b[A\n",
      "Processed prompts:  44%|████▍     | 93/210 [04:16<06:21,  3.26s/it]\u001b[A\n",
      "Processed prompts:  45%|████▍     | 94/210 [04:19<06:02,  3.13s/it]\u001b[A\n",
      "Processed prompts:  45%|████▌     | 95/210 [04:21<05:20,  2.79s/it]\u001b[A\n",
      "Processed prompts:  46%|████▌     | 96/210 [04:23<04:53,  2.57s/it]\u001b[A\n",
      "Processed prompts:  46%|████▌     | 97/210 [04:25<04:35,  2.44s/it]\u001b[A\n",
      "Processed prompts:  47%|████▋     | 98/210 [04:27<04:14,  2.28s/it]\u001b[A\n",
      "Processed prompts:  47%|████▋     | 99/210 [04:28<03:55,  2.12s/it]\u001b[A\n",
      "Processed prompts:  48%|████▊     | 100/210 [04:30<03:40,  2.01s/it]\u001b[A\n",
      "Processed prompts:  48%|████▊     | 101/210 [04:32<03:43,  2.05s/it]\u001b[A\n",
      "Processed prompts:  49%|████▊     | 102/210 [04:33<02:54,  1.62s/it]\u001b[A\n",
      "Processed prompts:  49%|████▉     | 103/210 [04:38<04:29,  2.51s/it]\u001b[A\n",
      "Processed prompts:  50%|████▉     | 104/210 [04:39<03:51,  2.18s/it]\u001b[A\n",
      "Processed prompts:  50%|█████     | 105/210 [04:42<04:07,  2.36s/it]\u001b[A\n",
      "Processed prompts:  50%|█████     | 106/210 [04:43<03:37,  2.09s/it]\u001b[A\n",
      "Processed prompts:  51%|█████     | 107/210 [04:44<02:48,  1.64s/it]\u001b[A\n",
      "Processed prompts:  51%|█████▏    | 108/210 [04:46<02:48,  1.65s/it]\u001b[A\n",
      "Processed prompts:  52%|█████▏    | 109/210 [04:52<05:06,  3.04s/it]\u001b[A\n",
      "Processed prompts:  52%|█████▏    | 110/210 [04:56<05:44,  3.44s/it]\u001b[A\n",
      "Processed prompts:  53%|█████▎    | 111/210 [05:06<08:53,  5.38s/it]\u001b[A\n",
      "Processed prompts:  53%|█████▎    | 112/210 [05:11<08:34,  5.25s/it]\u001b[A\n",
      "Processed prompts:  54%|█████▍    | 113/210 [05:18<09:09,  5.67s/it]\u001b[A\n",
      "Processed prompts:  54%|█████▍    | 114/210 [05:26<10:07,  6.33s/it]\u001b[A\n",
      "Processed prompts:  55%|█████▍    | 115/210 [05:30<09:18,  5.87s/it]\u001b[A\n",
      "Processed prompts:  55%|█████▌    | 116/210 [05:36<08:56,  5.70s/it]\u001b[A\n",
      "Processed prompts:  56%|█████▌    | 117/210 [05:39<07:31,  4.86s/it]\u001b[A\n",
      "Processed prompts:  56%|█████▌    | 118/210 [05:40<05:57,  3.88s/it]\u001b[A\n",
      "Processed prompts:  57%|█████▋    | 119/210 [05:43<05:16,  3.48s/it]\u001b[A\n",
      "Processed prompts:  57%|█████▋    | 120/210 [05:44<04:10,  2.79s/it]\u001b[A\n",
      "Processed prompts:  58%|█████▊    | 121/210 [05:49<04:59,  3.37s/it]\u001b[A\n",
      "Processed prompts:  58%|█████▊    | 122/210 [05:53<05:15,  3.59s/it]\u001b[A\n",
      "Processed prompts:  59%|█████▊    | 123/210 [05:56<05:02,  3.47s/it]\u001b[A\n",
      "Processed prompts:  59%|█████▉    | 124/210 [06:00<05:12,  3.63s/it]\u001b[A\n",
      "Processed prompts:  60%|█████▉    | 125/210 [06:00<03:48,  2.68s/it]\u001b[A\n",
      "Processed prompts:  60%|██████    | 126/210 [06:02<03:25,  2.45s/it]\u001b[A\n",
      "Processed prompts:  60%|██████    | 127/210 [06:03<02:49,  2.04s/it]\u001b[A\n",
      "Processed prompts:  61%|██████    | 128/210 [06:05<02:43,  2.00s/it]\u001b[A\n",
      "Processed prompts:  61%|██████▏   | 129/210 [06:06<02:07,  1.58s/it]\u001b[A\n",
      "Processed prompts:  62%|██████▏   | 130/210 [06:07<01:52,  1.41s/it]\u001b[A\n",
      "Processed prompts:  62%|██████▏   | 131/210 [06:08<01:50,  1.39s/it]\u001b[A\n",
      "Processed prompts:  63%|██████▎   | 132/210 [06:09<01:31,  1.17s/it]\u001b[A\n",
      "Processed prompts:  64%|██████▍   | 134/210 [06:10<01:01,  1.24it/s]\u001b[A\n",
      "Processed prompts:  64%|██████▍   | 135/210 [06:10<00:57,  1.31it/s]\u001b[A\n",
      "Processed prompts:  65%|██████▍   | 136/210 [06:11<00:59,  1.24it/s]\u001b[A\n",
      "Processed prompts:  65%|██████▌   | 137/210 [06:12<00:55,  1.31it/s]\u001b[A\n",
      "Processed prompts:  66%|██████▌   | 138/210 [06:12<00:52,  1.36it/s]\u001b[A\n",
      "Processed prompts:  66%|██████▌   | 139/210 [06:15<01:34,  1.33s/it]\u001b[A\n",
      "Processed prompts:  67%|██████▋   | 140/210 [06:33<07:02,  6.04s/it]\u001b[A\n",
      "Processed prompts:  67%|██████▋   | 141/210 [06:50<10:47,  9.38s/it]\u001b[A\n",
      "Processed prompts:  68%|██████▊   | 142/210 [06:58<09:57,  8.79s/it]\u001b[A\n",
      "Processed prompts:  68%|██████▊   | 143/210 [07:02<08:14,  7.39s/it]\u001b[A\n",
      "Processed prompts:  69%|██████▊   | 144/210 [07:12<09:12,  8.38s/it]\u001b[A\n",
      "Processed prompts:  69%|██████▉   | 145/210 [07:16<07:36,  7.02s/it]\u001b[A\n",
      "Processed prompts:  70%|██████▉   | 146/210 [07:21<06:42,  6.29s/it]\u001b[A\n",
      "Processed prompts:  70%|███████   | 147/210 [07:24<05:40,  5.41s/it]\u001b[A\n",
      "Processed prompts:  70%|███████   | 148/210 [07:34<06:47,  6.57s/it]\u001b[A\n",
      "Processed prompts:  71%|███████   | 149/210 [07:38<05:58,  5.87s/it]\u001b[A\n",
      "Processed prompts:  71%|███████▏  | 150/210 [07:41<05:03,  5.05s/it]\u001b[A\n",
      "Processed prompts:  72%|███████▏  | 151/210 [07:43<04:08,  4.22s/it]\u001b[A\n",
      "Processed prompts:  72%|███████▏  | 152/210 [07:46<03:31,  3.65s/it]\u001b[A\n",
      "Processed prompts:  73%|███████▎  | 153/210 [07:46<02:42,  2.85s/it]\u001b[A\n",
      "Processed prompts:  73%|███████▎  | 154/210 [07:47<02:08,  2.30s/it]\u001b[A\n",
      "Processed prompts:  74%|███████▍  | 155/210 [07:49<01:54,  2.07s/it]\u001b[A\n",
      "Processed prompts:  74%|███████▍  | 156/210 [07:49<01:22,  1.53s/it]\u001b[A\n",
      "Processed prompts:  75%|███████▍  | 157/210 [07:50<01:14,  1.41s/it]\u001b[A\n",
      "Processed prompts:  75%|███████▌  | 158/210 [07:51<01:03,  1.23s/it]\u001b[A\n",
      "Processed prompts:  76%|███████▌  | 159/210 [07:52<01:01,  1.20s/it]\u001b[A\n",
      "Processed prompts:  76%|███████▌  | 160/210 [07:53<00:48,  1.03it/s]\u001b[A\n",
      "Processed prompts:  77%|███████▋  | 161/210 [07:53<00:40,  1.22it/s]\u001b[A\n",
      "Processed prompts:  77%|███████▋  | 162/210 [07:54<00:34,  1.39it/s]\u001b[A\n",
      "Processed prompts:  78%|███████▊  | 163/210 [07:55<00:38,  1.23it/s]\u001b[A\n",
      "Processed prompts:  78%|███████▊  | 164/210 [07:58<01:08,  1.49s/it]\u001b[A\n",
      "Processed prompts:  79%|███████▊  | 165/210 [07:59<01:00,  1.35s/it]\u001b[A\n",
      "Processed prompts:  79%|███████▉  | 166/210 [07:59<00:47,  1.07s/it]\u001b[A\n",
      "Processed prompts:  80%|███████▉  | 167/210 [08:15<03:51,  5.38s/it]\u001b[A\n",
      "Processed prompts:  80%|████████  | 168/210 [08:35<06:56,  9.92s/it]\u001b[A\n",
      "Processed prompts:  80%|████████  | 169/210 [08:48<07:22, 10.80s/it]\u001b[A\n",
      "Processed prompts:  81%|████████  | 170/210 [08:53<05:59,  8.99s/it]\u001b[A\n",
      "Processed prompts:  81%|████████▏ | 171/210 [08:57<04:54,  7.54s/it]\u001b[A\n",
      "Processed prompts:  82%|████████▏ | 172/210 [09:00<03:48,  6.02s/it]\u001b[A\n",
      "Processed prompts:  82%|████████▏ | 173/210 [09:03<03:17,  5.33s/it]\u001b[A\n",
      "Processed prompts:  83%|████████▎ | 174/210 [09:05<02:34,  4.28s/it]\u001b[A\n",
      "Processed prompts:  83%|████████▎ | 175/210 [09:07<02:07,  3.64s/it]\u001b[A\n",
      "Processed prompts:  84%|████████▍ | 176/210 [09:10<01:51,  3.28s/it]\u001b[A\n",
      "Processed prompts:  84%|████████▍ | 177/210 [09:12<01:40,  3.05s/it]\u001b[A\n",
      "Processed prompts:  85%|████████▍ | 178/210 [09:17<01:53,  3.55s/it]\u001b[A\n",
      "Processed prompts:  85%|████████▌ | 179/210 [09:18<01:27,  2.84s/it]\u001b[A\n",
      "Processed prompts:  86%|████████▌ | 180/210 [09:21<01:22,  2.73s/it]\u001b[A\n",
      "Processed prompts:  86%|████████▌ | 181/210 [09:21<01:01,  2.13s/it]\u001b[A\n",
      "Processed prompts:  87%|████████▋ | 182/210 [09:22<00:44,  1.59s/it]\u001b[A\n",
      "Processed prompts:  87%|████████▋ | 183/210 [09:23<00:38,  1.42s/it]\u001b[A\n",
      "Processed prompts:  88%|████████▊ | 184/210 [09:23<00:31,  1.20s/it]\u001b[A\n",
      "Processed prompts:  88%|████████▊ | 185/210 [09:24<00:23,  1.05it/s]\u001b[A\n",
      "Processed prompts:  89%|████████▊ | 186/210 [09:25<00:23,  1.01it/s]\u001b[A\n",
      "Processed prompts:  89%|████████▉ | 187/210 [09:25<00:20,  1.10it/s]\u001b[A\n",
      "Processed prompts:  90%|████████▉ | 188/210 [09:27<00:20,  1.06it/s]\u001b[A\n",
      "Processed prompts:  90%|█████████ | 189/210 [09:32<00:48,  2.31s/it]\u001b[A\n",
      "Processed prompts:  90%|█████████ | 190/210 [09:33<00:40,  2.04s/it]\u001b[A\n",
      "Processed prompts:  91%|█████████ | 191/210 [09:42<01:18,  4.11s/it]\u001b[A\n",
      "Processed prompts:  91%|█████████▏| 192/210 [09:44<00:59,  3.32s/it]\u001b[A\n",
      "Processed prompts:  92%|█████████▏| 193/210 [09:50<01:13,  4.30s/it]\u001b[A\n",
      "Processed prompts:  92%|█████████▏| 194/210 [09:59<01:29,  5.60s/it]\u001b[A\n",
      "Processed prompts:  93%|█████████▎| 195/210 [10:03<01:16,  5.09s/it]\u001b[A\n",
      "Processed prompts:  93%|█████████▎| 196/210 [10:06<01:03,  4.56s/it]\u001b[A\n",
      "Processed prompts:  94%|█████████▍| 197/210 [10:10<00:54,  4.18s/it]\u001b[A\n",
      "Processed prompts:  94%|█████████▍| 198/210 [10:12<00:43,  3.66s/it]\u001b[A\n",
      "Processed prompts:  95%|█████████▍| 199/210 [10:15<00:36,  3.32s/it]\u001b[A\n",
      "Processed prompts:  95%|█████████▌| 200/210 [10:17<00:30,  3.07s/it]\u001b[A\n",
      "Processed prompts:  96%|█████████▌| 201/210 [10:21<00:31,  3.45s/it]\u001b[A\n",
      "Processed prompts:  96%|█████████▌| 202/210 [10:24<00:26,  3.29s/it]\u001b[A\n",
      "Processed prompts:  97%|█████████▋| 203/210 [10:28<00:22,  3.26s/it]\u001b[A\n",
      "Processed prompts:  97%|█████████▋| 204/210 [10:28<00:15,  2.52s/it]\u001b[A\n",
      "Processed prompts:  98%|█████████▊| 205/210 [10:29<00:10,  2.04s/it]\u001b[A\n",
      "Processed prompts:  98%|█████████▊| 206/210 [10:31<00:07,  1.82s/it]\u001b[A\n",
      "Processed prompts:  99%|█████████▊| 207/210 [10:32<00:04,  1.66s/it]\u001b[A\n",
      "Processed prompts:  99%|█████████▉| 208/210 [10:33<00:02,  1.43s/it]\u001b[A\n",
      "Processed prompts: 100%|█████████▉| 209/210 [10:34<00:01,  1.45s/it]\u001b[A\n",
      "Processed prompts: 100%|██████████| 210/210 [10:35<00:00,  3.02s/it]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497eee1ec7cb4fbbb491181f9ea89d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  43%|████▎     | 3/7 [30:27<41:32, 623.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] /root/autodl-fs/LRP/open_ended_data_generation/20251210_llama2_7b_base/20251210_LRP_generation_th_1_selected_LRP_kur_res_en_zscore_margin_selected_open_ended.json\n",
      "****************************************\n",
      "Model: th_1_selected_LRP_kur_res_vi_zscore\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/210 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts:   0%|          | 1/210 [00:04<14:40,  4.21s/it]\u001b[A\n",
      "Processed prompts:   1%|          | 2/210 [00:04<06:54,  1.99s/it]\u001b[A\n",
      "Processed prompts:   2%|▏         | 4/210 [00:05<03:10,  1.08it/s]\u001b[A\n",
      "Processed prompts:   2%|▏         | 5/210 [00:05<02:29,  1.37it/s]\u001b[A\n",
      "Processed prompts:   3%|▎         | 6/210 [00:05<02:02,  1.67it/s]\u001b[A\n",
      "Processed prompts:   4%|▍         | 8/210 [00:06<01:49,  1.84it/s]\u001b[A\n",
      "Processed prompts:   4%|▍         | 9/210 [00:07<01:37,  2.07it/s]\u001b[A\n",
      "Processed prompts:   5%|▌         | 11/210 [00:07<01:02,  3.18it/s]\u001b[A\n",
      "Processed prompts:   6%|▌         | 12/210 [00:07<00:55,  3.59it/s]\u001b[A\n",
      "Processed prompts:   6%|▌         | 13/210 [00:07<00:48,  4.04it/s]\u001b[A\n",
      "Processed prompts:   7%|▋         | 14/210 [00:08<01:03,  3.08it/s]\u001b[A\n",
      "Processed prompts:   8%|▊         | 16/210 [00:08<01:08,  2.83it/s]\u001b[A\n",
      "Processed prompts:   8%|▊         | 17/210 [00:09<01:03,  3.05it/s]\u001b[A\n",
      "Processed prompts:   9%|▉         | 19/210 [00:09<01:01,  3.11it/s]\u001b[A\n",
      "Processed prompts:  10%|▉         | 20/210 [00:10<01:17,  2.46it/s]\u001b[A\n",
      "Processed prompts:  10%|█         | 21/210 [00:10<01:09,  2.74it/s]\u001b[A\n",
      "Processed prompts:  10%|█         | 22/210 [00:12<02:33,  1.23it/s]\u001b[A\n",
      "Processed prompts:  11%|█         | 23/210 [00:13<02:06,  1.48it/s]\u001b[A\n",
      "Processed prompts:  11%|█▏        | 24/210 [00:13<02:15,  1.37it/s]\u001b[A\n",
      "Processed prompts:  12%|█▏        | 25/210 [00:18<05:34,  1.81s/it]\u001b[A\n",
      "Processed prompts:  13%|█▎        | 27/210 [00:18<03:08,  1.03s/it]\u001b[A\n",
      "Processed prompts:  14%|█▍        | 29/210 [00:19<02:15,  1.34it/s]\u001b[A\n",
      "Processed prompts:  15%|█▍        | 31/210 [00:19<01:30,  1.99it/s]\u001b[A\n",
      "Processed prompts:  15%|█▌        | 32/210 [00:23<03:29,  1.17s/it]\u001b[A\n",
      "Processed prompts:  16%|█▌        | 33/210 [00:23<02:58,  1.01s/it]\u001b[A\n",
      "Processed prompts:  16%|█▌        | 34/210 [00:26<04:03,  1.38s/it]\u001b[A\n",
      "Processed prompts:  17%|█▋        | 35/210 [00:26<03:07,  1.07s/it]\u001b[A\n",
      "Processed prompts:  18%|█▊        | 37/210 [00:32<05:35,  1.94s/it]\u001b[A\n",
      "Processed prompts:  18%|█▊        | 38/210 [00:32<04:24,  1.54s/it]\u001b[A\n",
      "Processed prompts:  19%|█▊        | 39/210 [00:33<03:31,  1.24s/it]\u001b[A\n",
      "Processed prompts:  19%|█▉        | 40/210 [00:35<04:21,  1.54s/it]\u001b[A\n",
      "Processed prompts:  20%|█▉        | 41/210 [00:36<03:36,  1.28s/it]\u001b[A\n",
      "Processed prompts:  20%|██        | 42/210 [00:38<04:41,  1.68s/it]\u001b[A\n",
      "Processed prompts:  20%|██        | 43/210 [00:39<03:54,  1.41s/it]\u001b[A\n",
      "Processed prompts:  21%|██        | 44/210 [00:40<03:15,  1.18s/it]\u001b[A\n",
      "Processed prompts:  21%|██▏       | 45/210 [00:43<05:08,  1.87s/it]\u001b[A\n",
      "Processed prompts:  22%|██▏       | 46/210 [00:51<10:03,  3.68s/it]\u001b[A\n",
      "Processed prompts:  22%|██▏       | 47/210 [01:57<59:43, 21.98s/it]\u001b[A\n",
      "Processed prompts:  29%|██▉       | 61/210 [02:02<08:42,  3.50s/it]\u001b[A\n",
      "Processed prompts:  30%|██▉       | 62/210 [02:03<08:06,  3.29s/it]\u001b[A\n",
      "Processed prompts:  30%|███       | 63/210 [02:10<09:02,  3.69s/it]\u001b[A\n",
      "Processed prompts:  30%|███       | 64/210 [02:16<09:48,  4.03s/it]\u001b[A\n",
      "Processed prompts:  31%|███       | 65/210 [02:22<10:21,  4.29s/it]\u001b[A\n",
      "Processed prompts:  31%|███▏      | 66/210 [02:27<10:32,  4.39s/it]\u001b[A\n",
      "Processed prompts:  32%|███▏      | 67/210 [02:32<10:36,  4.45s/it]\u001b[A\n",
      "Processed prompts:  32%|███▏      | 68/210 [02:36<10:24,  4.40s/it]\u001b[A\n",
      "Processed prompts:  33%|███▎      | 69/210 [02:39<09:53,  4.21s/it]\u001b[A\n",
      "Processed prompts:  33%|███▎      | 70/210 [02:43<09:30,  4.07s/it]\u001b[A\n",
      "Processed prompts:  34%|███▍      | 71/210 [02:46<08:54,  3.85s/it]\u001b[A\n",
      "Processed prompts:  34%|███▍      | 72/210 [02:49<08:21,  3.63s/it]\u001b[A\n",
      "Processed prompts:  35%|███▍      | 73/210 [02:52<07:45,  3.40s/it]\u001b[A\n",
      "Processed prompts:  35%|███▌      | 74/210 [02:55<07:04,  3.12s/it]\u001b[A\n",
      "Processed prompts:  36%|███▌      | 75/210 [02:57<06:41,  2.98s/it]\u001b[A\n",
      "Processed prompts:  36%|███▌      | 76/210 [03:00<06:06,  2.73s/it]\u001b[A\n",
      "Processed prompts:  37%|███▋      | 77/210 [03:02<05:47,  2.61s/it]\u001b[A\n",
      "Processed prompts:  37%|███▋      | 78/210 [03:04<05:25,  2.47s/it]\u001b[A\n",
      "Processed prompts:  38%|███▊      | 79/210 [03:06<05:01,  2.30s/it]\u001b[A\n",
      "Processed prompts:  38%|███▊      | 80/210 [03:07<04:21,  2.01s/it]\u001b[A\n",
      "Processed prompts:  39%|███▊      | 81/210 [03:08<03:22,  1.57s/it]\u001b[A\n",
      "Processed prompts:  39%|███▉      | 82/210 [03:10<03:34,  1.68s/it]\u001b[A\n",
      "Processed prompts:  40%|███▉      | 83/210 [03:11<03:33,  1.68s/it]\u001b[A\n",
      "Processed prompts:  40%|████      | 84/210 [03:13<03:32,  1.69s/it]\u001b[A\n",
      "Processed prompts:  40%|████      | 85/210 [03:14<03:14,  1.55s/it]\u001b[A\n",
      "Processed prompts:  41%|████      | 86/210 [03:15<02:39,  1.29s/it]\u001b[A\n",
      "Processed prompts:  41%|████▏     | 87/210 [03:18<03:52,  1.89s/it]\u001b[A\n",
      "Processed prompts:  42%|████▏     | 88/210 [03:19<03:05,  1.52s/it]\u001b[A\n",
      "Processed prompts:  42%|████▏     | 89/210 [03:20<02:41,  1.33s/it]\u001b[A\n",
      "Processed prompts:  43%|████▎     | 90/210 [03:32<09:24,  4.70s/it]\u001b[A\n",
      "Processed prompts:  43%|████▎     | 91/210 [03:35<07:57,  4.02s/it]\u001b[A\n",
      "Processed prompts:  44%|████▍     | 92/210 [03:46<12:07,  6.16s/it]\u001b[A\n",
      "Processed prompts:  44%|████▍     | 93/210 [03:55<13:26,  6.90s/it]\u001b[A\n",
      "Processed prompts:  45%|████▍     | 94/210 [04:00<12:18,  6.37s/it]\u001b[A\n",
      "Processed prompts:  45%|████▌     | 95/210 [04:03<10:23,  5.42s/it]\u001b[A\n",
      "Processed prompts:  46%|████▌     | 96/210 [04:06<09:13,  4.85s/it]\u001b[A\n",
      "Processed prompts:  46%|████▌     | 97/210 [04:10<08:26,  4.48s/it]\u001b[A\n",
      "Processed prompts:  47%|████▋     | 98/210 [04:14<07:48,  4.19s/it]\u001b[A\n",
      "Processed prompts:  47%|████▋     | 99/210 [04:15<06:24,  3.46s/it]\u001b[A\n",
      "Processed prompts:  48%|████▊     | 100/210 [04:20<06:52,  3.75s/it]\u001b[A\n",
      "Processed prompts:  48%|████▊     | 101/210 [04:23<06:45,  3.72s/it]\u001b[A\n",
      "Processed prompts:  49%|████▊     | 102/210 [04:26<05:56,  3.30s/it]\u001b[A\n",
      "Processed prompts:  49%|████▉     | 103/210 [04:29<05:39,  3.17s/it]\u001b[A\n",
      "Processed prompts:  50%|████▉     | 104/210 [04:30<04:38,  2.63s/it]\u001b[A\n",
      "Processed prompts:  50%|█████     | 105/210 [04:36<06:31,  3.73s/it]\u001b[A\n",
      "Processed prompts:  50%|█████     | 106/210 [04:37<04:59,  2.88s/it]\u001b[A\n",
      "Processed prompts:  51%|█████     | 107/210 [04:39<04:13,  2.46s/it]\u001b[A\n",
      "Processed prompts:  51%|█████▏    | 108/210 [04:39<03:05,  1.82s/it]\u001b[A\n",
      "Processed prompts:  52%|█████▏    | 109/210 [04:40<02:50,  1.69s/it]\u001b[A\n",
      "Processed prompts:  52%|█████▏    | 110/210 [04:42<02:39,  1.60s/it]\u001b[A\n",
      "Processed prompts:  53%|█████▎    | 111/210 [04:42<02:07,  1.29s/it]\u001b[A\n",
      "Processed prompts:  53%|█████▎    | 112/210 [04:44<02:19,  1.42s/it]\u001b[A\n",
      "Processed prompts:  54%|█████▍    | 113/210 [04:44<01:44,  1.08s/it]\u001b[A\n",
      "Processed prompts:  54%|█████▍    | 114/210 [04:47<02:28,  1.54s/it]\u001b[A\n",
      "Processed prompts:  55%|█████▍    | 115/210 [04:51<03:31,  2.23s/it]\u001b[A\n",
      "Processed prompts:  55%|█████▌    | 116/210 [04:52<03:03,  1.95s/it]\u001b[A\n",
      "Processed prompts:  56%|█████▌    | 117/210 [04:52<02:16,  1.47s/it]\u001b[A\n",
      "Processed prompts:  56%|█████▌    | 118/210 [04:55<02:41,  1.75s/it]\u001b[A\n",
      "Processed prompts:  57%|█████▋    | 119/210 [05:01<04:50,  3.20s/it]\u001b[A\n",
      "Processed prompts:  57%|█████▋    | 120/210 [05:04<04:40,  3.12s/it]\u001b[A\n",
      "Processed prompts:  58%|█████▊    | 121/210 [05:07<04:37,  3.11s/it]\u001b[A\n",
      "Processed prompts:  58%|█████▊    | 122/210 [05:14<06:17,  4.28s/it]\u001b[A\n",
      "Processed prompts:  59%|█████▊    | 123/210 [05:18<05:40,  3.91s/it]\u001b[A\n",
      "Processed prompts:  59%|█████▉    | 124/210 [05:26<07:30,  5.24s/it]\u001b[A\n",
      "Processed prompts:  60%|█████▉    | 125/210 [05:34<08:33,  6.04s/it]\u001b[A\n",
      "Processed prompts:  60%|██████    | 126/210 [05:40<08:42,  6.22s/it]\u001b[A\n",
      "Processed prompts:  60%|██████    | 127/210 [05:44<07:29,  5.42s/it]\u001b[A\n",
      "Processed prompts:  61%|██████    | 128/210 [05:47<06:35,  4.82s/it]\u001b[A\n",
      "Processed prompts:  61%|██████▏   | 129/210 [05:53<06:50,  5.07s/it]\u001b[A\n",
      "Processed prompts:  62%|██████▏   | 130/210 [05:58<06:45,  5.07s/it]\u001b[A\n",
      "Processed prompts:  62%|██████▏   | 131/210 [06:02<06:13,  4.73s/it]\u001b[A\n",
      "Processed prompts:  63%|██████▎   | 132/210 [06:07<06:25,  4.95s/it]\u001b[A\n",
      "Processed prompts:  63%|██████▎   | 133/210 [06:10<05:33,  4.33s/it]\u001b[A\n",
      "Processed prompts:  64%|██████▍   | 134/210 [06:12<04:22,  3.45s/it]\u001b[A\n",
      "Processed prompts:  64%|██████▍   | 135/210 [06:13<03:28,  2.77s/it]\u001b[A\n",
      "Processed prompts:  65%|██████▍   | 136/210 [06:14<02:52,  2.33s/it]\u001b[A\n",
      "Processed prompts:  65%|██████▌   | 137/210 [06:19<03:37,  2.99s/it]\u001b[A\n",
      "Processed prompts:  66%|██████▌   | 138/210 [06:20<02:54,  2.42s/it]\u001b[A\n",
      "Processed prompts:  66%|██████▌   | 139/210 [06:22<02:43,  2.30s/it]\u001b[A\n",
      "Processed prompts:  67%|██████▋   | 140/210 [06:23<02:20,  2.01s/it]\u001b[A\n",
      "Processed prompts:  67%|██████▋   | 141/210 [06:25<02:05,  1.82s/it]\u001b[A\n",
      "Processed prompts:  68%|██████▊   | 142/210 [06:27<02:17,  2.02s/it]\u001b[A\n",
      "Processed prompts:  68%|██████▊   | 143/210 [06:28<01:58,  1.77s/it]\u001b[A\n",
      "Processed prompts:  69%|██████▊   | 144/210 [06:32<02:28,  2.25s/it]\u001b[A\n",
      "Processed prompts:  69%|██████▉   | 145/210 [06:33<02:11,  2.02s/it]\u001b[A\n",
      "Processed prompts:  70%|██████▉   | 146/210 [06:41<04:10,  3.92s/it]\u001b[A\n",
      "Processed prompts:  70%|███████   | 147/210 [06:43<03:18,  3.15s/it]\u001b[A\n",
      "Processed prompts:  70%|███████   | 148/210 [06:51<04:57,  4.79s/it]\u001b[A\n",
      "Processed prompts:  71%|███████   | 149/210 [06:53<03:45,  3.70s/it]\u001b[A\n",
      "Processed prompts:  71%|███████▏  | 150/210 [06:56<03:33,  3.56s/it]\u001b[A\n",
      "Processed prompts:  72%|███████▏  | 151/210 [06:57<02:53,  2.95s/it]\u001b[A\n",
      "Processed prompts:  72%|███████▏  | 152/210 [07:09<05:23,  5.59s/it]\u001b[A\n",
      "Processed prompts:  73%|███████▎  | 153/210 [07:19<06:34,  6.92s/it]\u001b[A\n",
      "Processed prompts:  73%|███████▎  | 154/210 [07:28<06:54,  7.40s/it]\u001b[A\n",
      "Processed prompts:  74%|███████▍  | 155/210 [07:34<06:33,  7.16s/it]\u001b[A\n",
      "Processed prompts:  74%|███████▍  | 156/210 [07:41<06:22,  7.08s/it]\u001b[A\n",
      "Processed prompts:  75%|███████▍  | 157/210 [07:42<04:34,  5.18s/it]\u001b[A\n",
      "Processed prompts:  75%|███████▌  | 158/210 [07:45<03:57,  4.57s/it]\u001b[A\n",
      "Processed prompts:  76%|███████▌  | 159/210 [07:46<02:58,  3.50s/it]\u001b[A\n",
      "Processed prompts:  76%|███████▌  | 160/210 [07:47<02:23,  2.88s/it]\u001b[A\n",
      "Processed prompts:  77%|███████▋  | 161/210 [07:50<02:16,  2.79s/it]\u001b[A\n",
      "Processed prompts:  77%|███████▋  | 162/210 [07:55<02:44,  3.43s/it]\u001b[A\n",
      "Processed prompts:  78%|███████▊  | 163/210 [07:59<02:53,  3.70s/it]\u001b[A\n",
      "Processed prompts:  78%|███████▊  | 164/210 [08:02<02:31,  3.30s/it]\u001b[A\n",
      "Processed prompts:  79%|███████▊  | 165/210 [08:03<01:58,  2.62s/it]\u001b[A\n",
      "Processed prompts:  79%|███████▉  | 166/210 [08:04<01:38,  2.24s/it]\u001b[A\n",
      "Processed prompts:  80%|███████▉  | 167/210 [08:06<01:30,  2.11s/it]\u001b[A\n",
      "Processed prompts:  80%|████████  | 168/210 [08:08<01:29,  2.13s/it]\u001b[A\n",
      "Processed prompts:  80%|████████  | 169/210 [08:09<01:11,  1.74s/it]\u001b[A\n",
      "Processed prompts:  81%|████████  | 170/210 [08:10<00:57,  1.43s/it]\u001b[A\n",
      "Processed prompts:  81%|████████▏ | 171/210 [08:10<00:45,  1.16s/it]\u001b[A\n",
      "Processed prompts:  82%|████████▏ | 172/210 [08:12<00:54,  1.43s/it]\u001b[A\n",
      "Processed prompts:  82%|████████▏ | 173/210 [08:14<00:58,  1.57s/it]\u001b[A\n",
      "Processed prompts:  83%|████████▎ | 174/210 [08:16<01:00,  1.69s/it]\u001b[A\n",
      "Processed prompts:  83%|████████▎ | 175/210 [08:18<01:01,  1.76s/it]\u001b[A\n",
      "Processed prompts:  84%|████████▍ | 176/210 [08:22<01:25,  2.52s/it]\u001b[A\n",
      "Processed prompts:  84%|████████▍ | 177/210 [08:25<01:20,  2.43s/it]\u001b[A\n",
      "Processed prompts:  85%|████████▍ | 178/210 [08:25<00:57,  1.80s/it]\u001b[A\n",
      "Processed prompts:  85%|████████▌ | 179/210 [08:27<00:59,  1.92s/it]\u001b[A\n",
      "Processed prompts:  86%|████████▌ | 180/210 [08:37<02:12,  4.40s/it]\u001b[A\n",
      "Processed prompts:  86%|████████▌ | 181/210 [08:38<01:35,  3.29s/it]\u001b[A\n",
      "Processed prompts:  87%|████████▋ | 182/210 [08:47<02:22,  5.09s/it]\u001b[A\n",
      "Processed prompts:  87%|████████▋ | 183/210 [08:52<02:16,  5.05s/it]\u001b[A\n",
      "Processed prompts:  88%|████████▊ | 184/210 [08:57<02:05,  4.83s/it]\u001b[A\n",
      "Processed prompts:  88%|████████▊ | 185/210 [08:59<01:46,  4.27s/it]\u001b[A\n",
      "Processed prompts:  89%|████████▊ | 186/210 [09:05<01:48,  4.52s/it]\u001b[A\n",
      "Processed prompts:  89%|████████▉ | 187/210 [09:11<01:53,  4.95s/it]\u001b[A\n",
      "Processed prompts:  90%|████████▉ | 188/210 [09:17<02:00,  5.49s/it]\u001b[A\n",
      "Processed prompts:  90%|█████████ | 189/210 [09:21<01:45,  5.03s/it]\u001b[A\n",
      "Processed prompts:  90%|█████████ | 190/210 [09:26<01:39,  4.98s/it]\u001b[A\n",
      "Processed prompts:  91%|█████████ | 191/210 [09:30<01:26,  4.56s/it]\u001b[A\n",
      "Processed prompts:  91%|█████████▏| 192/210 [09:34<01:21,  4.52s/it]\u001b[A\n",
      "Processed prompts:  92%|█████████▏| 193/210 [09:35<01:00,  3.58s/it]\u001b[A\n",
      "Processed prompts:  92%|█████████▏| 194/210 [09:39<00:55,  3.48s/it]\u001b[A\n",
      "Processed prompts:  93%|█████████▎| 195/210 [09:39<00:39,  2.61s/it]\u001b[A\n",
      "Processed prompts:  93%|█████████▎| 196/210 [09:40<00:27,  1.99s/it]\u001b[A\n",
      "Processed prompts:  94%|█████████▍| 197/210 [09:41<00:21,  1.63s/it]\u001b[A\n",
      "Processed prompts:  94%|█████████▍| 198/210 [09:43<00:21,  1.80s/it]\u001b[A\n",
      "Processed prompts:  95%|█████████▍| 199/210 [09:43<00:15,  1.38s/it]\u001b[A\n",
      "Processed prompts:  95%|█████████▌| 200/210 [09:44<00:10,  1.09s/it]\u001b[A\n",
      "Processed prompts:  96%|█████████▌| 201/210 [09:45<00:09,  1.03s/it]\u001b[A\n",
      "Processed prompts:  96%|█████████▌| 202/210 [09:45<00:07,  1.10it/s]\u001b[A\n",
      "Processed prompts:  97%|█████████▋| 203/210 [09:46<00:06,  1.15it/s]\u001b[A\n",
      "Processed prompts:  97%|█████████▋| 204/210 [09:47<00:04,  1.21it/s]\u001b[A\n",
      "Processed prompts:  98%|█████████▊| 205/210 [09:51<00:09,  1.84s/it]\u001b[A\n",
      "Processed prompts:  98%|█████████▊| 206/210 [09:54<00:09,  2.34s/it]\u001b[A\n",
      "Processed prompts:  99%|█████████▊| 207/210 [09:56<00:06,  2.19s/it]\u001b[A\n",
      "Processed prompts:  99%|█████████▉| 208/210 [10:03<00:06,  3.48s/it]\u001b[A\n",
      "Processed prompts: 100%|█████████▉| 209/210 [10:08<00:03,  3.88s/it]\u001b[A\n",
      "Processed prompts: 100%|██████████| 210/210 [10:13<00:00,  2.92s/it]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6868bd70084c4e5fb441364d3a9d3279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  57%|█████▋    | 4/7 [40:41<30:58, 619.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] /root/autodl-fs/LRP/open_ended_data_generation/20251210_llama2_7b_base/20251210_LRP_generation_th_1_selected_LRP_kur_res_vi_zscore_open_ended.json\n",
      "****************************************\n",
      "Model: th_1_selected_LRP_kur_res_vi_zscore_margin_selected\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/210 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts:   0%|          | 1/210 [00:04<16:06,  4.62s/it]\u001b[A\n",
      "Processed prompts:   2%|▏         | 4/210 [00:04<03:16,  1.05it/s]\u001b[A\n",
      "Processed prompts:   3%|▎         | 6/210 [00:05<01:56,  1.75it/s]\u001b[A\n",
      "Processed prompts:   4%|▍         | 8/210 [00:05<01:16,  2.63it/s]\u001b[A\n",
      "Processed prompts:   5%|▍         | 10/210 [00:06<01:38,  2.02it/s]\u001b[A\n",
      "Processed prompts:   5%|▌         | 11/210 [00:06<01:24,  2.35it/s]\u001b[A\n",
      "Processed prompts:   6%|▌         | 13/210 [00:07<01:04,  3.07it/s]\u001b[A\n",
      "Processed prompts:   7%|▋         | 14/210 [00:07<00:56,  3.44it/s]\u001b[A\n",
      "Processed prompts:   7%|▋         | 15/210 [00:07<00:57,  3.40it/s]\u001b[A\n",
      "Processed prompts:   8%|▊         | 17/210 [00:08<01:09,  2.77it/s]\u001b[A\n",
      "Processed prompts:   9%|▉         | 19/210 [00:08<00:52,  3.67it/s]\u001b[A\n",
      "Processed prompts:  10%|▉         | 20/210 [00:09<00:58,  3.27it/s]\u001b[A\n",
      "Processed prompts:  10%|█         | 21/210 [00:09<01:14,  2.53it/s]\u001b[A\n",
      "Processed prompts:  10%|█         | 22/210 [00:09<01:03,  2.96it/s]\u001b[A\n",
      "Processed prompts:  11%|█         | 23/210 [00:10<00:58,  3.21it/s]\u001b[A\n",
      "Processed prompts:  11%|█▏        | 24/210 [00:10<01:09,  2.66it/s]\u001b[A\n",
      "Processed prompts:  12%|█▏        | 26/210 [00:10<00:45,  4.06it/s]\u001b[A\n",
      "Processed prompts:  13%|█▎        | 27/210 [00:11<00:41,  4.45it/s]\u001b[A\n",
      "Processed prompts:  14%|█▍        | 29/210 [00:11<00:38,  4.71it/s]\u001b[A\n",
      "Processed prompts:  14%|█▍        | 30/210 [00:11<00:35,  5.02it/s]\u001b[A\n",
      "Processed prompts:  15%|█▍        | 31/210 [00:12<01:15,  2.36it/s]\u001b[A\n",
      "Processed prompts:  15%|█▌        | 32/210 [00:12<01:02,  2.83it/s]\u001b[A\n",
      "Processed prompts:  16%|█▌        | 33/210 [00:13<01:06,  2.64it/s]\u001b[A\n",
      "Processed prompts:  16%|█▌        | 34/210 [00:13<01:16,  2.29it/s]\u001b[A\n",
      "Processed prompts:  17%|█▋        | 35/210 [00:14<01:05,  2.66it/s]\u001b[A\n",
      "Processed prompts:  17%|█▋        | 36/210 [00:14<00:57,  3.02it/s]\u001b[A\n",
      "Processed prompts:  18%|█▊        | 37/210 [00:17<03:02,  1.05s/it]\u001b[A\n",
      "Processed prompts:  18%|█▊        | 38/210 [00:17<02:40,  1.07it/s]\u001b[A\n",
      "Processed prompts:  19%|█▊        | 39/210 [00:18<02:21,  1.21it/s]\u001b[A\n",
      "Processed prompts:  19%|█▉        | 40/210 [00:19<02:56,  1.04s/it]\u001b[A\n",
      "Processed prompts:  20%|█▉        | 41/210 [00:22<03:56,  1.40s/it]\u001b[A\n",
      "Processed prompts:  20%|██        | 42/210 [00:23<03:27,  1.24s/it]\u001b[A\n",
      "Processed prompts:  20%|██        | 43/210 [00:24<03:25,  1.23s/it]\u001b[A\n",
      "Processed prompts:  21%|██        | 44/210 [00:26<04:32,  1.64s/it]\u001b[A\n",
      "Processed prompts:  21%|██▏       | 45/210 [00:28<04:11,  1.53s/it]\u001b[A\n",
      "Processed prompts:  22%|██▏       | 46/210 [00:28<03:28,  1.27s/it]\u001b[A\n",
      "Processed prompts:  22%|██▏       | 47/210 [00:29<03:11,  1.17s/it]\u001b[A\n",
      "Processed prompts:  23%|██▎       | 48/210 [00:32<04:26,  1.65s/it]\u001b[A\n",
      "Processed prompts:  23%|██▎       | 49/210 [00:33<03:30,  1.31s/it]\u001b[A\n",
      "Processed prompts:  24%|██▍       | 50/210 [00:35<04:15,  1.60s/it]\u001b[A\n",
      "Processed prompts:  24%|██▍       | 51/210 [00:39<06:10,  2.33s/it]\u001b[A\n",
      "Processed prompts:  25%|██▍       | 52/210 [00:52<15:01,  5.70s/it]\u001b[A\n",
      "Processed prompts:  25%|██▌       | 53/210 [01:56<1:00:32, 23.14s/it]\u001b[A\n",
      "Processed prompts:  32%|███▏      | 67/210 [02:02<08:45,  3.68s/it]  \u001b[A\n",
      "Processed prompts:  32%|███▏      | 68/210 [02:05<08:34,  3.63s/it]\u001b[A\n",
      "Processed prompts:  33%|███▎      | 69/210 [02:10<08:43,  3.71s/it]\u001b[A\n",
      "Processed prompts:  33%|███▎      | 70/210 [02:16<09:27,  4.05s/it]\u001b[A\n",
      "Processed prompts:  34%|███▍      | 71/210 [02:22<10:00,  4.32s/it]\u001b[A\n",
      "Processed prompts:  34%|███▍      | 72/210 [02:27<10:09,  4.42s/it]\u001b[A\n",
      "Processed prompts:  35%|███▍      | 73/210 [02:32<10:13,  4.48s/it]\u001b[A\n",
      "Processed prompts:  35%|███▌      | 74/210 [02:36<09:59,  4.41s/it]\u001b[A\n",
      "Processed prompts:  36%|███▌      | 75/210 [02:37<08:19,  3.70s/it]\u001b[A\n",
      "Processed prompts:  36%|███▌      | 76/210 [02:40<07:34,  3.39s/it]\u001b[A\n",
      "Processed prompts:  37%|███▋      | 77/210 [02:43<07:41,  3.47s/it]\u001b[A\n",
      "Processed prompts:  37%|███▋      | 78/210 [02:49<09:01,  4.10s/it]\u001b[A\n",
      "Processed prompts:  38%|███▊      | 79/210 [02:52<08:05,  3.71s/it]\u001b[A\n",
      "Processed prompts:  38%|███▊      | 80/210 [02:52<05:57,  2.75s/it]\u001b[A\n",
      "Processed prompts:  39%|███▊      | 81/210 [02:55<06:11,  2.88s/it]\u001b[A\n",
      "Processed prompts:  39%|███▉      | 82/210 [02:57<05:24,  2.53s/it]\u001b[A\n",
      "Processed prompts:  40%|███▉      | 83/210 [02:59<05:08,  2.43s/it]\u001b[A\n",
      "Processed prompts:  40%|████      | 84/210 [03:01<04:50,  2.30s/it]\u001b[A\n",
      "Processed prompts:  40%|████      | 85/210 [03:03<04:11,  2.01s/it]\u001b[A\n",
      "Processed prompts:  41%|████      | 86/210 [03:03<03:24,  1.65s/it]\u001b[A\n",
      "Processed prompts:  41%|████▏     | 87/210 [03:05<03:35,  1.75s/it]\u001b[A\n",
      "Processed prompts:  42%|████▏     | 88/210 [03:07<03:37,  1.78s/it]\u001b[A\n",
      "Processed prompts:  42%|████▏     | 89/210 [03:10<04:25,  2.20s/it]\u001b[A\n",
      "Processed prompts:  43%|████▎     | 90/210 [03:13<04:23,  2.20s/it]\u001b[A\n",
      "Processed prompts:  43%|████▎     | 91/210 [03:13<03:26,  1.73s/it]\u001b[A\n",
      "Processed prompts:  44%|████▍     | 92/210 [03:15<03:25,  1.74s/it]\u001b[A\n",
      "Processed prompts:  44%|████▍     | 93/210 [03:16<02:52,  1.48s/it]\u001b[A\n",
      "Processed prompts:  45%|████▍     | 94/210 [03:17<02:30,  1.30s/it]\u001b[A\n",
      "Processed prompts:  45%|████▌     | 95/210 [03:21<04:12,  2.20s/it]\u001b[A\n",
      "Processed prompts:  46%|████▌     | 96/210 [03:27<06:16,  3.30s/it]\u001b[A\n",
      "Processed prompts:  46%|████▌     | 97/210 [03:31<06:20,  3.37s/it]\u001b[A\n",
      "Processed prompts:  47%|████▋     | 98/210 [03:42<10:37,  5.69s/it]\u001b[A\n",
      "Processed prompts:  47%|████▋     | 99/210 [03:53<13:42,  7.41s/it]\u001b[A\n",
      "Processed prompts:  48%|████▊     | 100/210 [03:56<10:56,  5.97s/it]\u001b[A\n",
      "Processed prompts:  48%|████▊     | 101/210 [03:58<08:42,  4.80s/it]\u001b[A\n",
      "Processed prompts:  49%|████▊     | 102/210 [03:59<06:49,  3.79s/it]\u001b[A\n",
      "Processed prompts:  49%|████▉     | 103/210 [04:01<05:50,  3.28s/it]\u001b[A\n",
      "Processed prompts:  50%|████▉     | 104/210 [04:06<06:28,  3.66s/it]\u001b[A\n",
      "Processed prompts:  50%|█████     | 105/210 [04:08<05:34,  3.19s/it]\u001b[A\n",
      "Processed prompts:  50%|█████     | 106/210 [04:16<08:05,  4.67s/it]\u001b[A\n",
      "Processed prompts:  51%|█████     | 107/210 [04:17<06:15,  3.64s/it]\u001b[A\n",
      "Processed prompts:  51%|█████▏    | 108/210 [04:24<07:58,  4.69s/it]\u001b[A\n",
      "Processed prompts:  52%|█████▏    | 109/210 [04:27<07:01,  4.17s/it]\u001b[A\n",
      "Processed prompts:  52%|█████▏    | 110/210 [04:28<05:25,  3.26s/it]\u001b[A\n",
      "Processed prompts:  53%|█████▎    | 111/210 [04:32<05:17,  3.20s/it]\u001b[A\n",
      "Processed prompts:  53%|█████▎    | 112/210 [04:32<04:01,  2.47s/it]\u001b[A\n",
      "Processed prompts:  54%|█████▍    | 113/210 [04:33<03:12,  1.99s/it]\u001b[A\n",
      "Processed prompts:  54%|█████▍    | 114/210 [04:35<03:03,  1.91s/it]\u001b[A\n",
      "Processed prompts:  55%|█████▍    | 115/210 [04:37<03:00,  1.90s/it]\u001b[A\n",
      "Processed prompts:  55%|█████▌    | 116/210 [04:38<02:28,  1.58s/it]\u001b[A\n",
      "Processed prompts:  56%|█████▌    | 117/210 [04:40<02:37,  1.70s/it]\u001b[A\n",
      "Processed prompts:  56%|█████▌    | 118/210 [04:41<02:40,  1.74s/it]\u001b[A\n",
      "Processed prompts:  57%|█████▋    | 119/210 [04:43<02:31,  1.66s/it]\u001b[A\n",
      "Processed prompts:  57%|█████▋    | 120/210 [04:46<02:57,  1.98s/it]\u001b[A\n",
      "Processed prompts:  58%|█████▊    | 121/210 [04:46<02:16,  1.53s/it]\u001b[A\n",
      "Processed prompts:  58%|█████▊    | 122/210 [04:46<01:43,  1.18s/it]\u001b[A\n",
      "Processed prompts:  59%|█████▊    | 123/210 [04:51<03:21,  2.31s/it]\u001b[A\n",
      "Processed prompts:  60%|█████▉    | 125/210 [04:53<02:21,  1.66s/it]\u001b[A\n",
      "Processed prompts:  60%|██████    | 126/210 [04:56<02:53,  2.06s/it]\u001b[A\n",
      "Processed prompts:  60%|██████    | 127/210 [04:59<02:53,  2.09s/it]\u001b[A\n",
      "Processed prompts:  61%|██████    | 128/210 [05:00<02:31,  1.85s/it]\u001b[A\n",
      "Processed prompts:  61%|██████▏   | 129/210 [05:04<03:29,  2.58s/it]\u001b[A\n",
      "Processed prompts:  62%|██████▏   | 130/210 [05:11<04:57,  3.71s/it]\u001b[A\n",
      "Processed prompts:  62%|██████▏   | 131/210 [05:14<04:45,  3.62s/it]\u001b[A\n",
      "Processed prompts:  63%|██████▎   | 132/210 [05:21<05:44,  4.42s/it]\u001b[A\n",
      "Processed prompts:  63%|██████▎   | 133/210 [05:23<05:01,  3.91s/it]\u001b[A\n",
      "Processed prompts:  64%|██████▍   | 134/210 [05:25<04:08,  3.26s/it]\u001b[A\n",
      "Processed prompts:  64%|██████▍   | 135/210 [05:34<06:11,  4.95s/it]\u001b[A\n",
      "Processed prompts:  65%|██████▍   | 136/210 [05:36<04:57,  4.02s/it]\u001b[A\n",
      "Processed prompts:  65%|██████▌   | 137/210 [05:52<09:14,  7.60s/it]\u001b[A\n",
      "Processed prompts:  66%|██████▌   | 138/210 [05:58<08:39,  7.21s/it]\u001b[A\n",
      "Processed prompts:  66%|██████▌   | 139/210 [06:00<06:36,  5.59s/it]\u001b[A\n",
      "Processed prompts:  67%|██████▋   | 140/210 [06:07<06:56,  5.94s/it]\u001b[A\n",
      "Processed prompts:  67%|██████▋   | 141/210 [06:09<05:36,  4.88s/it]\u001b[A\n",
      "Processed prompts:  68%|██████▊   | 142/210 [06:11<04:27,  3.93s/it]\u001b[A\n",
      "Processed prompts:  68%|██████▊   | 143/210 [06:13<03:41,  3.30s/it]\u001b[A\n",
      "Processed prompts:  69%|██████▊   | 144/210 [06:14<03:00,  2.74s/it]\u001b[A\n",
      "Processed prompts:  69%|██████▉   | 145/210 [06:15<02:30,  2.32s/it]\u001b[A\n",
      "Processed prompts:  70%|██████▉   | 146/210 [06:17<02:08,  2.01s/it]\u001b[A\n",
      "Processed prompts:  70%|███████   | 147/210 [06:18<01:51,  1.77s/it]\u001b[A\n",
      "Processed prompts:  70%|███████   | 148/210 [06:18<01:28,  1.42s/it]\u001b[A\n",
      "Processed prompts:  71%|███████   | 149/210 [06:20<01:26,  1.42s/it]\u001b[A\n",
      "Processed prompts:  71%|███████▏  | 150/210 [06:21<01:11,  1.19s/it]\u001b[A\n",
      "Processed prompts:  72%|███████▏  | 151/210 [06:22<01:16,  1.30s/it]\u001b[A\n",
      "Processed prompts:  72%|███████▏  | 152/210 [06:24<01:32,  1.60s/it]\u001b[A\n",
      "Processed prompts:  73%|███████▎  | 153/210 [06:25<01:17,  1.36s/it]\u001b[A\n",
      "Processed prompts:  73%|███████▎  | 154/210 [06:26<01:01,  1.10s/it]\u001b[A\n",
      "Processed prompts:  74%|███████▍  | 155/210 [06:27<01:05,  1.19s/it]\u001b[A\n",
      "Processed prompts:  75%|███████▍  | 157/210 [06:27<00:37,  1.40it/s]\u001b[A\n",
      "Processed prompts:  75%|███████▌  | 158/210 [06:29<00:46,  1.13it/s]\u001b[A\n",
      "Processed prompts:  76%|███████▌  | 159/210 [06:29<00:41,  1.23it/s]\u001b[A\n",
      "Processed prompts:  76%|███████▌  | 160/210 [06:39<02:39,  3.19s/it]\u001b[A\n",
      "Processed prompts:  77%|███████▋  | 161/210 [06:40<02:05,  2.57s/it]\u001b[A\n",
      "Processed prompts:  77%|███████▋  | 162/210 [06:44<02:28,  3.09s/it]\u001b[A\n",
      "Processed prompts:  78%|███████▊  | 163/210 [06:47<02:16,  2.91s/it]\u001b[A\n",
      "Processed prompts:  78%|███████▊  | 164/210 [06:54<03:06,  4.06s/it]\u001b[A\n",
      "Processed prompts:  79%|███████▊  | 165/210 [07:06<04:58,  6.64s/it]\u001b[A\n",
      "Processed prompts:  79%|███████▉  | 166/210 [07:09<03:54,  5.34s/it]\u001b[A\n",
      "Processed prompts:  80%|███████▉  | 167/210 [07:09<02:44,  3.83s/it]\u001b[A\n",
      "Processed prompts:  80%|████████  | 168/210 [07:20<04:05,  5.83s/it]\u001b[A\n",
      "Processed prompts:  80%|████████  | 169/210 [07:22<03:16,  4.79s/it]\u001b[A\n",
      "Processed prompts:  81%|████████  | 170/210 [07:27<03:13,  4.83s/it]\u001b[A\n",
      "Processed prompts:  81%|████████▏ | 171/210 [07:34<03:37,  5.57s/it]\u001b[A\n",
      "Processed prompts:  82%|████████▏ | 172/210 [07:37<02:59,  4.72s/it]\u001b[A\n",
      "Processed prompts:  82%|████████▏ | 173/210 [07:40<02:34,  4.18s/it]\u001b[A\n",
      "Processed prompts:  83%|████████▎ | 174/210 [07:44<02:29,  4.15s/it]\u001b[A\n",
      "Processed prompts:  83%|████████▎ | 175/210 [07:47<02:17,  3.94s/it]\u001b[A\n",
      "Processed prompts:  84%|████████▍ | 176/210 [07:50<01:58,  3.50s/it]\u001b[A\n",
      "Processed prompts:  84%|████████▍ | 177/210 [07:53<01:56,  3.54s/it]\u001b[A\n",
      "Processed prompts:  85%|████████▍ | 178/210 [07:57<01:56,  3.65s/it]\u001b[A\n",
      "Processed prompts:  85%|████████▌ | 179/210 [07:59<01:36,  3.12s/it]\u001b[A\n",
      "Processed prompts:  86%|████████▌ | 180/210 [08:01<01:23,  2.79s/it]\u001b[A\n",
      "Processed prompts:  86%|████████▌ | 181/210 [08:03<01:14,  2.57s/it]\u001b[A\n",
      "Processed prompts:  87%|████████▋ | 182/210 [08:06<01:15,  2.68s/it]\u001b[A\n",
      "Processed prompts:  87%|████████▋ | 183/210 [08:07<00:53,  1.99s/it]\u001b[A\n",
      "Processed prompts:  88%|████████▊ | 184/210 [08:07<00:39,  1.51s/it]\u001b[A\n",
      "Processed prompts:  88%|████████▊ | 185/210 [08:08<00:37,  1.50s/it]\u001b[A\n",
      "Processed prompts:  89%|████████▊ | 186/210 [08:09<00:29,  1.23s/it]\u001b[A\n",
      "Processed prompts:  89%|████████▉ | 187/210 [08:09<00:20,  1.12it/s]\u001b[A\n",
      "Processed prompts:  90%|█████████ | 189/210 [08:09<00:11,  1.83it/s]\u001b[A\n",
      "Processed prompts:  90%|█████████ | 190/210 [08:13<00:25,  1.29s/it]\u001b[A\n",
      "Processed prompts:  91%|█████████ | 191/210 [08:27<01:29,  4.73s/it]\u001b[A\n",
      "Processed prompts:  91%|█████████▏| 192/210 [08:37<01:50,  6.15s/it]\u001b[A\n",
      "Processed prompts:  92%|█████████▏| 193/210 [08:44<01:47,  6.34s/it]\u001b[A\n",
      "Processed prompts:  92%|█████████▏| 194/210 [08:48<01:30,  5.67s/it]\u001b[A\n",
      "Processed prompts:  93%|█████████▎| 195/210 [08:53<01:21,  5.45s/it]\u001b[A\n",
      "Processed prompts:  93%|█████████▎| 196/210 [08:57<01:08,  4.87s/it]\u001b[A\n",
      "Processed prompts:  94%|█████████▍| 197/210 [09:00<00:59,  4.56s/it]\u001b[A\n",
      "Processed prompts:  94%|█████████▍| 198/210 [09:03<00:47,  3.92s/it]\u001b[A\n",
      "Processed prompts:  95%|█████████▍| 199/210 [09:06<00:41,  3.77s/it]\u001b[A\n",
      "Processed prompts:  95%|█████████▌| 200/210 [09:09<00:34,  3.47s/it]\u001b[A\n",
      "Processed prompts:  96%|█████████▌| 201/210 [09:11<00:28,  3.13s/it]\u001b[A\n",
      "Processed prompts:  96%|█████████▌| 202/210 [09:14<00:23,  2.94s/it]\u001b[A\n",
      "Processed prompts:  97%|█████████▋| 203/210 [09:15<00:16,  2.32s/it]\u001b[A\n",
      "Processed prompts:  97%|█████████▋| 204/210 [09:16<00:11,  1.91s/it]\u001b[A\n",
      "Processed prompts:  98%|█████████▊| 205/210 [09:17<00:08,  1.78s/it]\u001b[A\n",
      "Processed prompts:  98%|█████████▊| 206/210 [09:18<00:05,  1.45s/it]\u001b[A\n",
      "Processed prompts:  99%|█████████▊| 207/210 [09:18<00:03,  1.17s/it]\u001b[A\n",
      "Processed prompts:  99%|█████████▉| 208/210 [09:20<00:02,  1.28s/it]\u001b[A\n",
      "Processed prompts: 100%|█████████▉| 209/210 [09:21<00:01,  1.27s/it]\u001b[A\n",
      "Processed prompts: 100%|██████████| 210/210 [09:22<00:00,  2.68s/it]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b308be0754cc4f01ad849317cf62a04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  71%|███████▏  | 5/7 [50:03<19:57, 598.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] /root/autodl-fs/LRP/open_ended_data_generation/20251210_llama2_7b_base/20251210_LRP_generation_th_1_selected_LRP_kur_res_vi_zscore_margin_selected_open_ended.json\n",
      "****************************************\n",
      "Model: th_1_selected_LRP_kur_res_zh_zscore\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/210 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts:   0%|          | 1/210 [00:03<13:27,  3.86s/it]\u001b[A\n",
      "Processed prompts:   1%|          | 2/210 [00:04<06:31,  1.88s/it]\u001b[A\n",
      "Processed prompts:   1%|▏         | 3/210 [00:04<03:45,  1.09s/it]\u001b[A\n",
      "Processed prompts:   2%|▏         | 5/210 [00:04<01:49,  1.87it/s]\u001b[A\n",
      "Processed prompts:   3%|▎         | 6/210 [00:04<01:26,  2.36it/s]\u001b[A\n",
      "Processed prompts:   3%|▎         | 7/210 [00:05<01:09,  2.92it/s]\u001b[A\n",
      "Processed prompts:   4%|▍         | 8/210 [00:05<00:57,  3.49it/s]\u001b[A\n",
      "Processed prompts:   4%|▍         | 9/210 [00:06<01:49,  1.83it/s]\u001b[A\n",
      "Processed prompts:   5%|▍         | 10/210 [00:06<01:43,  1.93it/s]\u001b[A\n",
      "Processed prompts:   5%|▌         | 11/210 [00:07<01:35,  2.09it/s]\u001b[A\n",
      "Processed prompts:   6%|▌         | 12/210 [00:09<03:21,  1.02s/it]\u001b[A\n",
      "Processed prompts:   6%|▌         | 13/210 [00:10<03:02,  1.08it/s]\u001b[A\n",
      "Processed prompts:   7%|▋         | 14/210 [00:10<02:30,  1.30it/s]\u001b[A\n",
      "Processed prompts:   7%|▋         | 15/210 [00:11<02:25,  1.34it/s]\u001b[A\n",
      "Processed prompts:   8%|▊         | 17/210 [00:12<01:55,  1.68it/s]\u001b[A\n",
      "Processed prompts:   9%|▊         | 18/210 [00:12<01:43,  1.86it/s]\u001b[A\n",
      "Processed prompts:   9%|▉         | 19/210 [00:13<02:00,  1.58it/s]\u001b[A\n",
      "Processed prompts:  10%|▉         | 20/210 [00:16<04:02,  1.28s/it]\u001b[A\n",
      "Processed prompts:  10%|█         | 21/210 [00:18<05:03,  1.61s/it]\u001b[A\n",
      "Processed prompts:  10%|█         | 22/210 [00:20<04:54,  1.57s/it]\u001b[A\n",
      "Processed prompts:  11%|█         | 23/210 [00:21<04:46,  1.53s/it]\u001b[A\n",
      "Processed prompts:  11%|█▏        | 24/210 [00:24<05:32,  1.79s/it]\u001b[A\n",
      "Processed prompts:  12%|█▏        | 25/210 [00:26<05:59,  1.94s/it]\u001b[A\n",
      "Processed prompts:  12%|█▏        | 26/210 [00:34<11:13,  3.66s/it]\u001b[A\n",
      "Processed prompts:  13%|█▎        | 27/210 [00:39<12:51,  4.22s/it]\u001b[A\n",
      "Processed prompts:  13%|█▎        | 28/210 [00:40<09:37,  3.17s/it]\u001b[A\n",
      "Processed prompts:  14%|█▍        | 29/210 [00:40<06:59,  2.32s/it]\u001b[A\n",
      "Processed prompts:  14%|█▍        | 30/210 [00:41<05:55,  1.98s/it]\u001b[A\n",
      "Processed prompts:  15%|█▍        | 31/210 [00:43<05:21,  1.80s/it]\u001b[A\n",
      "Processed prompts:  15%|█▌        | 32/210 [00:45<05:54,  1.99s/it]\u001b[A\n",
      "Processed prompts:  16%|█▌        | 33/210 [00:53<10:45,  3.65s/it]\u001b[A\n",
      "Processed prompts:  16%|█▌        | 34/210 [00:55<09:32,  3.26s/it]\u001b[A\n",
      "Processed prompts:  17%|█▋        | 35/210 [01:00<10:44,  3.68s/it]\u001b[A\n",
      "Processed prompts:  17%|█▋        | 36/210 [01:27<31:33, 10.88s/it]\u001b[A\n",
      "Processed prompts:  18%|█▊        | 37/210 [01:45<36:54, 12.80s/it]\u001b[A\n",
      "Processed prompts:  18%|█▊        | 38/210 [01:50<30:02, 10.48s/it]\u001b[A\n",
      "Processed prompts:  19%|█▊        | 39/210 [01:51<21:56,  7.70s/it]\u001b[A\n",
      "Processed prompts:  19%|█▉        | 40/210 [01:58<20:54,  7.38s/it]\u001b[A\n",
      "Processed prompts:  25%|██▌       | 53/210 [02:03<03:55,  1.50s/it]\u001b[A\n",
      "Processed prompts:  26%|██▌       | 54/210 [02:15<06:02,  2.32s/it]\u001b[A\n",
      "Processed prompts:  26%|██▌       | 55/210 [02:16<05:43,  2.22s/it]\u001b[A\n",
      "Processed prompts:  27%|██▋       | 56/210 [02:16<05:06,  1.99s/it]\u001b[A\n",
      "Processed prompts:  27%|██▋       | 57/210 [02:28<08:50,  3.47s/it]\u001b[A\n",
      "Processed prompts:  28%|██▊       | 58/210 [02:34<10:11,  4.02s/it]\u001b[A\n",
      "Processed prompts:  28%|██▊       | 59/210 [02:40<11:05,  4.41s/it]\u001b[A\n",
      "Processed prompts:  29%|██▊       | 60/210 [02:44<10:33,  4.23s/it]\u001b[A\n",
      "Processed prompts:  29%|██▉       | 61/210 [02:47<09:55,  4.00s/it]\u001b[A\n",
      "Processed prompts:  30%|██▉       | 62/210 [02:50<09:14,  3.75s/it]\u001b[A\n",
      "Processed prompts:  30%|███       | 63/210 [02:53<08:31,  3.48s/it]\u001b[A\n",
      "Processed prompts:  30%|███       | 64/210 [02:55<07:47,  3.20s/it]\u001b[A\n",
      "Processed prompts:  31%|███       | 65/210 [02:58<07:17,  3.02s/it]\u001b[A\n",
      "Processed prompts:  31%|███▏      | 66/210 [03:00<06:42,  2.80s/it]\u001b[A\n",
      "Processed prompts:  32%|███▏      | 67/210 [03:02<06:16,  2.63s/it]\u001b[A\n",
      "Processed prompts:  32%|███▏      | 68/210 [03:05<06:25,  2.71s/it]\u001b[A\n",
      "Processed prompts:  33%|███▎      | 69/210 [03:06<04:47,  2.04s/it]\u001b[A\n",
      "Processed prompts:  33%|███▎      | 70/210 [03:10<06:30,  2.79s/it]\u001b[A\n",
      "Processed prompts:  34%|███▍      | 71/210 [03:15<07:46,  3.36s/it]\u001b[A\n",
      "Processed prompts:  34%|███▍      | 72/210 [03:16<06:18,  2.74s/it]\u001b[A\n",
      "Processed prompts:  35%|███▍      | 73/210 [03:17<05:16,  2.31s/it]\u001b[A\n",
      "Processed prompts:  35%|███▌      | 74/210 [03:19<04:35,  2.03s/it]\u001b[A\n",
      "Processed prompts:  36%|███▌      | 75/210 [03:20<04:06,  1.82s/it]\u001b[A\n",
      "Processed prompts:  36%|███▌      | 76/210 [03:22<04:20,  1.94s/it]\u001b[A\n",
      "Processed prompts:  37%|███▋      | 77/210 [03:23<03:46,  1.70s/it]\u001b[A\n",
      "Processed prompts:  37%|███▋      | 78/210 [03:31<07:30,  3.41s/it]\u001b[A\n",
      "Processed prompts:  38%|███▊      | 79/210 [03:33<06:23,  2.93s/it]\u001b[A\n",
      "Processed prompts:  38%|███▊      | 80/210 [03:48<14:36,  6.74s/it]\u001b[A\n",
      "Processed prompts:  39%|███▊      | 81/210 [03:53<13:02,  6.07s/it]\u001b[A\n",
      "Processed prompts:  39%|███▉      | 82/210 [03:59<12:51,  6.03s/it]\u001b[A\n",
      "Processed prompts:  40%|███▉      | 83/210 [04:05<12:57,  6.12s/it]\u001b[A\n",
      "Processed prompts:  40%|████      | 84/210 [04:19<17:43,  8.44s/it]\u001b[A\n",
      "Processed prompts:  40%|████      | 85/210 [04:21<13:36,  6.53s/it]\u001b[A\n",
      "Processed prompts:  41%|████      | 86/210 [04:24<11:30,  5.57s/it]\u001b[A\n",
      "Processed prompts:  41%|████▏     | 87/210 [04:26<08:49,  4.30s/it]\u001b[A\n",
      "Processed prompts:  42%|████▏     | 88/210 [04:28<07:43,  3.80s/it]\u001b[A\n",
      "Processed prompts:  42%|████▏     | 89/210 [04:29<05:49,  2.89s/it]\u001b[A\n",
      "Processed prompts:  43%|████▎     | 90/210 [04:31<05:00,  2.50s/it]\u001b[A\n",
      "Processed prompts:  43%|████▎     | 91/210 [04:32<04:31,  2.28s/it]\u001b[A\n",
      "Processed prompts:  44%|████▍     | 92/210 [04:33<03:42,  1.89s/it]\u001b[A\n",
      "Processed prompts:  44%|████▍     | 93/210 [04:36<03:59,  2.05s/it]\u001b[A\n",
      "Processed prompts:  45%|████▍     | 94/210 [04:37<03:27,  1.79s/it]\u001b[A\n",
      "Processed prompts:  45%|████▌     | 95/210 [04:39<03:34,  1.86s/it]\u001b[A\n",
      "Processed prompts:  46%|████▌     | 96/210 [04:43<04:28,  2.35s/it]\u001b[A\n",
      "Processed prompts:  46%|████▌     | 97/210 [04:46<05:11,  2.76s/it]\u001b[A\n",
      "Processed prompts:  47%|████▋     | 98/210 [04:49<05:12,  2.79s/it]\u001b[A\n",
      "Processed prompts:  47%|████▋     | 99/210 [04:52<05:17,  2.86s/it]\u001b[A\n",
      "Processed prompts:  48%|████▊     | 100/210 [04:53<04:19,  2.36s/it]\u001b[A\n",
      "Processed prompts:  48%|████▊     | 101/210 [04:57<04:47,  2.63s/it]\u001b[A\n",
      "Processed prompts:  49%|████▊     | 102/210 [05:04<07:21,  4.09s/it]\u001b[A\n",
      "Processed prompts:  49%|████▉     | 103/210 [05:13<09:56,  5.57s/it]\u001b[A\n",
      "Processed prompts:  50%|████▉     | 104/210 [05:19<09:49,  5.56s/it]\u001b[A\n",
      "Processed prompts:  50%|█████     | 105/210 [05:33<14:34,  8.33s/it]\u001b[A\n",
      "Processed prompts:  50%|█████     | 106/210 [05:36<11:21,  6.55s/it]\u001b[A\n",
      "Processed prompts:  51%|█████     | 107/210 [05:41<10:19,  6.01s/it]\u001b[A\n",
      "Processed prompts:  51%|█████▏    | 108/210 [05:45<09:21,  5.51s/it]\u001b[A\n",
      "Processed prompts:  52%|█████▏    | 109/210 [05:49<08:21,  4.97s/it]\u001b[A\n",
      "Processed prompts:  52%|█████▏    | 110/210 [05:51<07:11,  4.32s/it]\u001b[A\n",
      "Processed prompts:  53%|█████▎    | 111/210 [05:54<06:26,  3.90s/it]\u001b[A\n",
      "Processed prompts:  53%|█████▎    | 112/210 [05:58<06:04,  3.72s/it]\u001b[A\n",
      "Processed prompts:  54%|█████▍    | 113/210 [05:58<04:25,  2.73s/it]\u001b[A\n",
      "Processed prompts:  54%|█████▍    | 114/210 [06:03<05:24,  3.38s/it]\u001b[A\n",
      "Processed prompts:  55%|█████▍    | 115/210 [06:06<05:03,  3.19s/it]\u001b[A\n",
      "Processed prompts:  55%|█████▌    | 116/210 [06:07<04:08,  2.64s/it]\u001b[A\n",
      "Processed prompts:  56%|█████▌    | 117/210 [06:09<03:34,  2.30s/it]\u001b[A\n",
      "Processed prompts:  56%|█████▌    | 118/210 [06:09<02:31,  1.65s/it]\u001b[A\n",
      "Processed prompts:  57%|█████▋    | 119/210 [06:12<03:03,  2.01s/it]\u001b[A\n",
      "Processed prompts:  57%|█████▋    | 120/210 [06:14<02:58,  1.99s/it]\u001b[A\n",
      "Processed prompts:  58%|█████▊    | 121/210 [06:15<02:40,  1.80s/it]\u001b[A\n",
      "Processed prompts:  58%|█████▊    | 122/210 [06:16<02:22,  1.62s/it]\u001b[A\n",
      "Processed prompts:  59%|█████▊    | 123/210 [06:19<03:07,  2.16s/it]\u001b[A\n",
      "Processed prompts:  59%|█████▉    | 124/210 [06:21<02:39,  1.86s/it]\u001b[A\n",
      "Processed prompts:  60%|█████▉    | 125/210 [06:22<02:26,  1.73s/it]\u001b[A\n",
      "Processed prompts:  60%|██████    | 126/210 [06:23<02:12,  1.58s/it]\u001b[A\n",
      "Processed prompts:  60%|██████    | 127/210 [06:25<02:07,  1.53s/it]\u001b[A\n",
      "Processed prompts:  61%|██████    | 128/210 [06:31<04:12,  3.08s/it]\u001b[A\n",
      "Processed prompts:  61%|██████▏   | 129/210 [06:44<08:11,  6.06s/it]\u001b[A\n",
      "Processed prompts:  62%|██████▏   | 130/210 [06:53<09:11,  6.89s/it]\u001b[A\n",
      "Processed prompts:  62%|██████▏   | 131/210 [07:09<12:33,  9.54s/it]\u001b[A\n",
      "Processed prompts:  63%|██████▎   | 132/210 [07:13<10:03,  7.73s/it]\u001b[A\n",
      "Processed prompts:  63%|██████▎   | 133/210 [07:17<08:30,  6.63s/it]\u001b[A\n",
      "Processed prompts:  64%|██████▍   | 134/210 [07:21<07:40,  6.06s/it]\u001b[A\n",
      "Processed prompts:  64%|██████▍   | 135/210 [07:27<07:29,  6.00s/it]\u001b[A\n",
      "Processed prompts:  65%|██████▍   | 136/210 [07:32<06:56,  5.62s/it]\u001b[A\n",
      "Processed prompts:  65%|██████▌   | 137/210 [07:42<08:35,  7.06s/it]\u001b[A\n",
      "Processed prompts:  66%|██████▌   | 138/210 [07:45<06:51,  5.71s/it]\u001b[A\n",
      "Processed prompts:  66%|██████▌   | 139/210 [07:49<06:10,  5.22s/it]\u001b[A\n",
      "Processed prompts:  67%|██████▋   | 140/210 [07:53<05:42,  4.90s/it]\u001b[A\n",
      "Processed prompts:  67%|██████▋   | 141/210 [07:55<04:36,  4.00s/it]\u001b[A\n",
      "Processed prompts:  68%|██████▊   | 142/210 [07:57<03:59,  3.52s/it]\u001b[A\n",
      "Processed prompts:  68%|██████▊   | 143/210 [07:58<02:54,  2.60s/it]\u001b[A\n",
      "Processed prompts:  69%|██████▊   | 144/210 [07:58<02:09,  1.96s/it]\u001b[A\n",
      "Processed prompts:  69%|██████▉   | 145/210 [07:59<01:37,  1.50s/it]\u001b[A\n",
      "Processed prompts:  70%|██████▉   | 146/210 [07:59<01:16,  1.20s/it]\u001b[A\n",
      "Processed prompts:  70%|███████   | 147/210 [08:00<01:11,  1.14s/it]\u001b[A\n",
      "Processed prompts:  70%|███████   | 148/210 [08:01<01:07,  1.08s/it]\u001b[A\n",
      "Processed prompts:  71%|███████   | 149/210 [08:02<01:00,  1.00it/s]\u001b[A\n",
      "Processed prompts:  71%|███████▏  | 150/210 [08:03<01:07,  1.13s/it]\u001b[A\n",
      "Processed prompts:  72%|███████▏  | 151/210 [08:04<00:49,  1.18it/s]\u001b[A\n",
      "Processed prompts:  72%|███████▏  | 152/210 [08:04<00:42,  1.37it/s]\u001b[A\n",
      "Processed prompts:  73%|███████▎  | 153/210 [08:04<00:32,  1.76it/s]\u001b[A\n",
      "Processed prompts:  73%|███████▎  | 154/210 [08:05<00:40,  1.37it/s]\u001b[A\n",
      "Processed prompts:  74%|███████▍  | 155/210 [08:16<03:20,  3.65s/it]\u001b[A\n",
      "Processed prompts:  74%|███████▍  | 156/210 [08:20<03:27,  3.84s/it]\u001b[A\n",
      "Processed prompts:  75%|███████▍  | 157/210 [08:21<02:29,  2.82s/it]\u001b[A\n",
      "Processed prompts:  75%|███████▌  | 158/210 [08:38<06:14,  7.21s/it]\u001b[A\n",
      "Processed prompts:  76%|███████▌  | 159/210 [08:52<07:46,  9.15s/it]\u001b[A\n",
      "Processed prompts:  76%|███████▌  | 160/210 [09:00<07:21,  8.83s/it]\u001b[A\n",
      "Processed prompts:  77%|███████▋  | 161/210 [09:04<06:08,  7.53s/it]\u001b[A\n",
      "Processed prompts:  77%|███████▋  | 162/210 [09:10<05:33,  6.95s/it]\u001b[A\n",
      "Processed prompts:  78%|███████▊  | 163/210 [09:14<04:47,  6.12s/it]\u001b[A\n",
      "Processed prompts:  78%|███████▊  | 164/210 [09:18<04:11,  5.46s/it]\u001b[A\n",
      "Processed prompts:  79%|███████▊  | 165/210 [09:21<03:27,  4.61s/it]\u001b[A\n",
      "Processed prompts:  79%|███████▉  | 166/210 [09:22<02:45,  3.75s/it]\u001b[A\n",
      "Processed prompts:  80%|███████▉  | 167/210 [09:23<02:05,  2.92s/it]\u001b[A\n",
      "Processed prompts:  80%|████████  | 168/210 [09:26<01:55,  2.76s/it]\u001b[A\n",
      "Processed prompts:  80%|████████  | 169/210 [09:27<01:35,  2.33s/it]\u001b[A\n",
      "Processed prompts:  81%|████████  | 170/210 [09:29<01:30,  2.25s/it]\u001b[A\n",
      "Processed prompts:  81%|████████▏ | 171/210 [09:31<01:23,  2.13s/it]\u001b[A\n",
      "Processed prompts:  82%|████████▏ | 172/210 [09:32<01:12,  1.90s/it]\u001b[A\n",
      "Processed prompts:  82%|████████▏ | 173/210 [09:34<01:05,  1.78s/it]\u001b[A\n",
      "Processed prompts:  83%|████████▎ | 174/210 [09:35<01:01,  1.70s/it]\u001b[A\n",
      "Processed prompts:  83%|████████▎ | 175/210 [09:37<00:54,  1.55s/it]\u001b[A\n",
      "Processed prompts:  84%|████████▍ | 176/210 [09:38<00:50,  1.49s/it]\u001b[A\n",
      "Processed prompts:  84%|████████▍ | 177/210 [09:40<00:52,  1.59s/it]\u001b[A\n",
      "Processed prompts:  85%|████████▍ | 178/210 [09:41<00:49,  1.54s/it]\u001b[A\n",
      "Processed prompts:  85%|████████▌ | 179/210 [09:44<01:01,  1.98s/it]\u001b[A\n",
      "Processed prompts:  86%|████████▌ | 180/210 [09:45<00:44,  1.50s/it]\u001b[A\n",
      "Processed prompts:  86%|████████▌ | 181/210 [09:50<01:17,  2.69s/it]\u001b[A\n",
      "Processed prompts:  87%|████████▋ | 182/210 [09:59<02:09,  4.63s/it]\u001b[A\n",
      "Processed prompts:  87%|████████▋ | 183/210 [10:12<03:14,  7.20s/it]\u001b[A\n",
      "Processed prompts:  88%|████████▊ | 184/210 [10:19<03:02,  7.03s/it]\u001b[A\n",
      "Processed prompts:  88%|████████▊ | 185/210 [10:32<03:42,  8.89s/it]\u001b[A\n",
      "Processed prompts:  89%|████████▊ | 186/210 [10:38<03:09,  7.88s/it]\u001b[A\n",
      "Processed prompts:  89%|████████▉ | 187/210 [10:40<02:21,  6.16s/it]\u001b[A\n",
      "Processed prompts:  90%|████████▉ | 188/210 [10:44<01:59,  5.43s/it]\u001b[A\n",
      "Processed prompts:  90%|█████████ | 189/210 [10:47<01:41,  4.82s/it]\u001b[A\n",
      "Processed prompts:  90%|█████████ | 190/210 [10:49<01:17,  3.90s/it]\u001b[A\n",
      "Processed prompts:  91%|█████████ | 191/210 [10:52<01:10,  3.70s/it]\u001b[A\n",
      "Processed prompts:  91%|█████████▏| 192/210 [10:54<00:55,  3.10s/it]\u001b[A\n",
      "Processed prompts:  92%|█████████▏| 193/210 [10:55<00:44,  2.60s/it]\u001b[A\n",
      "Processed prompts:  92%|█████████▏| 194/210 [10:57<00:38,  2.42s/it]\u001b[A\n",
      "Processed prompts:  93%|█████████▎| 195/210 [10:58<00:28,  1.91s/it]\u001b[A\n",
      "Processed prompts:  93%|█████████▎| 196/210 [10:59<00:24,  1.73s/it]\u001b[A\n",
      "Processed prompts:  94%|█████████▍| 197/210 [11:01<00:23,  1.80s/it]\u001b[A\n",
      "Processed prompts:  94%|█████████▍| 198/210 [11:02<00:19,  1.61s/it]\u001b[A\n",
      "Processed prompts:  95%|█████████▍| 199/210 [11:03<00:14,  1.34s/it]\u001b[A\n",
      "Processed prompts:  95%|█████████▌| 200/210 [11:04<00:13,  1.37s/it]\u001b[A\n",
      "Processed prompts:  96%|█████████▌| 201/210 [11:05<00:10,  1.20s/it]\u001b[A\n",
      "Processed prompts:  96%|█████████▌| 202/210 [11:06<00:08,  1.12s/it]\u001b[A\n",
      "Processed prompts:  97%|█████████▋| 203/210 [11:11<00:16,  2.30s/it]\u001b[A\n",
      "Processed prompts:  97%|█████████▋| 204/210 [11:17<00:19,  3.31s/it]\u001b[A\n",
      "Processed prompts:  98%|█████████▊| 205/210 [11:23<00:21,  4.28s/it]\u001b[A\n",
      "Processed prompts:  98%|█████████▊| 206/210 [11:29<00:19,  4.82s/it]\u001b[A\n",
      "Processed prompts:  99%|█████████▊| 207/210 [11:31<00:11,  3.98s/it]\u001b[A\n",
      "Processed prompts:  99%|█████████▉| 208/210 [11:34<00:06,  3.44s/it]\u001b[A\n",
      "Processed prompts: 100%|█████████▉| 209/210 [11:36<00:03,  3.01s/it]\u001b[A\n",
      "Processed prompts: 100%|██████████| 210/210 [11:37<00:00,  3.32s/it]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a661f1e2b144b2b64dd37437eb0453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress:  86%|████████▌ | 6/7 [1:01:41<10:32, 632.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] /root/autodl-fs/LRP/open_ended_data_generation/20251210_llama2_7b_base/20251210_LRP_generation_th_1_selected_LRP_kur_res_zh_zscore_open_ended.json\n",
      "****************************************\n",
      "Model: th_1_selected_LRP_kur_res_zh_zscore_margin_selected\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/210 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts:   0%|          | 1/210 [00:03<12:13,  3.51s/it]\u001b[A\n",
      "Processed prompts:   1%|          | 2/210 [00:04<06:44,  1.94s/it]\u001b[A\n",
      "Processed prompts:   1%|▏         | 3/210 [00:04<04:19,  1.25s/it]\u001b[A\n",
      "Processed prompts:   2%|▏         | 4/210 [00:05<03:00,  1.14it/s]\u001b[A\n",
      "Processed prompts:   2%|▏         | 5/210 [00:05<03:00,  1.14it/s]\u001b[A\n",
      "Processed prompts:   3%|▎         | 7/210 [00:06<01:39,  2.04it/s]\u001b[A\n",
      "Processed prompts:   4%|▍         | 8/210 [00:06<01:29,  2.27it/s]\u001b[A\n",
      "Processed prompts:   4%|▍         | 9/210 [00:08<02:32,  1.32it/s]\u001b[A\n",
      "Processed prompts:   5%|▍         | 10/210 [00:08<02:16,  1.47it/s]\u001b[A\n",
      "Processed prompts:   5%|▌         | 11/210 [00:08<01:54,  1.73it/s]\u001b[A\n",
      "Processed prompts:   7%|▋         | 14/210 [00:09<01:10,  2.76it/s]\u001b[A\n",
      "Processed prompts:   7%|▋         | 15/210 [00:09<01:15,  2.58it/s]\u001b[A\n",
      "Processed prompts:   8%|▊         | 16/210 [00:10<01:18,  2.46it/s]\u001b[A\n",
      "Processed prompts:   8%|▊         | 17/210 [00:11<02:12,  1.46it/s]\u001b[A\n",
      "Processed prompts:   9%|▉         | 19/210 [00:12<01:27,  2.19it/s]\u001b[A\n",
      "Processed prompts:  10%|▉         | 20/210 [00:12<01:17,  2.47it/s]\u001b[A\n",
      "Processed prompts:  10%|█         | 21/210 [00:12<01:22,  2.30it/s]\u001b[A\n",
      "Processed prompts:  10%|█         | 22/210 [00:13<01:29,  2.09it/s]\u001b[A\n",
      "Processed prompts:  11%|█         | 23/210 [00:14<01:31,  2.04it/s]\u001b[A\n",
      "Processed prompts:  11%|█▏        | 24/210 [00:14<01:50,  1.68it/s]\u001b[A\n",
      "Processed prompts:  12%|█▏        | 26/210 [00:15<01:36,  1.90it/s]\u001b[A\n",
      "Processed prompts:  13%|█▎        | 27/210 [00:16<01:41,  1.81it/s]\u001b[A\n",
      "Processed prompts:  13%|█▎        | 28/210 [00:17<02:12,  1.37it/s]\u001b[A\n",
      "Processed prompts:  14%|█▍        | 29/210 [00:18<01:55,  1.56it/s]\u001b[A\n",
      "Processed prompts:  14%|█▍        | 30/210 [00:18<01:36,  1.87it/s]\u001b[A\n",
      "Processed prompts:  15%|█▌        | 32/210 [00:18<01:07,  2.63it/s]\u001b[A\n",
      "Processed prompts:  16%|█▌        | 33/210 [00:23<04:26,  1.50s/it]\u001b[A\n",
      "Processed prompts:  16%|█▌        | 34/210 [00:24<03:36,  1.23s/it]\u001b[A\n",
      "Processed prompts:  17%|█▋        | 35/210 [00:24<03:03,  1.05s/it]\u001b[A\n",
      "Processed prompts:  18%|█▊        | 37/210 [00:25<01:50,  1.56it/s]\u001b[A\n",
      "Processed prompts:  18%|█▊        | 38/210 [00:26<02:35,  1.11it/s]\u001b[A\n",
      "Processed prompts:  19%|█▉        | 40/210 [00:26<01:38,  1.72it/s]\u001b[A\n",
      "Processed prompts:  20%|█▉        | 41/210 [00:27<01:51,  1.52it/s]\u001b[A\n",
      "Processed prompts:  20%|██        | 42/210 [00:30<03:19,  1.19s/it]\u001b[A\n",
      "Processed prompts:  20%|██        | 43/210 [00:32<03:59,  1.44s/it]\u001b[A\n",
      "Processed prompts:  21%|██        | 44/210 [00:39<07:56,  2.87s/it]\u001b[A\n",
      "Processed prompts:  21%|██▏       | 45/210 [00:42<07:30,  2.73s/it]\u001b[A\n",
      "Processed prompts:  22%|██▏       | 46/210 [00:42<05:45,  2.11s/it]\u001b[A\n",
      "Processed prompts:  22%|██▏       | 47/210 [00:43<04:36,  1.70s/it]\u001b[A\n",
      "Processed prompts:  23%|██▎       | 48/210 [00:43<03:38,  1.35s/it]\u001b[A\n",
      "Processed prompts:  23%|██▎       | 49/210 [00:45<03:51,  1.44s/it]\u001b[A\n",
      "Processed prompts:  24%|██▍       | 50/210 [00:48<04:52,  1.83s/it]\u001b[A\n",
      "Processed prompts:  24%|██▍       | 51/210 [00:48<03:40,  1.39s/it]\u001b[A\n",
      "Processed prompts:  25%|██▍       | 52/210 [00:54<07:28,  2.84s/it]\u001b[A\n",
      "Processed prompts:  25%|██▌       | 53/210 [01:09<16:22,  6.26s/it]\u001b[A\n",
      "Processed prompts:  26%|██▌       | 54/210 [01:16<17:19,  6.66s/it]\u001b[A\n",
      "Processed prompts:  26%|██▌       | 55/210 [01:26<19:43,  7.64s/it]\u001b[A\n",
      "Processed prompts:  27%|██▋       | 56/210 [01:35<20:48,  8.11s/it]\u001b[A\n",
      "Processed prompts:  27%|██▋       | 57/210 [01:57<31:14, 12.25s/it]\u001b[A\n",
      "Processed prompts:  34%|███▍      | 71/210 [02:03<04:56,  2.13s/it]\u001b[A\n",
      "Processed prompts:  34%|███▍      | 72/210 [02:05<04:53,  2.12s/it]\u001b[A\n",
      "Processed prompts:  35%|███▍      | 73/210 [02:11<05:36,  2.45s/it]\u001b[A\n",
      "Processed prompts:  35%|███▌      | 74/210 [02:17<06:42,  2.96s/it]\u001b[A\n",
      "Processed prompts:  36%|███▌      | 75/210 [02:24<08:06,  3.61s/it]\u001b[A\n",
      "Processed prompts:  36%|███▌      | 76/210 [02:33<09:58,  4.47s/it]\u001b[A\n",
      "Processed prompts:  37%|███▋      | 77/210 [02:40<11:07,  5.02s/it]\u001b[A\n",
      "Processed prompts:  37%|███▋      | 78/210 [02:43<10:00,  4.55s/it]\u001b[A\n",
      "Processed prompts:  38%|███▊      | 79/210 [02:44<08:04,  3.70s/it]\u001b[A\n",
      "Processed prompts:  38%|███▊      | 80/210 [02:50<09:15,  4.27s/it]\u001b[A\n",
      "Processed prompts:  39%|███▊      | 81/210 [02:50<06:49,  3.17s/it]\u001b[A\n",
      "Processed prompts:  39%|███▉      | 82/210 [02:53<06:36,  3.10s/it]\u001b[A\n",
      "Processed prompts:  40%|███▉      | 83/210 [02:58<07:32,  3.56s/it]\u001b[A\n",
      "Processed prompts:  40%|████      | 84/210 [03:00<06:43,  3.20s/it]\u001b[A\n",
      "Processed prompts:  40%|████      | 85/210 [03:02<06:00,  2.88s/it]\u001b[A\n",
      "Processed prompts:  41%|████      | 86/210 [03:04<05:27,  2.64s/it]\u001b[A\n",
      "Processed prompts:  41%|████▏     | 87/210 [03:06<04:58,  2.43s/it]\u001b[A\n",
      "Processed prompts:  42%|████▏     | 88/210 [03:08<04:35,  2.25s/it]\u001b[A\n",
      "Processed prompts:  42%|████▏     | 89/210 [03:14<06:36,  3.27s/it]\u001b[A\n",
      "Processed prompts:  43%|████▎     | 90/210 [03:15<05:23,  2.70s/it]\u001b[A\n",
      "Processed prompts:  43%|████▎     | 91/210 [03:15<03:54,  1.97s/it]\u001b[A\n",
      "Processed prompts:  44%|████▍     | 92/210 [03:18<04:32,  2.31s/it]\u001b[A\n",
      "Processed prompts:  44%|████▍     | 93/210 [03:19<03:35,  1.85s/it]\u001b[A\n",
      "Processed prompts:  45%|████▍     | 94/210 [03:21<03:52,  2.00s/it]\u001b[A\n",
      "Processed prompts:  45%|████▌     | 95/210 [03:26<05:07,  2.67s/it]\u001b[A\n",
      "Processed prompts:  46%|████▌     | 96/210 [03:28<04:39,  2.45s/it]\u001b[A\n",
      "Processed prompts:  46%|████▌     | 97/210 [03:30<04:37,  2.46s/it]\u001b[A\n",
      "Processed prompts:  47%|████▋     | 98/210 [03:33<04:42,  2.53s/it]\u001b[A\n",
      "Processed prompts:  47%|████▋     | 99/210 [03:35<04:25,  2.39s/it]\u001b[A\n",
      "Processed prompts:  48%|████▊     | 100/210 [03:38<04:47,  2.61s/it]\u001b[A\n",
      "Processed prompts:  48%|████▊     | 101/210 [03:39<03:59,  2.20s/it]\u001b[A\n",
      "Processed prompts:  49%|████▊     | 102/210 [03:42<04:02,  2.24s/it]\u001b[A\n",
      "Processed prompts:  49%|████▉     | 103/210 [03:59<11:58,  6.72s/it]\u001b[A\n",
      "Processed prompts:  50%|████▉     | 104/210 [04:08<13:04,  7.40s/it]\u001b[A\n",
      "Processed prompts:  50%|█████     | 105/210 [04:15<12:43,  7.27s/it]\u001b[A\n",
      "Processed prompts:  50%|█████     | 106/210 [04:18<10:41,  6.17s/it]\u001b[A\n",
      "Processed prompts:  51%|█████     | 107/210 [04:26<11:14,  6.55s/it]\u001b[A\n",
      "Processed prompts:  51%|█████▏    | 108/210 [04:28<08:57,  5.27s/it]\u001b[A\n",
      "Processed prompts:  52%|█████▏    | 109/210 [04:31<07:50,  4.66s/it]\u001b[A\n",
      "Processed prompts:  52%|█████▏    | 110/210 [04:34<06:54,  4.15s/it]\u001b[A\n",
      "Processed prompts:  53%|█████▎    | 111/210 [04:36<05:43,  3.47s/it]\u001b[A\n",
      "Processed prompts:  53%|█████▎    | 112/210 [04:38<05:09,  3.16s/it]\u001b[A\n",
      "Processed prompts:  54%|█████▍    | 113/210 [04:40<04:20,  2.68s/it]\u001b[A\n",
      "Processed prompts:  54%|█████▍    | 114/210 [04:41<03:31,  2.20s/it]\u001b[A\n",
      "Processed prompts:  55%|█████▍    | 115/210 [04:44<03:54,  2.47s/it]\u001b[A\n",
      "Processed prompts:  55%|█████▌    | 116/210 [04:47<03:51,  2.46s/it]\u001b[A\n",
      "Processed prompts:  56%|█████▌    | 117/210 [04:50<04:12,  2.71s/it]\u001b[A\n",
      "Processed prompts:  56%|█████▌    | 118/210 [04:53<04:30,  2.94s/it]\u001b[A\n",
      "Processed prompts:  57%|█████▋    | 119/210 [04:55<03:54,  2.58s/it]\u001b[A\n",
      "Processed prompts:  57%|█████▋    | 120/210 [04:57<03:29,  2.33s/it]\u001b[A\n",
      "Processed prompts:  58%|█████▊    | 121/210 [04:59<03:17,  2.22s/it]\u001b[A\n",
      "Processed prompts:  58%|█████▊    | 122/210 [04:59<02:28,  1.69s/it]\u001b[A\n",
      "Processed prompts:  59%|█████▊    | 123/210 [05:00<01:56,  1.34s/it]\u001b[A\n",
      "Processed prompts:  59%|█████▉    | 124/210 [05:01<01:52,  1.31s/it]\u001b[A\n",
      "Processed prompts:  60%|█████▉    | 125/210 [05:02<01:46,  1.25s/it]\u001b[A\n",
      "Processed prompts:  60%|██████    | 126/210 [05:07<03:14,  2.31s/it]\u001b[A\n",
      "Processed prompts:  60%|██████    | 127/210 [05:10<03:27,  2.49s/it]\u001b[A\n",
      "Processed prompts:  61%|██████    | 128/210 [05:13<03:31,  2.58s/it]\u001b[A\n",
      "Processed prompts:  61%|██████▏   | 129/210 [05:14<02:52,  2.13s/it]\u001b[A\n",
      "Processed prompts:  62%|██████▏   | 130/210 [05:15<02:27,  1.84s/it]\u001b[A\n",
      "Processed prompts:  62%|██████▏   | 131/210 [05:20<03:38,  2.76s/it]\u001b[A\n",
      "Processed prompts:  63%|██████▎   | 132/210 [05:24<04:08,  3.19s/it]\u001b[A\n",
      "Processed prompts:  63%|██████▎   | 133/210 [05:29<04:52,  3.80s/it]\u001b[A\n",
      "Processed prompts:  64%|██████▍   | 134/210 [05:30<03:32,  2.79s/it]\u001b[A\n",
      "Processed prompts:  64%|██████▍   | 135/210 [05:39<06:06,  4.89s/it]\u001b[A\n",
      "Processed prompts:  65%|██████▍   | 136/210 [05:46<06:33,  5.32s/it]\u001b[A\n",
      "Processed prompts:  65%|██████▌   | 137/210 [05:51<06:21,  5.22s/it]\u001b[A\n",
      "Processed prompts:  66%|██████▌   | 138/210 [05:53<05:15,  4.39s/it]\u001b[A\n",
      "Processed prompts:  66%|██████▌   | 139/210 [05:56<04:44,  4.01s/it]\u001b[A\n",
      "Processed prompts:  67%|██████▋   | 140/210 [05:57<03:36,  3.10s/it]\u001b[A\n",
      "Processed prompts:  67%|██████▋   | 141/210 [06:02<04:03,  3.54s/it]\u001b[A\n",
      "Processed prompts:  68%|██████▊   | 142/210 [06:03<03:05,  2.73s/it]\u001b[A\n",
      "Processed prompts:  68%|██████▊   | 143/210 [06:05<02:52,  2.58s/it]\u001b[A\n",
      "Processed prompts:  69%|██████▊   | 144/210 [06:11<04:04,  3.70s/it]\u001b[A\n",
      "Processed prompts:  69%|██████▉   | 145/210 [06:17<04:39,  4.30s/it]\u001b[A\n",
      "Processed prompts:  70%|██████▉   | 146/210 [06:21<04:22,  4.10s/it]\u001b[A\n",
      "Processed prompts:  70%|███████   | 147/210 [06:23<03:39,  3.48s/it]\u001b[A\n",
      "Processed prompts:  70%|███████   | 148/210 [06:26<03:29,  3.37s/it]\u001b[A\n",
      "Processed prompts:  71%|███████   | 149/210 [06:27<02:54,  2.86s/it]\u001b[A\n",
      "Processed prompts:  71%|███████▏  | 150/210 [06:28<02:12,  2.21s/it]\u001b[A\n",
      "Processed prompts:  72%|███████▏  | 151/210 [06:29<01:44,  1.77s/it]\u001b[A\n",
      "Processed prompts:  72%|███████▏  | 152/210 [06:30<01:34,  1.63s/it]\u001b[A\n",
      "Processed prompts:  73%|███████▎  | 153/210 [06:31<01:10,  1.24s/it]\u001b[A\n",
      "Processed prompts:  73%|███████▎  | 154/210 [06:32<01:13,  1.31s/it]\u001b[A\n",
      "Processed prompts:  74%|███████▍  | 156/210 [06:33<00:46,  1.16it/s]\u001b[A\n",
      "Processed prompts:  75%|███████▌  | 158/210 [06:33<00:31,  1.63it/s]\u001b[A\n",
      "Processed prompts:  76%|███████▌  | 159/210 [06:33<00:26,  1.89it/s]\u001b[A\n",
      "Processed prompts:  76%|███████▌  | 160/210 [06:35<00:44,  1.12it/s]\u001b[A\n",
      "Processed prompts:  77%|███████▋  | 161/210 [06:36<00:45,  1.09it/s]\u001b[A\n",
      "Processed prompts:  77%|███████▋  | 162/210 [06:38<00:56,  1.18s/it]\u001b[A\n",
      "Processed prompts:  78%|███████▊  | 163/210 [06:38<00:41,  1.13it/s]\u001b[A\n",
      "Processed prompts:  78%|███████▊  | 164/210 [06:39<00:32,  1.43it/s]\u001b[A\n",
      "Processed prompts:  79%|███████▊  | 165/210 [06:39<00:26,  1.70it/s]\u001b[A\n",
      "Processed prompts:  79%|███████▉  | 166/210 [06:44<01:21,  1.84s/it]\u001b[A\n",
      "Processed prompts:  80%|███████▉  | 167/210 [07:03<04:53,  6.82s/it]\u001b[A\n",
      "Processed prompts:  80%|████████  | 168/210 [07:12<05:12,  7.44s/it]\u001b[A\n",
      "Processed prompts:  80%|████████  | 169/210 [07:21<05:33,  8.13s/it]\u001b[A\n",
      "Processed prompts:  81%|████████  | 170/210 [07:35<06:29,  9.75s/it]\u001b[A\n",
      "Processed prompts:  81%|████████▏ | 171/210 [07:45<06:24,  9.86s/it]\u001b[A\n",
      "Processed prompts:  82%|████████▏ | 172/210 [07:47<04:42,  7.44s/it]\u001b[A\n",
      "Processed prompts:  82%|████████▏ | 173/210 [07:55<04:44,  7.68s/it]\u001b[A\n",
      "Processed prompts:  83%|████████▎ | 174/210 [07:58<03:51,  6.43s/it]\u001b[A\n",
      "Processed prompts:  83%|████████▎ | 175/210 [08:00<02:54,  4.98s/it]\u001b[A\n",
      "Processed prompts:  84%|████████▍ | 176/210 [08:04<02:33,  4.52s/it]\u001b[A\n",
      "Processed prompts:  84%|████████▍ | 177/210 [08:04<01:53,  3.45s/it]\u001b[A\n",
      "Processed prompts:  85%|████████▍ | 178/210 [08:06<01:28,  2.77s/it]\u001b[A\n",
      "Processed prompts:  85%|████████▌ | 179/210 [08:08<01:19,  2.56s/it]\u001b[A\n",
      "Processed prompts:  86%|████████▌ | 180/210 [08:09<01:09,  2.30s/it]\u001b[A\n",
      "Processed prompts:  86%|████████▌ | 181/210 [08:12<01:05,  2.25s/it]\u001b[A\n",
      "Processed prompts:  87%|████████▋ | 182/210 [08:12<00:49,  1.77s/it]\u001b[A\n",
      "Processed prompts:  87%|████████▋ | 183/210 [08:13<00:39,  1.45s/it]\u001b[A\n",
      "Processed prompts:  88%|████████▊ | 184/210 [08:14<00:31,  1.20s/it]\u001b[A\n",
      "Processed prompts:  88%|████████▊ | 185/210 [08:14<00:25,  1.02s/it]\u001b[A\n",
      "Processed prompts:  89%|████████▊ | 186/210 [08:16<00:29,  1.22s/it]\u001b[A\n",
      "Processed prompts:  89%|████████▉ | 187/210 [08:16<00:20,  1.13it/s]\u001b[A\n",
      "Processed prompts:  90%|████████▉ | 188/210 [08:16<00:14,  1.47it/s]\u001b[A\n",
      "Processed prompts:  90%|█████████ | 189/210 [08:16<00:10,  1.97it/s]\u001b[A\n",
      "Processed prompts:  90%|█████████ | 190/210 [08:19<00:21,  1.08s/it]\u001b[A\n",
      "Processed prompts:  91%|█████████ | 191/210 [08:22<00:36,  1.91s/it]\u001b[A\n",
      "Processed prompts:  91%|█████████▏| 192/210 [08:24<00:31,  1.74s/it]\u001b[A\n",
      "Processed prompts:  92%|█████████▏| 193/210 [08:25<00:28,  1.67s/it]\u001b[A\n",
      "Processed prompts:  92%|█████████▏| 194/210 [08:36<01:07,  4.22s/it]\u001b[A\n",
      "Processed prompts:  93%|█████████▎| 195/210 [08:37<00:50,  3.34s/it]\u001b[A\n",
      "Processed prompts:  93%|█████████▎| 196/210 [08:43<01:00,  4.32s/it]\u001b[A\n",
      "Processed prompts:  94%|█████████▍| 197/210 [08:49<01:02,  4.77s/it]\u001b[A\n",
      "Processed prompts:  94%|█████████▍| 198/210 [08:54<00:55,  4.64s/it]\u001b[A\n",
      "Processed prompts:  95%|█████████▍| 199/210 [08:57<00:45,  4.16s/it]\u001b[A\n",
      "Processed prompts:  95%|█████████▌| 200/210 [09:00<00:39,  3.95s/it]\u001b[A\n",
      "Processed prompts:  96%|█████████▌| 201/210 [09:03<00:31,  3.55s/it]\u001b[A\n",
      "Processed prompts:  96%|█████████▌| 202/210 [09:05<00:26,  3.31s/it]\u001b[A\n",
      "Processed prompts:  97%|█████████▋| 203/210 [09:07<00:19,  2.81s/it]\u001b[A\n",
      "Processed prompts:  97%|█████████▋| 204/210 [09:11<00:18,  3.10s/it]\u001b[A\n",
      "Processed prompts:  98%|█████████▊| 205/210 [09:12<00:12,  2.51s/it]\u001b[A\n",
      "Processed prompts:  98%|█████████▊| 206/210 [09:13<00:08,  2.08s/it]\u001b[A\n",
      "Processed prompts:  99%|█████████▊| 207/210 [09:14<00:04,  1.60s/it]\u001b[A\n",
      "Processed prompts:  99%|█████████▉| 208/210 [09:16<00:03,  1.79s/it]\u001b[A\n",
      "Processed prompts: 100%|█████████▉| 209/210 [09:16<00:01,  1.46s/it]\u001b[A\n",
      "Processed prompts: 100%|██████████| 210/210 [09:17<00:00,  2.65s/it]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e142087a0c244929852e3f55c4a31a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Progress: 100%|██████████| 7/7 [1:11:00<00:00, 608.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] /root/autodl-fs/LRP/open_ended_data_generation/20251210_llama2_7b_base/20251210_LRP_generation_th_1_selected_LRP_kur_res_zh_zscore_margin_selected_open_ended.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbe7d02dab74b848d4af6ca260a14ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved final merged] /root/autodl-fs/LRP/open_ended_data_generation/20251210_llama2_7b_base/20251210_LRP_generation_all_models.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import datasets\n",
    "import torch\n",
    "from transformers import AutoConfig\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ======== Config ========\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "# load test data\n",
    "ds_test = datasets.load_dataset(\n",
    "    'json',\n",
    "    data_files='/root/autodl-fs/LRP/open_ended_dataset/all_data.json'\n",
    ")\n",
    "\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# ======== Date stamp ========\n",
    "dt = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "\n",
    "\n",
    "def build_model_list(llm, base=BASE):\n",
    "    ml = [(llm, \"org_model\")]\n",
    "    for fname in sorted(os.listdir(base)):\n",
    "        if fname.endswith(\".pt\") and not fname.startswith(\"all_mlp\"):\n",
    "            path = f\"{base}/{fname}\"\n",
    "            ml.append((path, fname.replace(\".pt\",\"\")))\n",
    "    return ml\n",
    "\n",
    "model_list = build_model_list(llm)\n",
    "\n",
    "# filter\n",
    "model_list = (get_need_exp(model_list))\n",
    "\n",
    "\n",
    "data_list = [\n",
    "    (ds_test, 'open_ended')\n",
    "]\n",
    "\n",
    "# ======== merge buffer ========\n",
    "ds_list = []\n",
    "\n",
    "print(f\"[INFO] Total models: {len(model_list)}\")\n",
    "\n",
    "model_list_bar = tqdm(total=len(model_list), desc=\"Model Progress\")\n",
    "\n",
    "# ======== Running ========\n",
    "for model_path_pt, model_name in model_list:\n",
    "    print(\"*\" * 40)\n",
    "    print(\"Model:\", model_name)\n",
    "\n",
    "    # loop datasets\n",
    "    for i_data, data_name in data_list:\n",
    "        \n",
    "        # ======== Save path ========\n",
    "        save_name = f\"{dt}_LRP_generation_{model_name}_{data_name}.json\"\n",
    "        save_path = os.path.join(save_dir, save_name)\n",
    "\n",
    "        # ======== Skip if exists ========\n",
    "        if os.path.exists(save_path):\n",
    "            print(f\"[Skip] Exists: {save_path}\")\n",
    "            continue\n",
    "\n",
    "        # ======== Load Mask ========\n",
    "        if model_name != 'org_model':\n",
    "            tmp_neuron = torch.load(model_path_pt)\n",
    "            i_model = get_mask_neuron_model_vllm_LRP(llm, convert_LAPE_format(tmp_neuron))\n",
    "        else:\n",
    "            i_model = llm\n",
    "\n",
    "        # ======== Run ========\n",
    "        ds_test_tmp = copy.deepcopy(i_data)\n",
    "        text_list = list(ds_test_tmp['train']['text'])\n",
    "\n",
    "        ans_list = get_open_ended_answer_vllm(i_model, text_list)\n",
    "\n",
    "        # ======== Meta ========\n",
    "        model_col = [model_name + \"|\" + data_name] * len(ans_list)\n",
    "        ds_test_tmp['train'] = ds_test_tmp['train'].add_column(name='answer', column=ans_list)\n",
    "        ds_test_tmp['train'] = ds_test_tmp['train'].add_column(name='model_type', column=model_col)\n",
    "\n",
    "        # ======== append global ========\n",
    "        ds_list.append(ds_test_tmp['train'])\n",
    "\n",
    "        # ======== Save one snapshot ========\n",
    "        ds_test_tmp['train'].to_json(save_path, force_ascii=False)\n",
    "        print(f\"[Saved] {save_path}\")\n",
    "\n",
    "    model_list_bar.update(1)\n",
    "\n",
    "\n",
    "# ======== Save final merge ========\n",
    "if len(ds_list) > 0:\n",
    "    ds_final = datasets.concatenate_datasets(ds_list)\n",
    "    final_path = os.path.join(save_dir, f\"{dt}_LRP_generation_all_models.json\")\n",
    "    ds_final.to_json(final_path, force_ascii=False)\n",
    "    print(f\"[Saved final merged] {final_path}\")\n",
    "else:\n",
    "    print(\"[Info] No new results, final merge skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909d13e-06c5-4932-b0f8-13983069191c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3fdd64-a14a-4a08-80f6-757a6bc2c57f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57e59b-b0c3-4f93-a3b4-51fdf9abbd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9d2f7-0e10-4f79-83a0-4282e176934a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45fee6-158e-4add-afef-21257404a538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14147e92-502a-430d-b509-7d44c6c6e2e4",
   "metadata": {},
   "source": [
    "## before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2ab2923-d00b-409e-8a46-393be1922746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "Model: th_0_selected_LRP_kur_res_en_zscore\n",
      "====================\n",
      "Data: open_ended\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 210/210 [03:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e214df7ddcc64f27aef47b18167f3d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved per model] /root/autodl-fs/LRP/open_ended_data_generation/20251204_all_exp/20251204_LRP_generation_th_0_selected_LRP_kur_res_en_zscore_open_ended.json\n",
      "****************************************\n",
      "Model: th_0_selected_LRP_kur_res_vi_zscore\n",
      "====================\n",
      "Data: open_ended\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 210/210 [02:45<00:00,  1.27it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeee16667646443184a93740c1d6194b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved per model] /root/autodl-fs/LRP/open_ended_data_generation/20251204_all_exp/20251204_LRP_generation_th_0_selected_LRP_kur_res_vi_zscore_open_ended.json\n",
      "****************************************\n",
      "Model: th_0_selected_LRP_kur_res_zh_zscore\n",
      "====================\n",
      "Data: open_ended\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 210/210 [03:27<00:00,  1.01it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a863a41f2f49e0bffc16941fe27986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved per model] /root/autodl-fs/LRP/open_ended_data_generation/20251204_all_exp/20251204_LRP_generation_th_0_selected_LRP_kur_res_zh_zscore_open_ended.json\n",
      "****************************************\n",
      "Model: th_1_selected_LRP_kur_res_en_zscore\n",
      "====================\n",
      "Data: open_ended\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   3%|▎         | 7/210 [00:10<03:30,  1.04s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m ds_test_tmp \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(i_data)\n\u001b[1;32m     57\u001b[0m text_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ds_test_tmp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 59\u001b[0m ans_list \u001b[38;5;241m=\u001b[39m \u001b[43mget_open_ended_answer_vllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# meta\u001b[39;00m\n\u001b[1;32m     62\u001b[0m model_col \u001b[38;5;241m=\u001b[39m [model_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m data_name] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(ans_list)\n",
      "File \u001b[0;32m/autodl-fs/data/LRP/util.py:113\u001b[0m, in \u001b[0;36mget_open_ended_answer_vllm\u001b[0;34m(llm, textlist, max_new_tokens)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# vLLM 支持一次性批量输入\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtextlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m itext, out \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(textlist, outputs):\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# vLLM 的 output 为 list，每个 item 里有 text\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     full_output \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/entrypoints/llm.py:165\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm)\u001b[0m\n\u001b[1;32m    162\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m prompt_token_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m prompt_token_ids[\n\u001b[1;32m    163\u001b[0m         i]\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_request(prompt, sampling_params, token_ids)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/entrypoints/llm.py:185\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    183\u001b[0m outputs: List[RequestOutput] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 185\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/engine/llm_engine.py:628\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    624\u001b[0m seq_group_metadata_list, scheduler_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mschedule()\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheduler_outputs\u001b[38;5;241m.\u001b[39mis_empty():\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# Execute the model.\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m     all_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexecute_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdriver_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq_group_metadata_list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks_to_swap_in\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_swap_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks_to_swap_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_swap_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks_to_copy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;66;03m# Only the driver worker returns the sampling results.\u001b[39;00m\n\u001b[1;32m    638\u001b[0m     output \u001b[38;5;241m=\u001b[39m all_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/engine/llm_engine.py:795\u001b[0m, in \u001b[0;36mLLMEngine._run_workers\u001b[0;34m(self, method, driver_args, driver_kwargs, max_concurrent_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    792\u001b[0m     driver_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m    794\u001b[0m \u001b[38;5;66;03m# Start the driver worker after all the ray workers.\u001b[39;00m\n\u001b[0;32m--> 795\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;66;03m# Get the results of the ray workers.\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/worker/worker.py:189\u001b[0m, in \u001b[0;36mWorker.execute_model\u001b[0;34m(self, seq_group_metadata_list, blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_seq_groups \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 189\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/worker/model_runner.py:461\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, seq_group_metadata_list, kv_caches)\u001b[0m\n\u001b[1;32m    453\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m model_executable(\n\u001b[1;32m    454\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_tokens,\n\u001b[1;32m    455\u001b[0m     positions\u001b[38;5;241m=\u001b[39minput_positions,\n\u001b[1;32m    456\u001b[0m     kv_caches\u001b[38;5;241m=\u001b[39mkv_caches,\n\u001b[1;32m    457\u001b[0m     input_metadata\u001b[38;5;241m=\u001b[39minput_metadata,\n\u001b[1;32m    458\u001b[0m )\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:295\u001b[0m, in \u001b[0;36mLlamaForCausalLM.sample\u001b[0;34m(self, hidden_states, sampling_metadata)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    292\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    293\u001b[0m     sampling_metadata: SamplingMetadata,\n\u001b[1;32m    294\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[SamplerOutput]:\n\u001b[0;32m--> 295\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m                               \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_tokens\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:93\u001b[0m, in \u001b[0;36mSampler.forward\u001b[0;34m(self, embedding, hidden_states, sampling_metadata, embedding_bias)\u001b[0m\n\u001b[1;32m     90\u001b[0m logprobs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Sample the next tokens.\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m sample_results \u001b[38;5;241m=\u001b[39m \u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Get the logprobs query results.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m prompt_logprobs, sample_logprobs \u001b[38;5;241m=\u001b[39m _get_logprobs(\n\u001b[1;32m     96\u001b[0m     logprobs, sampling_metadata, sample_results)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:408\u001b[0m, in \u001b[0;36m_sample\u001b[0;34m(probs, logprobs, sampling_metadata)\u001b[0m\n\u001b[1;32m    405\u001b[0m seq_group_ids, seq_groups, is_prompts, sample_indices \u001b[38;5;241m=\u001b[39m sample_metadata[\n\u001b[1;32m    406\u001b[0m     sampling_type]\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m SamplingType\u001b[38;5;241m.\u001b[39mGREEDY:\n\u001b[0;32m--> 408\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m \u001b[43m_greedy_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m SamplingType\u001b[38;5;241m.\u001b[39mRANDOM:\n\u001b[1;32m    410\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _random_sample(seq_groups, is_prompts,\n\u001b[1;32m    411\u001b[0m                                     multinomial_samples)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:241\u001b[0m, in \u001b[0;36m_greedy_sample\u001b[0;34m(selected_seq_groups, samples)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_greedy_sample\u001b[39m(\n\u001b[1;32m    238\u001b[0m     selected_seq_groups: List[Tuple[List[\u001b[38;5;28mint\u001b[39m], SamplingParams]],\n\u001b[1;32m    239\u001b[0m     samples: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    240\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[List[\u001b[38;5;28mint\u001b[39m], List[\u001b[38;5;28mint\u001b[39m]]]:\n\u001b[0;32m--> 241\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[43msamples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    243\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import datasets\n",
    "import torch\n",
    "from transformers import AutoConfig\n",
    "from datetime import datetime\n",
    "\n",
    "# ======== Config ========\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "# load test data\n",
    "ds_test = datasets.load_dataset(\n",
    "    'json',\n",
    "    data_files='/root/autodl-fs/LRP/open_ended_dataset/all_data.json'\n",
    ")\n",
    "\n",
    "save_dir = \"/root/autodl-fs/LRP/open_ended_data_generation/20251204_all_exp\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# calc ppl\n",
    "_ = [\n",
    "    # (llm, 'org_model'),\n",
    "    ('/root/autodl-fs/LRP_kur_res/20251204_5000samples_cal_llama2_7b_chat/th_0_selected_LRP_kur_res_en_zscore.pt', 'th_0_selected_LRP_kur_res_en_zscore'),\n",
    "    ('/root/autodl-fs/LRP_kur_res/20251204_5000samples_cal_llama2_7b_chat/th_0_selected_LRP_kur_res_vi_zscore.pt', 'th_0_selected_LRP_kur_res_vi_zscore'),\n",
    "    ('/root/autodl-fs/LRP_kur_res/20251204_5000samples_cal_llama2_7b_chat/th_0_selected_LRP_kur_res_zh_zscore.pt', 'th_0_selected_LRP_kur_res_zh_zscore'),\n",
    "]\n",
    "\n",
    "data_list = [\n",
    "    (ds_test, 'open_ended')\n",
    "]\n",
    "\n",
    "# for final concat\n",
    "ds_list = []\n",
    "\n",
    "# date stamp\n",
    "dt = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "model_list_bar = tqdm(total=len(model_list))\n",
    "# ======== Running ========\n",
    "for model_path_pt, model_name in model_list:\n",
    "    print(\"*\" * 40)\n",
    "    print(\"Model:\", model_name)\n",
    "\n",
    "    # load neuron and mask LLM\n",
    "    if model_name != 'org_model':\n",
    "        tmp_neuron = torch.load(model_path_pt)\n",
    "        i_model = get_mask_neuron_model_vllm_LRP(llm, convert_LAPE_format(tmp_neuron))\n",
    "    else:\n",
    "        i_model = llm\n",
    "\n",
    "    # loop datasets\n",
    "    for i_data, data_name in data_list:\n",
    "        print(\"=\" * 20)\n",
    "        print(\"Data:\", data_name)\n",
    "\n",
    "        ds_test_tmp = copy.deepcopy(i_data)\n",
    "        text_list = list(ds_test_tmp['train']['text'])\n",
    "\n",
    "        ans_list = get_open_ended_answer_vllm(i_model, text_list)\n",
    "\n",
    "        # meta\n",
    "        model_col = [model_name + \"|\" + data_name] * len(ans_list)\n",
    "\n",
    "        ds_test_tmp['train'] = ds_test_tmp['train'].add_column(name='answer', column=ans_list)\n",
    "        ds_test_tmp['train'] = ds_test_tmp['train'].add_column(name='model_type', column=model_col)\n",
    "\n",
    "        # ======== append to global result ========\n",
    "        ds_list.append(ds_test_tmp['train'])\n",
    "\n",
    "        # ======== Save one snapshot ========\n",
    "        save_name = f\"{dt}_LRP_generation_{model_name}_{data_name}.json\"\n",
    "        save_path = os.path.join(save_dir, save_name)\n",
    "\n",
    "        ds_test_tmp['train'].to_json(save_path, force_ascii=False)\n",
    "        print(f\"[Saved per model] {save_path}\")\n",
    "        \n",
    "    model_list_bar.update(1)\n",
    "\n",
    "\n",
    "# ======== Save final merge ========\n",
    "if len(ds_list) > 0:\n",
    "    ds_final = datasets.concatenate_datasets(ds_list)\n",
    "    final_path = os.path.join(save_dir, f\"{dt}_LRP_generation_all_models.json\")\n",
    "    ds_final.to_json(final_path, force_ascii=False)\n",
    "    print(f\"[Saved final merged] {final_path}\")\n",
    "else:\n",
    "    print(\"[Warning] No results in ds_list\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a406a-16dc-4c52-88b4-04aab3adb679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa0a517-0009-4753-acee-e94278d4cf7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cfcc6d-d4bd-4706-9c7e-4b6df4f4e660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0de305-45a0-4593-ae79-9244eaab77f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce41ec22-4f88-4e79-b7c4-3ab8da7e1fb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "model name: th_0_selected_LRP_kur_res_en_zscore\n",
      "====================\n",
      "data name: open_ended\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 210/210 [03:22<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "model name: th_0_selected_LRP_kur_res_vi_zscore\n",
      "====================\n",
      "data name: open_ended\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 210/210 [02:47<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "model name: th_0_selected_LRP_kur_res_zh_zscore\n",
      "====================\n",
      "data name: open_ended\n",
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/210 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata name:\u001b[39m\u001b[38;5;124m'\u001b[39m, data_name)\n\u001b[0;32m---> 40\u001b[0m ans_list\u001b[38;5;241m=\u001b[39m \u001b[43mget_open_ended_answer_vllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mds_test_tmp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#calc_ppl(i_model, tokenizer, i_data, max_len= 2048)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# create model -col\u001b[39;00m\n\u001b[1;32m     42\u001b[0m model_describe \u001b[38;5;241m=\u001b[39m [model_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mdata_name]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(ans_list)\n",
      "File \u001b[0;32m/autodl-fs/data/LRP/util.py:113\u001b[0m, in \u001b[0;36mget_open_ended_answer_vllm\u001b[0;34m(llm, textlist, max_new_tokens)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# vLLM 支持一次性批量输入\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtextlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m itext, out \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(textlist, outputs):\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# vLLM 的 output 为 list，每个 item 里有 text\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     full_output \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/entrypoints/llm.py:165\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm)\u001b[0m\n\u001b[1;32m    162\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m prompt_token_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m prompt_token_ids[\n\u001b[1;32m    163\u001b[0m         i]\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_request(prompt, sampling_params, token_ids)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/entrypoints/llm.py:185\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    183\u001b[0m outputs: List[RequestOutput] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 185\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/engine/llm_engine.py:628\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    624\u001b[0m seq_group_metadata_list, scheduler_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mschedule()\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheduler_outputs\u001b[38;5;241m.\u001b[39mis_empty():\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# Execute the model.\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m     all_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexecute_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdriver_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq_group_metadata_list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks_to_swap_in\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_swap_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks_to_swap_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_swap_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks_to_copy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks_to_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;66;03m# Only the driver worker returns the sampling results.\u001b[39;00m\n\u001b[1;32m    638\u001b[0m     output \u001b[38;5;241m=\u001b[39m all_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/engine/llm_engine.py:795\u001b[0m, in \u001b[0;36mLLMEngine._run_workers\u001b[0;34m(self, method, driver_args, driver_kwargs, max_concurrent_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    792\u001b[0m     driver_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m    794\u001b[0m \u001b[38;5;66;03m# Start the driver worker after all the ray workers.\u001b[39;00m\n\u001b[0;32m--> 795\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdriver_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;66;03m# Get the results of the ray workers.\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/worker/worker.py:189\u001b[0m, in \u001b[0;36mWorker.execute_model\u001b[0;34m(self, seq_group_metadata_list, blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_seq_groups \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m--> 189\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_group_metadata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/worker/model_runner.py:461\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, seq_group_metadata_list, kv_caches)\u001b[0m\n\u001b[1;32m    453\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m model_executable(\n\u001b[1;32m    454\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_tokens,\n\u001b[1;32m    455\u001b[0m     positions\u001b[38;5;241m=\u001b[39minput_positions,\n\u001b[1;32m    456\u001b[0m     kv_caches\u001b[38;5;241m=\u001b[39mkv_caches,\n\u001b[1;32m    457\u001b[0m     input_metadata\u001b[38;5;241m=\u001b[39minput_metadata,\n\u001b[1;32m    458\u001b[0m )\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:295\u001b[0m, in \u001b[0;36mLlamaForCausalLM.sample\u001b[0;34m(self, hidden_states, sampling_metadata)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    292\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    293\u001b[0m     sampling_metadata: SamplingMetadata,\n\u001b[1;32m    294\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[SamplerOutput]:\n\u001b[0;32m--> 295\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m                               \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_tokens\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:93\u001b[0m, in \u001b[0;36mSampler.forward\u001b[0;34m(self, embedding, hidden_states, sampling_metadata, embedding_bias)\u001b[0m\n\u001b[1;32m     90\u001b[0m logprobs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Sample the next tokens.\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m sample_results \u001b[38;5;241m=\u001b[39m \u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Get the logprobs query results.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m prompt_logprobs, sample_logprobs \u001b[38;5;241m=\u001b[39m _get_logprobs(\n\u001b[1;32m     96\u001b[0m     logprobs, sampling_metadata, sample_results)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:408\u001b[0m, in \u001b[0;36m_sample\u001b[0;34m(probs, logprobs, sampling_metadata)\u001b[0m\n\u001b[1;32m    405\u001b[0m seq_group_ids, seq_groups, is_prompts, sample_indices \u001b[38;5;241m=\u001b[39m sample_metadata[\n\u001b[1;32m    406\u001b[0m     sampling_type]\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m SamplingType\u001b[38;5;241m.\u001b[39mGREEDY:\n\u001b[0;32m--> 408\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m \u001b[43m_greedy_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m SamplingType\u001b[38;5;241m.\u001b[39mRANDOM:\n\u001b[1;32m    410\u001b[0m     sample_results \u001b[38;5;241m=\u001b[39m _random_sample(seq_groups, is_prompts,\n\u001b[1;32m    411\u001b[0m                                     multinomial_samples)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:241\u001b[0m, in \u001b[0;36m_greedy_sample\u001b[0;34m(selected_seq_groups, samples)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_greedy_sample\u001b[39m(\n\u001b[1;32m    238\u001b[0m     selected_seq_groups: List[Tuple[List[\u001b[38;5;28mint\u001b[39m], SamplingParams]],\n\u001b[1;32m    239\u001b[0m     samples: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    240\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[List[\u001b[38;5;28mint\u001b[39m], List[\u001b[38;5;28mint\u001b[39m]]]:\n\u001b[0;32m--> 241\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[43msamples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    243\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "\n",
    "# load test data\n",
    "ds_test = datasets.load_dataset('json', data_files ='/root/autodl-fs/LRP/open_ended_dataset/all_data.json')\n",
    "               \n",
    "# calc ppl\n",
    "model_list= [\n",
    "    #(llm, 'org_model'),\n",
    "    ('/root/autodl-fs/LRP_kur_res/20251204_5000samples_cal_llama2_7b_chat/th_0_selected_LRP_kur_res_en_zscore.pt', 'th_0_selected_LRP_kur_res_en_zscore'),\n",
    "    ('/root/autodl-fs/LRP_kur_res/20251204_5000samples_cal_llama2_7b_chat/th_0_selected_LRP_kur_res_vi_zscore.pt', 'th_0_selected_LRP_kur_res_vi_zscore'),\n",
    "    ('/root/autodl-fs/LRP_kur_res/20251204_5000samples_cal_llama2_7b_chat/th_0_selected_LRP_kur_res_zh_zscore.pt', 'th_0_selected_LRP_kur_res_zh_zscore'),\n",
    "    \n",
    "   \n",
    "]\n",
    "\n",
    "data_list = [\n",
    "    (ds_test, 'open_ended')    \n",
    "]\n",
    "\n",
    "ds_list = []\n",
    "\n",
    "for i_model, model_name in model_list:\n",
    "    print('*'*20)\n",
    "    print('*'*20)\n",
    "    print('model name:', model_name)\n",
    "\n",
    "    if model_name != 'org_model':\n",
    "        # load  neuron\n",
    "        tmp_neuron = torch.load(i_model)\n",
    "        i_model = get_mask_neuron_model_vllm_LRP(llm, convert_LAPE_format(tmp_neuron))\n",
    "\n",
    "\n",
    "    for i_data, data_name in data_list:\n",
    "        ds_test_tmp = copy.deepcopy(i_data)\n",
    "        print('='*20)\n",
    "        print('data name:', data_name)\n",
    "        \n",
    "        ans_list= get_open_ended_answer_vllm(i_model, list(ds_test_tmp['train']['text']))#calc_ppl(i_model, tokenizer, i_data, max_len= 2048)\n",
    "        # create model -col\n",
    "        model_describe = [model_name+'|'+data_name]*len(ans_list)\n",
    "        \n",
    "        #ds_test['train'] = ds_test['train'].add_column(name='describ'+('|'+model_name+'|'+data_name), column=model_describe)        \n",
    "        ds_test_tmp['train'] = ds_test_tmp['train'].add_column(name='answer', column=ans_list)   \n",
    "\n",
    "        ds_test_tmp['train'] = ds_test_tmp['train'].add_column(name='model_type', column=model_describe)      \n",
    "\n",
    "        ds_list.append(ds_test_tmp['train'])\n",
    "        \n",
    "# \n",
    "ds_final = datasets.concatenate_datasets(ds_list)\n",
    "ds_final.to_json('~/autodl-fs/LRP/open_ended_data_generation/20251204_LRP_based_generation.json', force_ascii=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47529f1-5c01-4d04-be49-932d72e9d6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e27bdf7-c17b-4d35-ac80-1cc5384d2a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a58856-20c3-4c0d-85da-0fd724223ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7bd279b-026d-4c8e-aa0f-aef79b2f0b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = ds_final.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c909782c-4299-4c53-aac5-99328c99f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "生成judge的prompt\n",
    "\n",
    "'''\n",
    "\n",
    "prompt_template = (\n",
    "    \"You are a neutral judge. Score the model’s answer from 1 to 10 based on correctness, completeness, clarity, and usefulness.\\n\"\n",
    "    \"Respond ONLY in this JSON format:\\n\"\n",
    "    '{{\"score\": <1-10>, \"reason\": \"<brief reason>\"}}\\n\\n'\n",
    "    \"Question:\\n{question}\\n\\n\"\n",
    "    \"Answer:\\n{answer}\"\n",
    ")\n",
    "\n",
    "def get_judge_promt(x):\n",
    "    return prompt_template.format(question= x[0], answer= x[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c726d69b-3757-4652-84c7-c204d4fdbb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1422/3206873007.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df_final['judge_prompt'] = df_final[['text','answer']].apply(lambda x: prompt_template.format(question= x[0], answer= x[1]), axis=1)\n"
     ]
    }
   ],
   "source": [
    "df_final['judge_prompt'] = df_final[['text','answer']].apply(lambda x: prompt_template.format(question= x[0], answer= x[1]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0da9d64-c1da-4a10-9bca-869a24fa3832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>__index_level_0__</th>\n",
       "      <th>answer</th>\n",
       "      <th>model_type</th>\n",
       "      <th>judge_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I improve my time management skills?</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Time management is the process of planning and...</td>\n",
       "      <td>org_model|open_ended</td>\n",
       "      <td>You are a neutral judge. Score the model’s ans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the most effective ways to deal with ...</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>Stress is a normal part of life, but chronic s...</td>\n",
       "      <td>org_model|open_ended</td>\n",
       "      <td>You are a neutral judge. Score the model’s ans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the main differences between Python a...</td>\n",
       "      <td>en</td>\n",
       "      <td>2</td>\n",
       "      <td>Python and JavaScript are two popular programm...</td>\n",
       "      <td>org_model|open_ended</td>\n",
       "      <td>You are a neutral judge. Score the model’s ans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can I increase my productivity while worki...</td>\n",
       "      <td>en</td>\n",
       "      <td>3</td>\n",
       "      <td>Here are some tips to help you increase your p...</td>\n",
       "      <td>org_model|open_ended</td>\n",
       "      <td>You are a neutral judge. Score the model’s ans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can you explain the basics of quantum computing?</td>\n",
       "      <td>en</td>\n",
       "      <td>4</td>\n",
       "      <td>Quantum computing is a rapidly growing field t...</td>\n",
       "      <td>org_model|open_ended</td>\n",
       "      <td>You are a neutral judge. Score the model’s ans...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text lang  __index_level_0__  \\\n",
       "0       How can I improve my time management skills?   en                  0   \n",
       "1  What are the most effective ways to deal with ...   en                  1   \n",
       "2  What are the main differences between Python a...   en                  2   \n",
       "3  How can I increase my productivity while worki...   en                  3   \n",
       "4   Can you explain the basics of quantum computing?   en                  4   \n",
       "\n",
       "                                              answer            model_type  \\\n",
       "0  Time management is the process of planning and...  org_model|open_ended   \n",
       "1  Stress is a normal part of life, but chronic s...  org_model|open_ended   \n",
       "2  Python and JavaScript are two popular programm...  org_model|open_ended   \n",
       "3  Here are some tips to help you increase your p...  org_model|open_ended   \n",
       "4  Quantum computing is a rapidly growing field t...  org_model|open_ended   \n",
       "\n",
       "                                        judge_prompt  \n",
       "0  You are a neutral judge. Score the model’s ans...  \n",
       "1  You are a neutral judge. Score the model’s ans...  \n",
       "2  You are a neutral judge. Score the model’s ans...  \n",
       "3  You are a neutral judge. Score the model’s ans...  \n",
       "4  You are a neutral judge. Score the model’s ans...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff787aef-5d7f-400d-81cd-ee5aff6e24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_final = datasets.Dataset.from_pandas(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5686ba14-4482-4e4e-a7d0-84046e9b8dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631e4e827f1c476793cd548e6d8b1d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "16234032"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_final.to_json('~/autodl-fs/LRP/open_ended_data_generation/20251201_LRP_based_generation4judge.json',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbbc9eb-0570-4509-90f5-b822e83f3a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bde4ba4b-3b8d-4654-b5d6-927f59e6b333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75682e8f5124b06ae9394778ec6259c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_judge_res = datasets.load_dataset('json', data_files = '/root/autodl-fs/LRP/open_ended_data_generation/20251201_LRP_based_generation4judge.json_gpt4o.json')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a84bd6b-d642-4c02-ac22-eba581eff820",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_judge_res = ds_judge_res['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad429b88-88b5-4b6a-8958-5712748e1bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>__index_level_0__</th>\n",
       "      <th>answer</th>\n",
       "      <th>model_type</th>\n",
       "      <th>judge_prompt</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I improve my time management skills?</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Time management is the process of planning and...</td>\n",
       "      <td>org_model|open_ended</td>\n",
       "      <td>You are a neutral judge. Score the model’s ans...</td>\n",
       "      <td>{\"score\": 10, \"reason\": \"The answer is correct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the most effective ways to deal with ...</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>Stress is a normal part of life, but chronic s...</td>\n",
       "      <td>org_model|open_ended</td>\n",
       "      <td>You are a neutral judge. Score the model’s ans...</td>\n",
       "      <td>{\"score\": 9, \"reason\": \"The answer is comprehe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text lang  __index_level_0__  \\\n",
       "0       How can I improve my time management skills?   en                  0   \n",
       "1  What are the most effective ways to deal with ...   en                  1   \n",
       "\n",
       "                                              answer            model_type  \\\n",
       "0  Time management is the process of planning and...  org_model|open_ended   \n",
       "1  Stress is a normal part of life, but chronic s...  org_model|open_ended   \n",
       "\n",
       "                                        judge_prompt  \\\n",
       "0  You are a neutral judge. Score the model’s ans...   \n",
       "1  You are a neutral judge. Score the model’s ans...   \n",
       "\n",
       "                                              output  \n",
       "0  {\"score\": 10, \"reason\": \"The answer is correct...  \n",
       "1  {\"score\": 9, \"reason\": \"The answer is comprehe...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_judge_res.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33c65e1b-7b9c-45a9-9545-17661fbda9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"score\": 10, \"reason\": \"The answer is correct, complete, clear, and useful. It provides a comprehensive list of practical tips for improving time management skills, including setting goals, using a planner, prioritizing tasks, and more. Each tip is explained clearly, making it easy for the reader to understand and apply.\"}'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_judge_res['output'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26150011-0b04-42fa-9e85-4d9407254425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get score\n",
    "def get_score(x):\n",
    "    try:\n",
    "        return  json.loads(x)['score']\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "df_judge_res['score'] = df_judge_res['output'].apply(get_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91fe1030-683c-4a5e-834e-e9e7a7c92330",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score\n",
       " 9     1057\n",
       " 8      521\n",
       " 3      241\n",
       " 7      210\n",
       " 4      152\n",
       " 2      152\n",
       " 6      143\n",
       " 1      128\n",
       " 5       64\n",
       " 10      51\n",
       "-1        8\n",
       " 0        3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_judge_res['score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fac6fa4-96b3-45c4-a6c2-fef7ab0c7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_judge_res['class_tpye'] = df_judge_res['lang']+'|' + df_judge_res['model_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d1c1530-d866-4e1a-b080-63cfcfb714cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class_tpye\n",
       "en|org_model|open_ended                      70\n",
       "vi|org_model|open_ended                      70\n",
       "zh|org_model|open_ended                      70\n",
       "en|bottom1perc_mask_en|open_ended            70\n",
       "vi|bottom1perc_mask_en|open_ended            70\n",
       "zh|bottom1perc_mask_en|open_ended            70\n",
       "en|bottom1perc_mask_vi|open_ended            70\n",
       "vi|bottom1perc_mask_vi|open_ended            70\n",
       "zh|bottom1perc_mask_vi|open_ended            70\n",
       "en|bottom1perc_mask_zh|open_ended            70\n",
       "vi|bottom1perc_mask_zh|open_ended            70\n",
       "zh|bottom1perc_mask_zh|open_ended            70\n",
       "en|gaprate_bottom1perc_mask_en|open_ended    70\n",
       "vi|gaprate_bottom1perc_mask_en|open_ended    70\n",
       "zh|gaprate_bottom1perc_mask_en|open_ended    70\n",
       "en|gaprate_bottom1perc_mask_vi|open_ended    70\n",
       "vi|gaprate_bottom1perc_mask_vi|open_ended    70\n",
       "zh|gaprate_bottom1perc_mask_vi|open_ended    70\n",
       "en|gaprate_bottom1perc_mask_zh|open_ended    70\n",
       "vi|gaprate_bottom1perc_mask_zh|open_ended    70\n",
       "zh|gaprate_bottom1perc_mask_zh|open_ended    70\n",
       "en|gaprate_top1perc_mask_en|open_ended       70\n",
       "vi|gaprate_top1perc_mask_en|open_ended       70\n",
       "zh|gaprate_top1perc_mask_en|open_ended       70\n",
       "en|gaprate_top1perc_mask_vi|open_ended       70\n",
       "vi|gaprate_top1perc_mask_vi|open_ended       70\n",
       "zh|gaprate_top1perc_mask_vi|open_ended       70\n",
       "en|gaprate_top1perc_mask_zh|open_ended       70\n",
       "vi|gaprate_top1perc_mask_zh|open_ended       70\n",
       "zh|gaprate_top1perc_mask_zh|open_ended       70\n",
       "en|top1perc_mask_en|open_ended               70\n",
       "vi|top1perc_mask_en|open_ended               70\n",
       "zh|top1perc_mask_en|open_ended               70\n",
       "en|top1perc_mask_vi|open_ended               70\n",
       "vi|top1perc_mask_vi|open_ended               70\n",
       "zh|top1perc_mask_vi|open_ended               70\n",
       "en|top1perc_mask_zh|open_ended               70\n",
       "vi|top1perc_mask_zh|open_ended               70\n",
       "zh|top1perc_mask_zh|open_ended               70\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_judge_res['class_tpye'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b34647b9-1d1e-4747-a48e-09c5051d234b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "imodel_type: en|org_model|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: vi|org_model|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: zh|org_model|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: en|bottom1perc_mask_en|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: vi|bottom1perc_mask_en|open_ended\n",
      "valid count: 69\n",
      "====================\n",
      "imodel_type: zh|bottom1perc_mask_en|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: en|bottom1perc_mask_vi|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: vi|bottom1perc_mask_vi|open_ended\n",
      "valid count: 69\n",
      "====================\n",
      "imodel_type: zh|bottom1perc_mask_vi|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: en|bottom1perc_mask_zh|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: vi|bottom1perc_mask_zh|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: zh|bottom1perc_mask_zh|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: en|gaprate_bottom1perc_mask_en|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: vi|gaprate_bottom1perc_mask_en|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: zh|gaprate_bottom1perc_mask_en|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: en|gaprate_bottom1perc_mask_vi|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: vi|gaprate_bottom1perc_mask_vi|open_ended\n",
      "valid count: 68\n",
      "====================\n",
      "imodel_type: zh|gaprate_bottom1perc_mask_vi|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: en|gaprate_bottom1perc_mask_zh|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: vi|gaprate_bottom1perc_mask_zh|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: zh|gaprate_bottom1perc_mask_zh|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: en|gaprate_top1perc_mask_en|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: vi|gaprate_top1perc_mask_en|open_ended\n",
      "valid count: 69\n",
      "====================\n",
      "imodel_type: zh|gaprate_top1perc_mask_en|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: en|gaprate_top1perc_mask_vi|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: vi|gaprate_top1perc_mask_vi|open_ended\n",
      "valid count: 69\n",
      "====================\n",
      "imodel_type: zh|gaprate_top1perc_mask_vi|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: en|gaprate_top1perc_mask_zh|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: vi|gaprate_top1perc_mask_zh|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: zh|gaprate_top1perc_mask_zh|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: en|top1perc_mask_en|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: vi|top1perc_mask_en|open_ended\n",
      "valid count: 69\n",
      "====================\n",
      "imodel_type: zh|top1perc_mask_en|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: en|top1perc_mask_vi|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: vi|top1perc_mask_vi|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: zh|top1perc_mask_vi|open_ended\n",
      "valid count: 69\n",
      "====================\n",
      "imodel_type: en|top1perc_mask_zh|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: vi|top1perc_mask_zh|open_ended\n",
      "valid count: 70\n",
      "====================\n",
      "imodel_type: zh|top1perc_mask_zh|open_ended\n",
      "valid count: 70\n"
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "for imodel_type in list(df_judge_res['class_tpye'].value_counts().keys()):\n",
    "    print('='*20)\n",
    "    print('imodel_type:', imodel_type)\n",
    "    df_tmp = df_judge_res.loc[df_judge_res['class_tpye'] == imodel_type].reset_index(drop=True)\n",
    "    df_tmp = df_tmp.loc[df_tmp['score']!=-1].reset_index(drop=True)\n",
    "    print('valid count:', len(df_tmp))\n",
    "    result_dict[imodel_type] = df_tmp['score'].mean()\n",
    "          \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6161b0-c697-4614-bdec-1a4166d11819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d610e18c-e461-424f-aa77-96671af065b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en|org_model|open_ended': 8.057142857142857,\n",
       " 'vi|org_model|open_ended': 6.9714285714285715,\n",
       " 'zh|org_model|open_ended': 6.828571428571428,\n",
       " 'en|bottom1perc_mask_en|open_ended': 7.014285714285714,\n",
       " 'vi|bottom1perc_mask_en|open_ended': 5.7101449275362315,\n",
       " 'zh|bottom1perc_mask_en|open_ended': 6.4,\n",
       " 'en|bottom1perc_mask_vi|open_ended': 7.7,\n",
       " 'vi|bottom1perc_mask_vi|open_ended': 7.072463768115942,\n",
       " 'zh|bottom1perc_mask_vi|open_ended': 7.085714285714285,\n",
       " 'en|bottom1perc_mask_zh|open_ended': 7.442857142857143,\n",
       " 'vi|bottom1perc_mask_zh|open_ended': 5.885714285714286,\n",
       " 'zh|bottom1perc_mask_zh|open_ended': 6.014285714285714,\n",
       " 'en|gaprate_bottom1perc_mask_en|open_ended': 7.0,\n",
       " 'vi|gaprate_bottom1perc_mask_en|open_ended': 5.8,\n",
       " 'zh|gaprate_bottom1perc_mask_en|open_ended': 6.357142857142857,\n",
       " 'en|gaprate_bottom1perc_mask_vi|open_ended': 7.985714285714286,\n",
       " 'vi|gaprate_bottom1perc_mask_vi|open_ended': 6.676470588235294,\n",
       " 'zh|gaprate_bottom1perc_mask_vi|open_ended': 6.5285714285714285,\n",
       " 'en|gaprate_bottom1perc_mask_zh|open_ended': 7.871428571428571,\n",
       " 'vi|gaprate_bottom1perc_mask_zh|open_ended': 5.771428571428571,\n",
       " 'zh|gaprate_bottom1perc_mask_zh|open_ended': 5.928571428571429,\n",
       " 'en|gaprate_top1perc_mask_en|open_ended': 7.757142857142857,\n",
       " 'vi|gaprate_top1perc_mask_en|open_ended': 7.028985507246377,\n",
       " 'zh|gaprate_top1perc_mask_en|open_ended': 6.7,\n",
       " 'en|gaprate_top1perc_mask_vi|open_ended': 7.8,\n",
       " 'vi|gaprate_top1perc_mask_vi|open_ended': 6.550724637681159,\n",
       " 'zh|gaprate_top1perc_mask_vi|open_ended': 6.628571428571429,\n",
       " 'en|gaprate_top1perc_mask_zh|open_ended': 7.942857142857143,\n",
       " 'vi|gaprate_top1perc_mask_zh|open_ended': 6.514285714285714,\n",
       " 'zh|gaprate_top1perc_mask_zh|open_ended': 6.8428571428571425,\n",
       " 'en|top1perc_mask_en|open_ended': 7.685714285714286,\n",
       " 'vi|top1perc_mask_en|open_ended': 6.971014492753623,\n",
       " 'zh|top1perc_mask_en|open_ended': 5.757142857142857,\n",
       " 'en|top1perc_mask_vi|open_ended': 7.9714285714285715,\n",
       " 'vi|top1perc_mask_vi|open_ended': 5.928571428571429,\n",
       " 'zh|top1perc_mask_vi|open_ended': 6.130434782608695,\n",
       " 'en|top1perc_mask_zh|open_ended': 7.642857142857143,\n",
       " 'vi|top1perc_mask_zh|open_ended': 6.242857142857143,\n",
       " 'zh|top1perc_mask_zh|open_ended': 6.3}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9dd32521-0ece-4d14-b87d-9049d5658566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 数据\n",
    "data =result_dict\n",
    "\n",
    "languages = ['en', 'vi', 'zh']\n",
    "masks = ['org_model', 'mask_en'] #, 'mask_vi', 'mask_zh']\n",
    "\n",
    "# 构建每个语种的分数序列\n",
    "scores = {lang: [data[f\"{lang}|{mask}|open_ended\"] for mask in masks] for lang in languages}\n",
    "\n",
    "# 画折线图\n",
    "plt.figure(figsize=(8,5))\n",
    "for lang in languages:\n",
    "    plt.plot(masks, scores[lang], marker='o', label=lang)\n",
    "\n",
    "plt.ylim(4, 8.5)\n",
    "plt.xlabel(\"Mask Type\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"mask trendy\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.legend(title=\"Language\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ba0aaa-118f-4cf8-a90f-8ce0b6ee53bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c05b8f7-8d6f-4a21-b33d-1be1d98c87e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b085ce7e-c333-4a0e-a671-a79bff718631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48aaea8-11c8-435b-9e14-e96c1f6b293b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc89a3c3-ead4-4020-9d0f-57a8a8499815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd42f55f-01cf-4d0a-92d0-a8e870389e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = ds_final.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f88f23c3-28fa-425d-9854-21608107679e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['answer'][210] == df_final['answer'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8f999b2-35b2-4415-a93b-070d8a822a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Time management is the ability to make the most of your time. It involves prioritizing tasks, setting goals, and managing distractions to increase productivity and efficiency. Here are some tips for improving your time management skills:\\n\\n1. Set clear goals: Establishing clear goals helps you focus on what needs to be done and how to achieve it. Write down your short-term and long-term goals and review them regularly to track progress.\\n2. Prioritize tasks: Determine which tasks are urgent and important, and allocate your time accordingly. Use the Eisenhower Matrix to categorize tasks into four quadrants: do first, schedule, delegate, and eliminate.\\n3. Create a schedule: Plan out your day, week, or month using a calendar or planner. Be realistic about how much time each task will take, and schedule breaks throughout the day.\\n4. Manage Distractions: Identify things that distract you during the day such as social media, email, or meetings, and find ways to minimize them. Use tools like website blockers or apps that help you stay focused.\\n5. Learn to say no: Be mindful of taking on too much at once, and learn to say no to tasks that are not aligned with your priorities or that you cannot realistically complete.\\n6. Take Breaks: Taking regular breaks throughout the day can help you recharge and come back to tasks with renewed energy and focus.\\n7. Evaluate and adjust: Regularly evaluate your time management strategy and make adjustments as needed. Reflect on what works well and what doesn't, and look for ways to improve your approach.\\n8. Stay Organized: Keep your workspace organized by having a designated place for everything and keeping your digital files organized.\\n9. Avoid multitasking: Multitasking can lead to decreased productivity and increased stress. Instead, focus on one task at a time and give it your undivided attention.\\n10. Use Technology: There are many tools available that can help you manage your time more effectively, such as project management software, time tracking apps, and reminders. Experiment to find what works best for you.\\n\\nBy implementing these strategies, you can improve your time management skills and make the most of your time. Remember, time management is a skill that takes practice, so be patient with yourself and keep working at it.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['answer'][210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46db2c36-86ef-48da-98e6-97c35efde956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Time management is the process of planning and controlling the amount of time spent on different activities to maximize efficiency and productivity. Here are some tips that can help you improve your time management skills:\\n\\n1. Set clear goals: Identify what you want to achieve and set specific, measurable, achievable, relevant, and time-bound (SMART) goals. This will help you prioritize your tasks and focus on what's important.\\n2. Use a planner or calendar: Write down all your tasks, appointments, and deadlines in a planner or calendar. This will help you keep track of your schedule and plan your day effectively.\\n3. Prioritize tasks: Determine which tasks are most important and need to be completed first. Use the Eisenhower Matrix to categorize tasks into urgent vs. important, and focus on the most critical ones first.\\n4. Break down large tasks: Divide big projects into smaller, manageable chunks. This will help you avoid feeling overwhelmed and make it easier to stay focused.\\n5. Eliminate distractions: Identify things that waste your time, such as social media, email, or meetings that don't add value, and eliminate them. Create a conducive work environment by turning off notifications, using noise-cancelling headphones, or finding a quiet space to work.\\n6. Learn to say no: Be mindful of taking on too much work or commitments. Learn to say no to requests that don't align with your goals or values, or that you cannot realistically fulfill.\\n7. Take breaks: It's essential to take breaks throughout the day to recharge and avoid burnout. Schedule breaks into your day, and use them to do something enjoyable or relaxing.\\n8. Delegate tasks: If possible, delegate tasks to others to free up time for more important tasks or to reduce your workload.\\n9. Stay organized: Keep your workspace organized, and file away documents and papers when you're done with them. This will save you time searching for information and reduce stress.\\n10. Review and adjust: Regularly review your progress, and adjust your time management strategy as needed. Reflect on what works well and what doesn't, and make changes accordingly.\\n\\nRemember, improving your time management skills takes practice, so be patient and persistent. With consistent effort, you can become more efficient and productive, and achieve your goals more quickly.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['answer'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b949cc5-8e4e-46c3-815c-0dc92450fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model, tokenizer = load_model('/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-chat-hf') #load_model('/root/autodl-fs/model_zoo/google/gemma-3-1b-it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1df2af-da89-4166-93f7-c3088168ba92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b25c708-3f51-404a-a756-4b168316667b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4e093e4-e3ed-4a02-b927-c13271c0c70a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fa72c50-56b8-4daa-9ac3-701a96b5578b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6b3edc6-1539-40d1-a7ac-a4d60048f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_en = torch.load('/root/autodl-fs/LRP_kur_res/20251202_llama2_7b_chat/bottom1perc_LRP_kur_res_en.pt', weights_only=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb795a5a-2bdd-4ba6-9de0-5906aaa6cbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e993927d-8e9f-4bc7-9515-33f922790baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'up_proj': [7391, 8880, 6352, 9710, 4395, 9686, 4445, 9027, 4746, 7377, 192, 5964, 9755, 912, 7441, 7027, 1079, 3769, 11004, 3055, 1169, 10866, 7775, 2797, 945, 6636, 2171, 119, 3054, 8777, 3122, 8569, 322, 1366, 6248, 5522, 9258, 1290, 9414, 4062, 4858, 10824, 1449, 3734, 3108, 575, 4093, 2295, 4442, 1806, 6344, 6748, 628, 2615, 1671, 8595, 4854, 5410, 5683, 5519, 5161, 78, 9805, 3038, 4894, 5608, 186, 10500, 10776, 2983, 3526, 6395, 8503, 1718, 4326, 7502, 948, 6635, 7187, 8264, 5511, 3174, 1335, 6867, 5731, 10969, 835, 8489, 46, 10839, 10178, 2192, 1468, 3025, 6892, 7976, 2701, 9689, 8723, 3865, 9682, 2755, 7323, 4446, 317, 6517, 8900, 6276, 2253], 'gate_proj': [9710, 9651, 119, 9258, 10642, 6867, 2427, 4912, 9016, 6892, 6622, 7795, 9686, 1820, 3122, 322, 4599, 4729, 4746, 3356, 688, 626, 7377, 4395, 6657, 7463, 1169, 3865, 5964, 5493, 1070, 8252, 3956, 8224, 2134, 4806, 10578, 9822, 6179, 4953, 3146, 1400, 5612, 10551, 10435, 10550, 6155, 2983, 5417, 1245, 6396, 1079, 1335, 3178, 4945, 4244, 6646, 4985, 6395, 4445, 9469, 8049, 3576, 4093, 1776, 5042, 7851, 2316, 6591, 6919, 8655, 5457, 7341, 3912, 2312, 7417, 3418, 10095, 1671, 2094, 7032, 1419, 4288, 8880, 9896, 9115, 8569, 848, 6327, 7627, 1006, 8984, 8944, 5491], 'down_proj': [3648, 264, 1936, 4009, 1145, 2409, 261, 1931, 2130, 950, 2384, 183, 2591, 2526, 653, 203, 3133, 1475, 2932, 4031, 3533, 1045, 2247, 3333, 2771, 460, 2877, 2996, 7, 620, 2702, 3861, 3608, 3231, 714, 516, 2967, 1795, 2252, 224, 1964, 1369, 1573, 2978, 2095, 571, 65, 2429, 608, 3598, 631, 590, 3571, 242, 3447, 1000, 2235, 2927, 1728, 1581, 2563, 979, 2733, 1428, 497, 1578, 94, 2511, 1729, 1384, 2199, 3709, 3736, 2639, 2545, 2477, 1995, 4007, 3771, 888, 2475, 3814, 3556, 319, 2676, 2282, 3040, 1625, 220, 2160, 2852, 4081, 436, 1956, 93, 1926, 3232, 1232, 1652, 3171, 1184, 2912, 1468, 1857, 3083, 1808, 899, 1797, 3118, 3668, 282, 3112, 2049, 4048, 2881, 1527, 1452, 2756, 3732, 3066, 1389, 3254, 1427, 1431, 2607, 246, 1424, 1646, 2836, 3555, 3366, 1195, 4022, 2669, 110, 1135]}, {'up_proj': [4893, 257, 7085, 137, 940, 238, 1935, 3924, 241, 9430, 1533, 9241, 4817, 43, 2366, 731, 8285, 5992, 1147, 3569, 10077, 11000, 8124, 9959, 10961, 3580, 9247, 10048, 7261, 10412, 1237, 10798, 4375, 1817, 10974, 10612, 6708, 10337, 8276, 10015, 4533, 894, 1475, 3993, 6109, 7239, 8834, 3634], 'gate_proj': [4786, 5067, 4574, 2115, 7721, 3634, 4300, 9492, 6497, 6234, 11000, 4533, 2340, 5335, 3215, 3569, 1134, 9121, 5923, 5813, 504, 8873, 2348, 6389, 105, 7739], 'down_proj': [945, 186, 80, 1812, 2621, 463, 2323, 4039, 1741, 2752, 862, 2217, 1044, 3031, 124, 2, 2532, 736, 1987, 3200, 1163, 65, 3220, 294, 558, 86, 1914, 3240, 2205, 1962, 3086, 1541, 616, 3103, 3074, 2165, 3071, 832, 1705, 3582, 2920, 141, 51, 230]}, {'up_proj': [10974, 1058, 2623, 5742, 10595, 6547, 8665, 10150, 2609, 9632, 10059, 1305, 10831, 6580, 10904, 8204, 2736, 326, 9411, 11007, 10743, 9165, 1094, 482, 6664, 387, 6665, 10468, 9307, 1558, 8997], 'gate_proj': [1856, 5259, 10188, 837, 387, 7479, 1629, 9817, 3830, 10831, 5054, 6057, 5478, 9583, 6286, 6408, 2190, 2252, 9503, 1812], 'down_proj': [3078, 528, 1589, 1590, 2261, 2291, 1320, 2820, 203, 360, 1623, 94, 1619, 2139, 2207, 619, 217, 1366, 3751, 1563, 1242, 1456, 944]}, {'up_proj': [895, 6807, 3598, 675, 8167, 9213, 3359, 10526, 3955, 124, 1752, 11006, 961, 8680, 5018, 3842, 4426, 10664, 6659, 10883, 1082, 18, 4627, 934, 6538, 3805, 5814, 7379, 8565, 9612, 1683, 8187, 1908, 6212, 7299], 'gate_proj': [8487, 8502, 9598, 8272, 8565, 243, 7210, 3693, 10998, 6128, 1491, 6474, 10526, 553, 10565, 7214, 675, 11002, 3145, 3414], 'down_proj': [3384, 1159, 417, 2393, 2335, 3209, 4015, 3333, 907, 781, 2841, 592, 310, 3609, 3406, 2789]}, {'up_proj': [3661, 2982, 4505, 1542, 8462, 10965, 5895, 9488, 6522, 7981, 8865, 5271, 5917, 1028, 7031, 9949, 4138, 4924, 8422, 2524, 3300, 8177, 10711, 10881, 5912, 4747, 2281, 156, 1762, 620, 2390, 8082, 5621, 8623, 10255, 5715, 1328, 7879, 4378, 9228, 7871, 5708, 5060], 'gate_proj': [2982, 738, 1542, 3672, 3784, 9259, 2052, 9266, 1608, 6929, 3179, 4090, 7031, 544, 10262, 2842, 1334, 10589, 2281, 4846, 4167, 9714, 8422, 102, 8342, 2039, 1405, 1874, 7011, 2170, 905, 1549, 8220, 6375, 4697, 2593], 'down_proj': [747, 1512, 853, 1964, 481, 3868, 1894, 1335, 1859, 2246, 2779, 1439, 3431, 2745, 3792, 341, 2190, 2319, 861, 3430, 339, 2533, 2052, 2126, 2902]}, {'up_proj': [4613, 7702, 7461, 2050, 6867, 1974, 4856, 7562, 6628, 2676, 7889, 6367, 5037, 1824, 9564, 100, 5115, 5572, 4361, 10079, 4290, 7444, 10344, 9107, 3672, 3764, 8114, 528, 4388, 9182, 5543, 4792, 2663, 7526, 1445, 2983, 8543, 1201, 9359, 10619, 2623, 7325, 4970, 4459, 8440, 8498, 8691, 7425, 878, 10396, 2898, 6614, 3722, 6504], 'gate_proj': [9273, 6246, 4424, 5705, 4138, 5115, 571, 6859, 2676, 10638, 432, 3768, 4972, 1645, 2682, 124, 5062, 6042, 8876, 3896, 6973, 1889, 328, 4314], 'down_proj': [2023, 3230, 1115, 3431, 2789, 3673, 2825, 2050, 2749, 1076, 1246, 141, 2350, 2760, 4007, 1545, 618, 555, 3736, 926, 2168, 2870, 2393, 3661, 2469, 4094, 360, 641, 3937, 2848, 1300, 1702, 2014, 927, 1576, 1396, 747, 2747, 1512, 976, 1610, 3667, 590, 3780, 1567, 3742, 1621, 3570, 3994, 2361, 3586, 950, 3959, 2401, 2615, 2533, 2745, 1238, 3766, 914, 672, 1653, 2158, 339, 3209, 2641, 4084, 1737, 2828, 2488, 2190, 849, 3919, 2219, 1685, 2504, 2104, 3793, 606, 87, 575, 2424, 490, 3466, 679]}, {'up_proj': [10686, 233, 5373, 9544, 10487, 3294, 10268, 3412, 10639, 7438, 2904, 7110, 10950, 4575, 6405, 7050, 5097, 8291, 5466, 5779, 6769, 2812, 329, 10931, 1316, 8372, 8820, 3726, 7967, 7496, 2687, 8413, 4661, 3657, 9007, 4237, 9423, 2075, 10095, 3101, 10066, 5322, 10595, 8233, 9013, 9776, 5465, 9110, 5454, 6122, 688, 5703, 1304, 9943, 2378, 4043, 8087, 2205, 6164, 3643, 3745, 9365, 1771, 10530, 5895, 3374, 2050, 8780, 264, 2399, 4094, 2969, 10677, 1822, 8058, 5252], 'gate_proj': [7050, 5511, 5107, 3214, 8068, 5864, 8485, 5166, 8372, 3034, 10686, 10246, 6246, 3722, 510, 9463, 5289], 'down_proj': [337, 2298, 2050, 39, 3877, 3209, 2126, 324, 3570, 2750, 3951, 155, 85, 1251, 1585, 3431, 94, 1023, 996, 1568, 393, 1095, 869, 2792, 3236, 3789, 3147, 906, 2210, 3323, 76, 2211, 2789, 1272, 2825, 3269, 2512, 2740, 1537, 1651, 2244, 55, 2482, 3137, 661, 839, 2393, 3139, 3229, 1512, 1661, 3874, 1492, 2927, 671, 2155, 656, 1885, 2332, 2559, 40, 1916, 1043, 1075, 2760, 1841, 1249, 1061, 2772, 2890, 3994, 2304, 3647, 16, 2300, 1952, 2353, 2175, 3189, 1717, 417, 2962, 2533, 1802, 1760, 3780, 3336, 1300, 1598, 957, 287, 976, 3135, 242, 13, 2208, 3611, 1074, 3424, 2795, 2689, 1384, 672, 2283, 1530, 2749, 850, 162, 1737, 2983, 2841, 868, 2044, 1890, 1809, 2715, 2525, 2919, 3628, 2936, 2243, 1483, 3281, 2939, 2912, 41, 2920, 1482, 1758, 946, 4026, 1497, 508, 2829, 2972, 1394, 72, 1002, 37, 2476, 1241, 785, 983, 1076, 2880, 328, 1265, 970, 2812, 620, 2013, 3082, 2513, 2335, 481, 507, 1902, 4014, 2669, 560, 451, 3885, 2418, 3887, 1954, 954, 1232, 3537, 3291, 2394, 1859, 4022, 2699, 575, 2848, 4000, 3918, 1647, 102, 339, 854, 832, 3524, 3095, 1735, 1306, 3514, 3306, 2421, 2970, 3577, 2356, 4056, 149, 1544, 196, 955, 1677, 169, 590, 1509, 2212, 1486, 2124, 3471, 802, 2536, 1616, 3230, 2797, 631, 3105, 3794, 2460, 515, 1329, 2100, 880, 2827, 2728, 2070, 1268, 3736, 3870, 3648, 1707, 38, 4083, 1793, 3332, 1352, 1404, 1874, 1620, 2459, 3019, 3767, 137, 113, 2664, 1439, 2449, 2479, 2245, 2354, 234, 1868, 3696, 3921, 1670, 3630, 2468, 1172, 2158, 2696, 2631, 2063, 1316, 2677, 483, 1924, 3165, 3451, 1950, 3491, 17, 3, 2497, 191]}, {'up_proj': [5794, 3832, 9283, 6345, 3285, 735, 5412, 2180, 6452, 10271, 7679, 1631, 3723, 7881, 2084, 5578, 5514, 874, 10203, 6756, 3384, 7339, 3276, 3944, 2111, 8558, 3753, 1458, 8812, 2805, 10013, 10360, 9606, 868, 2528, 10257, 10750, 10474, 1274, 1910, 9823, 8220, 8388, 3046, 2344, 4851, 2418, 2174, 3544, 5151, 1423, 1520, 2940, 8389, 4293, 656, 2199, 10677, 7360, 1928, 2486, 10396, 9447, 7403, 10263, 7431, 8285, 3663, 8886, 7022, 5567, 6583, 1503, 4137, 8605, 9033, 4504, 8450, 1603, 9092, 10052, 7964, 9553, 2306, 108, 6141, 9768, 3505, 6714, 10221, 7716, 9736, 6229, 159, 10075, 4395, 9141, 3213, 5486, 10778, 6898, 7468, 3277, 44, 6981, 5482, 3981, 1178, 6613, 10679, 8077, 7653, 5897, 6965, 9482], 'gate_proj': [3832, 8731, 2111, 3723, 95, 10673, 9875, 3466, 3823, 2928, 10935, 485, 4742, 3969, 2357, 8605, 2238, 6418, 7282, 865, 385, 1758, 790, 6093, 1807, 5463, 10638, 5448, 1644, 8803, 2255, 993, 3617], 'down_proj': [3230, 1512, 1260, 2789, 2747, 3909, 2393, 1339, 1422, 149, 96, 1153, 1040, 3611, 2014, 2714, 3234, 76, 339, 3748, 3137, 2519, 1300, 2792, 3431, 921, 233, 2465, 2862, 607, 580, 3716, 2220, 2595, 3209, 2533, 3930, 3586, 2246, 3516, 94, 839, 1800, 854, 2023, 2637, 1661, 763, 2091, 2733, 661, 4026, 1483, 2529, 2243, 1836, 1739, 3632, 2048, 575, 1726, 3198, 2749, 3766, 2469, 2378, 606, 1767, 3781, 1272, 1873, 1184, 1737, 2699, 3673, 3667, 3972, 2515, 491, 2421, 3874, 2851, 3589, 2577, 1074, 1377, 3339, 2561, 4014, 3742, 1068, 2190, 4018, 775, 680, 4076, 3626, 2825, 671, 964, 535, 3408, 3851, 2126, 1549, 3877, 2245, 975, 1358, 1151, 2211, 1310, 1809, 2212, 3807, 2219, 917, 3135, 2297, 4084, 2158, 749, 2168, 180, 2024, 914, 2556, 3793, 1335, 3565, 1043, 1583, 3871, 2291, 359, 2034, 1014, 868, 2205, 813, 331, 616, 2641, 909, 1902, 1238, 2945, 3424, 2623, 3286, 3215, 3269, 2036, 1746, 1844, 2664, 1702, 3754, 3960, 2509, 3610, 2098, 2864, 1061, 1868, 2883, 1952, 2357, 2335, 3576, 169, 3945, 3932, 2534, 1482, 2431, 1306, 1188, 2870, 1568, 3229, 957, 3767, 1317, 2338, 3430, 3761, 1103, 361, 2395, 2244, 1251, 3196, 3491, 3923, 1981, 1076]}, {'up_proj': [1530, 2198, 10035, 618, 10403, 3452, 10898, 7105, 8834, 1308, 6442, 10156, 9165, 3217, 8887, 7756, 6602, 4765, 352, 7403, 2903, 1796, 3604, 8226, 6084, 5800, 7908, 802, 5496, 10313, 7461, 6598, 6950, 7907, 5730, 2920, 1298, 1855, 8152, 10780, 8225, 6132, 10872, 4433, 7738, 2469, 2189, 3672, 10623, 6814, 8598, 8613, 3366, 3121, 10940, 2588, 7006, 1505, 7129, 4213, 3415, 1368, 5567, 5513, 9248, 1996, 51, 9057, 8446, 2683, 4618, 6962, 8667, 1041, 9544, 2179, 5556, 9130, 1705, 6517, 6655, 10229, 6651, 7272, 4840, 3199, 5503, 6291, 2130, 8066, 10359, 2664, 1524, 2451, 10666, 2841, 9368, 3593, 9720, 2802, 5673, 2010, 8561, 10580, 2670, 5092, 10477, 9916, 8464, 8705, 5451, 2060, 9625, 7689, 9402, 8058, 10320, 3977, 8410, 8220, 10318, 8830, 10278, 4174, 10722, 10997, 8559, 1310, 9302, 1733, 5565, 8969, 10004, 4911, 2202, 10871, 8404, 3228, 8106, 2922, 2544, 8886, 10105, 8047, 5209, 10489, 512, 3084, 4211, 10503, 9622, 4728, 8286, 4769, 9014, 94, 10534, 9575], 'gate_proj': [1622, 4816, 2145, 8279, 2806, 6013, 3088, 5693, 2621, 2396, 5944, 4530, 3071, 10875, 4916, 776, 9498, 9720, 1072, 1614, 2866, 4828, 1921, 7502, 286, 9987, 6300, 8602, 10043, 3121, 6879, 951, 10318, 4934, 6669, 10210, 2186, 7899, 3067, 4781, 1800, 8212], 'down_proj': [408, 662, 3147, 1512, 2789, 3716, 287, 1300, 1661, 167, 1153, 2358, 1055, 914, 315, 1994, 4000, 2927, 1208, 3994, 3689, 2825, 1585, 1729, 3139, 2393, 1772, 3859, 1061, 2533, 1702, 2421, 1737, 1952, 443, 2792, 4037, 540, 1747, 3857, 555, 975, 607, 3684, 2747, 3249, 4066, 3685, 3589, 4035, 2812, 3493, 181, 3431, 3361, 983, 508, 2864, 1486, 86, 2171, 2272, 2015, 810, 4001, 2014, 1651, 1335, 2297, 2332, 127, 2168, 3426, 3397, 2353, 2595, 1895, 2177, 1272, 1537, 149, 1719, 1427, 3806, 700, 2905, 2469, 1684, 749, 503, 839, 1396, 791, 3387, 1919, 864, 189, 1523, 2959, 3234, 2212, 3471, 1749, 3442, 3408, 3793, 3364, 490, 2920, 1839, 3780, 588, 2778, 230, 3561, 1509, 1744, 921, 3877, 2027, 3767, 2578, 850, 3942, 3641, 1841, 2870, 3233, 2243, 2822, 755, 2976, 3996, 1091, 2136, 3761, 2655, 138, 441, 1004, 2327, 3432, 339, 225, 695, 3350, 813, 2401, 1847, 2112, 1868, 1288, 341, 1358, 245, 3945, 1250, 3219, 2178, 594, 2515, 3491, 371, 234, 1637, 3730, 1591, 3230, 2183, 2220, 2366, 3424, 3327, 957, 2157, 908, 631, 2732, 636, 1217, 3369, 1316, 270, 616, 2925, 1452, 802, 779, 3870, 3404, 1339, 1937, 2750, 2725, 1568, 1349, 3667, 23, 2052, 3276, 3707, 3818, 3169, 2211, 491, 1115, 3934, 1220, 3114, 1894, 3283, 310, 3376, 3066, 2873, 3378, 1032, 94, 2246, 1708, 4084, 2688, 2748, 481, 763, 2890, 112, 4075, 3570, 212, 939, 3166, 2975, 3886, 2921, 2749, 661, 663, 870, 1726, 2745, 2820, 2526, 412, 3111, 3339, 1068]}, {'up_proj': [2425, 1457, 9818, 5133, 8362, 1261, 498, 6288, 1527, 6227, 10020, 7260, 857, 5027, 6990, 712, 2347, 6192, 7298, 3608, 7161, 1696, 1854, 9807, 2896, 7487, 3682, 6263, 7402, 8165, 513, 7200, 7146, 1244, 1342, 3520, 4729, 299, 1980, 4775, 8709, 3721, 8449, 2070, 10709, 6585, 9019, 8398, 3186, 844, 3182, 4675, 7918, 5944, 1676, 10057, 9013, 5508, 10420, 5290, 4941, 6276, 411, 3877, 7535, 1378, 10186, 9786, 3579, 5499, 5590, 3805, 10869, 8473, 456, 161, 4442, 10530, 1317, 7685, 10227, 9516, 6249, 1384, 8152, 1132, 5393, 2535, 7401, 687, 8769, 6530, 2219, 6420, 6781, 462, 9341, 2227, 5796, 1971, 1114, 3888, 1846, 7137, 7847, 10506, 1026, 4322, 830, 9470, 2475, 8448, 6632, 7184, 5855, 1788, 7998, 7417, 4908, 10710, 1398, 6962, 4363, 10678, 7366, 10776, 6360, 8814, 1875, 7063, 10109, 2387, 10838, 3421, 7679, 8850, 818, 2191, 7217, 951, 10629, 4872, 3243, 5687, 10435, 3113, 6760, 10999, 10847, 2593, 8136, 5872, 1785, 1364, 3595, 10026, 5261, 4928, 1296, 8867, 10033, 5319, 5376, 4412, 10134, 937, 8668, 6896, 9583, 4478, 9677, 975, 9158, 3004, 5018, 1030, 10372, 1480, 10976, 6465, 8593, 5995, 899, 1897, 10774, 3781, 2465, 2663, 5698, 10793, 702, 674, 200, 141, 4072, 2017, 2591, 267, 2808, 8672, 225], 'gate_proj': [5325, 10066, 2396, 9979, 6640, 1261, 4096, 6714, 10862, 7762, 3450, 3113, 9026, 1688, 1514, 2628, 6670, 7736, 7918, 1792, 4455, 1943, 7296, 59, 1193, 435, 4732, 8898, 2221, 242, 3683, 2866, 4361, 610, 516, 5261, 10812, 1266, 7896, 9169, 3794, 4824, 10582, 431, 2287, 5105, 1415, 791, 8730, 8063, 982, 5433, 855, 5783, 5875, 5133], 'down_proj': [1300, 680, 2168, 2870, 3083, 2393, 1512, 3671, 3951, 141, 3231, 2533, 3135, 2664, 2230, 2493, 3431, 2789, 2412, 1395, 2305, 2549, 588, 1817, 4000, 2561, 16, 621, 339, 491, 2048, 1272, 1025, 3364, 1254, 2556, 2792, 3209, 1061, 2469, 1719, 1993, 2205, 421, 1565, 894, 3553, 3326, 40, 1081, 2126, 3015, 3230, 167, 2747, 3408, 946, 3672, 4003, 856, 1209, 3741, 2298, 393, 3113, 3709, 1486, 4075, 3976, 2357, 1826, 3101, 2669, 2022, 2623, 225, 2177, 3491, 3662, 755, 607, 127, 2209, 1957, 1915, 3205, 1637, 331, 1153, 3249, 2520, 1825, 149, 434, 1411, 2880, 1193, 1901, 1555, 1668, 230, 1919, 2395, 807, 490, 1235, 260, 975, 521, 3639, 593, 1055, 2341, 2024, 2825, 3495, 3768, 972, 785, 1169, 2622, 2919, 2367, 2822, 3959, 2584, 1044, 21, 540, 2224, 2820, 527, 546, 396, 3913, 2096, 3874, 1354, 2591, 2732, 1339, 2037, 2270, 3691, 3767, 2660, 1115, 1692, 2301, 1515, 3361, 3673, 3028, 259, 3089, 1968, 879, 3271, 2642, 3372, 1927, 1633, 2087, 1236, 2406, 2944, 1809, 505, 1864, 3471, 684, 1969, 1260, 2867, 2883, 1601, 987, 2857, 451, 3789, 3317, 3252, 1708, 1425, 1528, 2725, 2324, 1942, 783, 847, 2927, 3645, 3236, 125, 695, 618, 3909, 4040, 1208, 499, 1999, 3092, 3960, 545, 162, 2526, 751, 1807, 3475, 2543, 3581, 574, 772, 2943, 2589, 417, 2563, 3998, 2271, 2476, 3589, 2236, 3257, 3742, 26, 863, 1306, 1737, 3894, 1562, 2158, 5, 3808, 55, 675, 3570, 3829, 1072, 3139, 3158, 4085, 2576, 2837, 126, 118, 461, 471, 1356, 3835, 4032, 1646, 4070, 2745, 2350, 3749, 954, 2787, 3130, 2735, 3980, 924, 3137, 3876, 3848, 3666, 2147, 3766, 1606, 3872, 2625, 3019, 2681, 1042, 2063, 2756, 2497, 2650, 3615, 3291, 1586, 2728, 1902, 1404, 500, 1483, 460, 420, 4035, 2042, 1556, 2286, 543, 2759, 27, 117, 945, 1895, 2148, 628, 3930, 361, 809, 2863, 2596, 2394, 786, 1062, 255, 594, 2323, 752, 1113, 611, 3464, 1455, 1427, 3185, 3539, 776, 3847, 63, 2143, 2485, 1504, 3972, 4092, 3911, 2467, 1684, 1319, 1004, 1608, 2688, 2670, 1579, 2002, 3512, 3661, 2017, 2136, 413, 3498, 2375, 2219, 3026, 548, 1439, 3649, 2582, 2376, 3264, 47, 3065, 3332, 1568, 2290, 2689, 3549, 1906, 849, 652, 1591, 2023, 2676, 3428, 3189, 1931, 1114, 474, 1359, 2612, 1868, 274, 714, 3199, 555, 3014, 3234, 195, 3136, 3598, 328, 1330, 363, 4001, 1255, 2157, 2770, 3586, 3493, 3711, 2573, 3286, 1265, 802, 2757, 1923, 1207, 3470, 624, 3267, 2437, 4084, 3359, 681, 1134, 2277, 351, 1383, 359, 650, 3229, 2334, 1613, 3116, 3020, 2273, 3060, 2353, 592, 2190, 2631, 2848, 1854, 2922, 119, 2327, 2958, 674, 4094, 3836, 560, 287, 3541, 3948, 169, 1721, 189, 3352, 2801, 3350, 3996, 1671, 58, 3696, 2484, 1370, 759, 1729, 2592, 3328, 2192, 1345, 2251, 1223, 2318, 2748, 3309, 315, 3265, 2773, 3396, 2280, 32, 3227, 408, 2715, 1580, 3453, 2067, 41, 2957, 2210, 2827, 1585, 146, 977, 2066, 156, 980, 2908, 4017, 2252, 3457, 535, 2155, 1760, 3318, 616, 3284, 1607, 2074, 3356, 3781, 1583, 1533, 2515, 2633, 2282, 4072, 2243, 1877, 874, 2788, 1167, 1328, 733, 3879, 1110, 3994, 3369, 3000, 2509, 2171, 3707, 3699, 950, 1996, 2585, 2888, 113, 109, 2423, 2730, 3619, 3814, 2133, 3895, 1068, 3259, 636, 2931, 1941, 1711, 2790, 1893, 3503, 2130, 921, 3915, 245, 2862, 1094]}, {'up_proj': [997, 8962, 965, 4478, 3436, 6377, 8120, 6179, 5516, 4330, 10765, 8296, 9716, 8078, 3368, 2752, 3403, 7318, 4722, 6030, 6234, 5266, 1039, 936, 2326, 2712, 3741, 4342, 7281, 6188, 4056, 10779, 2254, 4579, 9466, 5728, 2261, 8532, 2487, 3294, 9570, 3639, 6017, 8587, 93, 57, 9050, 7442, 4612, 4710, 10140, 5795, 8604, 7239, 1993, 7015, 3132, 558, 4521, 6217, 10778, 9800, 6305, 10219, 3255, 2611, 10820, 2994, 3847, 4952, 1911, 10028, 3146, 4130, 7683, 1721, 1021, 2269, 5909, 4543, 3760, 4667, 9671, 5425, 4736, 5996, 7274, 10822, 8354, 2104, 10142, 9856, 2055, 403, 2699, 9021, 9428, 4593, 2922, 8595, 124, 7561, 2029, 1304, 1129, 2228, 2051, 8062, 9496, 9798, 6162, 2145, 6582, 9109, 4820, 6060, 9503, 10111, 1518, 1041, 8494, 2979, 10940, 8735, 9868, 9654, 5943, 8993, 7383, 7696, 4604, 5905, 9463, 3029, 3184, 10576, 934, 2086, 2278, 3175, 6715, 3380, 5989, 5498, 3188, 1368, 2697, 6418, 10202, 6416, 4123, 5335, 2244, 8188, 4877, 3635, 4745, 9793, 1004, 3484, 10043, 3750, 3442, 10624, 5832, 7823, 10713, 2980, 907, 122, 10590, 3500, 4367, 9291, 5242, 6088, 10412, 5630, 3162, 1144, 7168, 6326, 4959, 5505, 10323, 9952, 2530, 10505, 10810, 1765, 171, 2320, 2568, 6541, 10363, 9359, 6113, 2311, 9166, 9435, 8332, 10783, 7858, 6753, 9753, 3451, 2783, 9481, 2255, 7651, 1951, 2591, 9698, 3805, 10375, 4259, 3400, 2136, 1630, 6407, 9488, 7528, 10914, 10190, 10898, 9985, 3081, 5562, 4149, 2306, 951, 5736, 2888, 6764, 75, 7517, 10275, 4716, 5724, 1372, 8368, 272, 5839, 10579], 'gate_proj': [2846, 4716, 8964, 8738, 3550, 4831, 599, 5644, 66, 3027, 4787, 5715, 917, 8721, 5710, 8548, 3505, 6838, 10672, 1380, 5173, 10147, 7382, 6186, 7990, 4776, 7486, 2021, 5, 592, 892, 8450, 4379, 9503, 9580, 8007, 7329, 7, 3113, 10971, 3017, 3202, 3597, 7383, 9231, 781, 10538, 9945, 2051, 7336], 'down_proj': [1153, 1633, 3570, 779, 3960, 1692, 3618, 3350, 2393, 1298, 1377, 76, 1339, 1207, 2376, 3343, 149, 851, 2669, 3431, 3364, 2664, 3542, 451, 3084, 2648, 2478, 3687, 3166, 1404, 3264, 3723, 2357, 3615, 1512, 2822, 3475, 887, 731, 2748, 752, 1424, 2006, 2533, 1224, 2469, 1226, 3249, 3774, 2418, 2514, 2789, 1115, 1467, 3015, 3942, 2396, 2168, 3661, 3230, 1110, 3778, 1042, 900, 421, 2270, 964, 3318, 1613, 1993, 1365, 1081, 3794, 3995, 1868, 2792, 1825, 2526, 1055, 3361, 1163, 336, 3709, 2015, 2276, 1205, 1356, 2595, 2829, 2244, 4070, 4000, 1004, 2732, 1014, 2273, 3978, 3231, 1600, 1926, 3645, 1668, 2927, 4084, 863, 1507, 2039, 126, 807, 3305, 5, 3038, 1477, 616, 1737, 2747, 2825, 3691, 502, 3972, 142, 3589, 1354, 2050, 1619, 526, 2749, 753, 339, 759, 230, 4035, 3989, 3793, 3151, 242, 595, 2735, 109, 490, 733, 1427, 1260, 1332, 2230, 10, 3829, 578, 3959, 1830, 975, 1265, 3707, 1862, 1256, 2220, 154, 3945, 2908, 555, 264, 4014, 1628, 496, 3722, 3888, 2271, 979, 1104, 833, 3306, 2486, 1796, 3847, 408, 1962, 2812, 2500, 3996, 3236, 516, 1454, 40, 3894, 1255, 2758, 169, 1324, 1300, 3189, 1662, 2433, 3781, 4095, 330, 945, 3223, 1904, 3541, 3832, 20, 916, 1035, 1636, 1735, 3432, 4032, 2300, 3981, 700, 2126, 3675, 1175, 741, 2282, 1112, 2280, 658, 1912, 2236, 2205, 2820, 2810, 2070, 3513, 2600, 2880, 3980, 576, 527, 1302, 1708, 2363, 2615, 1422, 3770, 3886, 679, 1759, 27, 2958, 119, 2485, 388, 274, 1998, 4056, 3703, 2437, 2625, 3826, 3502, 922, 2763, 3195, 2140, 1134, 3814, 2725, 2855, 813, 856, 2556, 3985, 2170, 2790]}, {'up_proj': [9665, 10629, 13, 7634, 2108, 1459, 8753, 9046, 652, 6534, 5468, 2393, 7539, 9794, 7080, 3977, 5363, 2170, 9805, 2246, 10947, 2164, 9192, 1795, 4731, 1556, 8037, 2639, 8538, 1844, 1057, 285, 3100, 4740, 5955, 2907, 10921, 3571, 8870, 9852, 1942, 3080, 8858, 4922, 9431, 8327, 3164, 8739, 3315, 5720, 2145, 387, 1310, 136, 3448, 2727, 1656, 159, 10365, 5236, 9899, 9146, 4171, 3733, 1642, 7004, 992, 3493, 831, 2537, 2869, 6843, 6393, 8780, 3986, 5628, 3446, 10436, 659, 5060, 9450, 5267, 1908, 9535, 8211, 2459, 2692, 10437, 9155, 6227, 7512, 4554, 10087, 7480, 2750, 5503, 8666, 6148, 1526, 403, 1516, 6850, 10526, 9494, 7502, 5625, 8368, 6194, 3224, 342, 2450, 8325, 6684, 3620, 1262, 1270, 5499, 8570, 10758, 2425, 6987, 1280, 5674, 6645, 7907, 8284, 9693, 5530, 3833, 9201, 5160, 9595, 9953, 941, 9013, 5696, 7119, 1651], 'gate_proj': [10235, 8718, 613, 3548, 1884, 1736, 8533, 8095, 10079, 8807, 10414, 413, 3149, 10776, 10366, 1147, 9904, 1941, 9123, 2635, 7985, 2959, 9823], 'down_proj': [3085, 3541, 396, 2319, 2323, 3720, 2688, 2050, 2650, 2595, 1014, 1042, 113, 783, 2745, 2735, 2801, 4095, 3681, 2190, 3766, 2789, 2246, 3886, 1613, 2171, 1926, 3234, 1760, 1483, 809, 2573, 3628, 3450, 491, 2682, 1205, 4066, 700, 936, 1268, 177, 1296, 1316, 2393, 2561, 1898, 3050, 2509, 2825, 2677, 1528, 420, 847, 10, 3113, 1763, 2666, 3247, 225, 2908, 2072, 1718, 3589, 2719, 1196, 2664, 2597, 1335, 2406, 3331, 3141, 3099, 2851, 2575, 3865, 3330, 1692, 2300, 1966, 1265, 377, 3493, 2014, 928, 3305, 1507, 479, 2366, 2608, 2251, 2067, 1817, 2175, 169, 2280, 2133, 3083, 3781, 4071, 3954]}, {'up_proj': [5129, 4253, 1922, 7428, 5911, 2941, 2602, 5524, 10315, 7151, 2593, 1145, 3883, 875, 3810, 9431, 5241, 3143, 7568, 7858, 6040, 4529, 10208, 3241, 4550, 7030, 1618, 5691, 7402, 2342, 8650, 9745, 2333, 5347, 1739, 2271, 7230, 1684, 5424, 4984, 6410, 5777, 3396, 3702, 912, 1046, 5255, 295, 1880, 6563, 5728, 2975, 778, 10806, 3383, 5528, 9305, 10083, 620, 8325, 8537, 9663, 258, 5401, 927, 4241, 3091, 1653, 1188, 3769, 6147, 7433, 5063, 1223, 598, 4312, 2321, 2099, 4966, 936, 2440, 10976, 6732, 1577, 825, 2347, 3229, 6734, 401, 8071, 3974, 10146, 3574, 10325, 4035, 2110, 8985, 10184, 8702, 4422, 2108, 4802, 4121, 3931, 5075, 4279, 2089, 1527, 10821, 3508, 754, 6398, 4788, 5349, 4823, 2348, 1465, 4162, 271, 4676, 9531, 7614, 9577, 6219, 8732, 10051], 'gate_proj': [747, 9378, 1863, 10817, 9683, 8042, 8078, 2702, 10351, 275, 10216, 7741, 6968, 1030, 10878, 10447, 4044, 2647, 10548, 7787], 'down_proj': [1115, 3271, 2712, 3628, 2219, 4070, 2732, 2827, 1793, 964, 1120, 3364, 2870, 779, 2792, 631, 1049, 870, 2282, 2050, 3626, 3870, 2801, 3069, 1528, 2363, 540, 3396, 4095, 3615, 4010, 141, 3794, 3409, 3667, 3083, 4050, 3879, 2133, 951, 2014, 2104, 189, 1692, 981, 21, 1825, 2957, 3550, 240, 847, 41, 3373, 3570, 399, 1110, 2548, 2879, 1830, 2225, 1887, 1237, 3625, 10, 752, 3646, 2977, 3593, 511, 1393, 1291, 1260, 1541, 1633, 2794, 2993, 1364, 2341, 902, 3765, 1737, 719, 3692, 457, 3995, 527, 2789, 535, 274, 2695, 2608, 1829, 5, 2366, 3547, 2573, 1601, 1201, 119, 2917, 2493, 1483, 1898, 1453, 3305, 3711, 1764, 3165, 3618, 2810, 4081, 4065, 1004, 2376, 3523, 1556, 3886, 700, 502, 1991, 1782, 1296, 2735, 1801, 3356, 3661, 2682, 3724, 48, 2549, 934, 3147, 3818, 3942, 2138, 2074, 3112, 2749, 2941, 3486, 373, 3230, 3980, 2298, 3051, 2385, 2171, 1384, 658, 1342, 2911, 2625, 2174, 2664, 421, 1022, 2126, 526, 2393, 1246, 2230, 8, 3704, 1466, 2345, 1747, 1208, 2902, 1981, 769, 2323, 1591, 2273, 2168, 2224, 3633, 44, 4049, 995, 1380, 3680, 1256, 287, 1724, 4054, 2473, 1261, 3092, 169, 2059, 1818, 3876, 3709, 1571, 3168, 853, 313, 2197, 3116, 2481, 1062, 3205, 211, 681, 1196, 2441, 1899, 3984, 1641, 1659, 2888, 3843, 2942, 677, 3220, 2750, 543, 1671, 3526, 1879, 3269, 2405, 3195, 3967, 360, 922, 2578, 916, 2300, 611, 2650, 635, 149, 2812, 925, 34, 3015, 61, 1372, 2788, 406, 1409, 3431, 3379, 1362, 530, 2829, 2117, 741, 2332, 113, 3072, 2336, 450, 3894, 2813, 1828, 2593, 2747, 2244, 506, 1780, 759, 3541, 1479, 1763, 1977, 2861, 668, 245, 1300, 4066, 1708, 2329, 2595, 667, 946, 1102, 439, 2931, 2878, 3531, 3525, 4074, 3019, 2898, 1613, 3960, 640, 3822, 1952, 2592, 3723, 3359, 2745, 906, 1042, 1915, 1367, 3780, 1050, 251, 1084, 3972, 1580, 1781, 2758, 2463, 1853, 1464, 3778, 2423, 126, 396, 1916, 597, 2220, 80, 3836, 2281, 3650, 2308, 3156, 3229, 3232, 3456, 2409, 2082, 285, 4035, 3214, 2469, 39, 3953, 3162, 3119, 2504, 120, 1106]}, {'up_proj': [9945, 9349, 7655, 9055, 911, 6340, 1333, 3585, 5043, 10037, 3913, 1221, 2381, 507, 9873, 7538, 4876, 6088, 330, 117, 10578, 4988, 4047, 1601, 7987, 9977, 5436, 798, 2338, 254, 2119, 726, 2208, 6212, 8601, 8805, 3032, 8500, 8575, 4098, 1758, 10002, 1663, 3650, 7009, 9618, 4121, 7493, 1612, 1527, 9065, 601, 7452, 5049, 6089, 10920, 9701, 1945, 1396, 445, 2390, 2209, 9715, 8750, 10155, 5979, 4838, 7229, 7364, 8410, 9706, 3120, 2593, 789, 2007, 5725, 7349, 4922, 5264, 1904, 2191, 4806, 8945, 1534, 4533, 9247, 5898, 7322, 7678, 720, 2766, 4423, 5467, 4206, 5222, 9144, 9080, 1109, 1418, 887, 9591, 4360, 10122, 9255, 802, 7677, 1051, 675, 4857, 8422, 2922, 4768, 4977, 9093, 5089, 8131, 850, 6747, 4200, 3016, 6758, 8490, 3539, 7178, 8210, 4810, 10996, 946, 10019, 2350, 6765, 3553, 10949, 3975, 4886, 1838, 6154, 2747, 11001, 8151, 9392, 2933, 6454, 4900, 6519, 4825, 6702, 6592, 10801, 4708, 7224, 5172, 2990, 2022, 7544, 10588, 8182, 2871, 7001, 93, 8375, 9069, 3141, 7718, 7151, 3625, 10342, 6887, 5068, 5480, 6763, 10553, 4432, 1258, 2792, 5860, 8275, 2474, 5962, 4937, 276, 9071, 8928, 7999, 4578, 288, 5139, 1706, 7016, 10743, 4733, 8267, 6880, 5433, 10880, 8970, 3895, 1208, 1019, 4938, 5051, 2185, 10596, 70, 356, 491, 5150, 7388, 4531, 10534, 364, 2446, 502, 6178, 1142, 1951, 7606, 7953, 9622, 10794, 1434, 8680, 9039, 7276, 10543, 7651, 8339, 5245, 10723, 3877, 5543, 6425, 3406, 614, 4599, 140, 710, 1445, 1779, 7323, 10370, 8548, 6042, 1902, 1262, 7091, 2951, 2973, 2008, 7661, 10455, 809, 4553, 3080, 5626, 3933, 3074, 3875, 5661, 4178, 329, 5831, 5581, 5743, 379, 4665, 7639], 'gate_proj': [1015, 3728, 7694, 8192, 8016, 3994, 10231, 5569, 956, 5259, 7742, 1146, 3552, 6758, 929, 10171, 1036, 464, 1282, 2826, 4997, 2785, 1756, 1598, 3498, 8835, 3297, 11002, 4116, 1714, 10684, 1192, 8457, 229, 6369, 1804, 3041, 8461, 9416, 5092, 9826, 3558, 4363, 6869, 4081, 5342, 8577, 9216, 5783, 907, 8627, 8138, 6115, 1005, 5324, 10968, 2120, 4601, 8848, 1755, 609, 6837], 'down_proj': [2393, 2695, 2213, 535, 2270, 2864, 2669, 2754, 228, 2822, 41, 3137, 3491, 4000, 3014, 1404, 2183, 3795, 280, 3703, 1442, 3643, 3720, 1659, 5, 3667, 3359, 2608, 3141, 2573, 3218, 3049, 2442, 3072, 964, 189, 397, 1153, 3116, 3570, 3051, 2789, 1977, 1004, 3579, 936, 3780, 1208, 809, 1801, 1367, 3778, 762, 3542, 313, 3113, 16, 641, 34, 3711, 3123, 1931, 281, 1793, 3453, 2380, 2138, 3015, 3685, 1045, 4049, 2946, 169, 2717, 3886, 2478, 1031, 3387, 1737, 2024, 3139, 2679, 142, 3876, 167, 3526, 855, 192, 1466, 2273, 1739, 1775, 3980, 2625, 3523, 3197, 2944, 2801, 406, 1893, 3525, 143, 3468, 2917, 2135, 230, 1300, 3847, 310, 3361, 10, 55, 1326, 1892, 2291, 2396, 3798, 1393, 3707, 252, 1763, 1652, 3431, 506, 850, 3540, 3364, 863, 1092, 1962, 3463, 2300, 2980, 394, 2533, 928, 1830, 652, 3765, 98, 930, 70, 3510, 1968, 530, 1874, 2633, 359, 2938, 127, 875, 368, 1372, 2578, 331, 2050, 1619, 4035, 647, 450, 898, 1483, 1377, 1606, 149, 4095, 1306, 1650, 3824, 983, 1467, 417, 2585, 3147, 44, 3573, 1990, 2807, 1718, 3071, 3821, 4085, 922, 1291, 2879, 2792, 4094, 1265, 2817, 2022, 3252, 2469, 2354, 21, 3195, 3318, 3403, 753, 1764, 2637, 3203, 2236, 3053, 240, 2014, 140, 3626, 175, 1222, 1552, 1528, 3507, 2612]}, {'up_proj': [2371, 1506, 925, 4874, 8446, 7975, 8222, 2578, 685, 5017, 9247, 2806, 5840, 9258, 4812, 1412, 9903, 2179, 7481, 1826, 1087, 5807, 9045, 8945, 4259, 9625, 6049, 4460, 2198, 7216, 10361, 516, 4249, 6623, 5356, 2977, 2828, 292, 178, 9186, 10459, 5388, 1370, 2539, 8589, 10081, 2139, 5834, 1735, 6404, 7507, 8617, 102, 5360, 9092, 10907, 6584, 2349, 8381, 3090, 3785, 3593, 835, 927, 2288, 5401, 1786, 5289, 2212, 6742, 7687, 6055, 1198, 6843, 9751, 2718, 5722, 877, 10929, 9034, 9619, 5213, 5, 10776, 4273, 4468, 3759, 2410, 7546, 10257, 9944, 64, 10798, 10512, 7807, 5690, 5934, 1134, 5463, 4113, 8113, 6424, 4180, 1283, 2099, 1358, 1291, 1731, 8367, 2911, 7723, 9985, 10950, 3071, 5495, 3342, 7185, 6641, 7821, 8344, 6218, 8902, 10569, 9428, 5466, 7526, 9638, 4702, 1958, 6963, 9773, 2294, 10575, 6702, 10744, 6681, 2648, 6550, 10144, 4931, 2222, 6313, 6046, 9824, 8078, 7263, 10258, 4232, 5936, 9080, 1613, 3486, 9005, 9689, 9504, 1435, 10118, 10426, 9805, 72, 4894, 3617, 10484, 2942, 4328, 4443, 5652, 6605, 8096, 9017, 2348, 8165, 3471, 4302, 2946, 4573, 5529, 8677, 8212, 4373, 2641, 8638, 1222, 8525, 8563, 80, 73, 2635, 3102, 8947, 4030, 2900, 8962, 5260, 9605, 8841, 716, 10217, 207, 1749, 5377, 9553, 828, 9654, 7416, 8237, 3971, 4350, 559, 4660, 4901, 8275, 9726, 5675, 10582, 2280, 5695, 1344, 10939, 2562, 895, 8215, 10418, 9319, 8399, 8787, 2639, 9210, 8707, 205, 1591, 7741, 3101, 9946, 9741, 1932, 3535, 492, 7054, 318, 8852, 10670, 228, 3145, 10601, 1545, 5312, 3985, 5366, 6690, 2982, 8017, 3481, 2891, 2947, 5012, 4312, 4162, 4116, 5239, 4423, 7266, 10554, 2421, 5029, 1279, 9048, 1721, 1951, 6772, 10362, 9503, 5405, 10090, 9690, 4725, 9647, 7861, 9860, 9809, 9170, 6678, 3635, 133, 5569, 3781, 4062, 6381, 10538, 2161, 8523, 1793, 5532, 7852, 7809, 1256, 8934, 15, 3482, 8925, 1571, 3293, 5452, 5846, 9389, 2922, 910, 8959, 6554, 1629, 2701, 6618, 997, 9981, 4182, 765, 1232, 10956, 7570, 10297, 6567, 214, 2367, 2714, 6875, 1480, 5230, 6191, 1463, 6669], 'gate_proj': [4032, 2390, 9826, 5809, 9392, 21, 4452, 5556, 9714, 6140, 1146, 7809, 10280, 1721, 2661, 1300, 10190, 6708, 4260, 10034, 6882, 8131, 9037, 6995, 6097, 2129, 6615, 8896, 7751, 7079, 8527, 8256, 674, 6169, 224, 6105, 2218, 6742, 8069, 8542, 4895, 7127, 6965, 9580, 3305, 2195, 6798, 10113, 562, 10374, 5807, 5891, 2131], 'down_proj': [1991, 1692, 568, 3334, 167, 964, 495, 2829, 2264, 2595, 2664, 3931, 4035, 3886, 2608, 2168, 3864, 274, 780, 3513, 3661, 2132, 2441, 406, 2801, 2585, 506, 960, 3130, 3824, 3691, 3571, 2725, 898, 2622, 203, 1385, 2977, 1479, 3980, 1780, 3247, 679, 189, 2280, 2281, 3011, 1766, 2219, 3628, 536, 3523, 3092, 2917, 2050, 1828, 675, 1915, 1589, 3620, 1105, 3318, 1220, 3069, 1483, 2545, 3102, 2048, 1072, 1377, 2273, 1613, 490, 119, 922, 2588, 3147, 3356, 3218, 2329, 2623, 508, 10, 207, 3525, 2795, 3939, 1659, 3649, 3220, 3544, 3019, 588, 1115, 2789, 2279, 3083, 1959, 1442, 1977, 1652, 2721, 2933, 1903, 2677, 230, 2971, 3901, 2910, 1401, 2409, 2735, 2879, 667, 2498, 3141, 2010, 149, 1174, 304, 502, 178, 3640, 687, 1269, 1759, 3765, 1364, 2548, 371, 280, 2396, 3929, 3518, 2079, 2757, 1246, 1372, 793, 2323, 2794, 108, 1706, 3232, 847, 2924, 668, 608, 3150, 3123, 2573, 3195, 348, 44, 3850, 578, 3299, 2420, 2837, 3165, 2188, 1923, 3777]}, {'up_proj': [4012, 4510, 7861, 820, 4260, 4538, 6143, 4813, 10543, 10327, 3059, 1890, 8302, 9315, 7946, 6150, 105, 2937, 2165, 5544, 4269, 8885, 8514, 312, 5479, 6458, 6132, 8527, 1430, 1994, 897, 8028, 2657, 9443, 967, 964, 8753, 9121, 4891, 6714, 2426, 3930, 9463, 9291, 8112, 2867, 3028, 2461, 1594, 79, 8032, 3189, 4147, 2934, 10409, 5089, 1759, 10649, 10993, 6689, 5688, 8611, 2067, 9966, 4197, 4304, 4586, 3578, 10200, 9594, 8933, 8676, 1653, 7973, 1829, 7703, 10185, 4765, 2438, 617, 9033, 9540, 4734, 10914, 10283, 3334, 4847, 1930, 5297, 4328, 9285, 7591, 5637, 10545, 7724, 5758, 4549, 4604, 8719, 114, 5099, 9250, 1731, 8048, 4307, 1407, 1428, 4976, 5775, 5773, 2616, 6093, 3644, 4499, 9404, 2178, 5365, 10309, 4655, 2085, 2259, 2826, 9161, 6517, 4327, 9648, 10353, 2027, 172, 7219, 10187, 1779, 5418, 1607, 9691, 8035, 576, 7928, 9667, 3185, 4486, 5636, 3134, 4532, 4735, 459, 7538, 5535, 10379, 7347, 259, 8318, 4590, 6672, 2623, 8443, 1576, 5845, 5706, 9455, 6697, 2840, 5704, 35, 3555, 4964, 7941, 2224, 2335, 1101, 10995, 7494, 10074, 3260, 3233, 3971, 9698, 1128, 8036, 2019, 7866, 2875, 7229, 3517, 3135, 8220, 4640, 6348, 9752, 1677, 6648, 9238, 427, 1735, 3220, 4487, 10491, 7519, 2644, 8828, 2478, 7788, 8655, 5591, 3259, 10751, 7308, 10979, 6120, 1356, 729, 3311, 3361], 'gate_proj': [1288, 9048, 8903, 8832, 7031, 2249, 937, 4914, 10583, 6641, 7358, 7357, 4124, 8595, 3999, 10484, 3281, 1247, 8946, 4116, 4808, 4211, 7187, 9168, 6012, 9672, 9887, 2428, 5775, 3248, 2847, 823, 5735, 573, 5603, 5882, 8998, 9896, 8521, 9252, 1001, 1354, 2347, 8996, 8771, 10764, 10173, 8017, 3095, 10659, 10675, 9502, 3296, 4337, 5184, 9579, 6840, 6139, 1723, 661, 3691, 6660, 3945], 'down_proj': [675, 2912, 3926, 1155, 117, 809, 203, 4071, 1898, 3526, 2393, 3931, 3102, 149, 878, 3166, 94, 2188, 2725, 1613, 3794, 2674, 2789, 3580, 3832, 3071, 1718, 1034, 1553, 1453, 3149]}, {'up_proj': [6874, 10139, 8049, 4323, 10439, 1517, 9482, 4986, 10392, 10339, 707, 106, 6512, 2025, 8058, 322, 2542, 10844, 6296, 10044, 5483, 2498, 2099, 3844, 5915, 3952, 9171, 6453, 5836, 8943, 1092, 1834, 5925, 986, 10302, 531, 2212, 1688, 8029, 152, 8056, 10874, 3736, 5235, 8070, 5710, 5392, 4401, 2963], 'gate_proj': [7661, 6874, 4640, 10392, 2390, 8117, 7089, 8191, 9702, 9664, 1096, 230, 6913, 7835, 3591, 70, 5004, 2847, 10952, 3134, 4671, 4137, 3649, 132, 7033, 3026, 1278], 'down_proj': [3136, 397, 915, 2573, 2393, 964, 1243, 2092, 3289, 183, 2298, 783, 2789, 345, 3872, 4071, 2883, 3640, 675, 2938, 3220, 2679, 94, 2984, 2637, 2168, 3391, 1363, 1401, 2757, 2251, 3994, 2089, 3832, 4094, 225, 1103, 1453, 1489, 3486, 2016, 1306, 3491, 155, 3871, 3643, 1011, 2898, 2721, 4006, 719, 3967, 1619, 4066, 2430, 3726, 3397, 2442, 3431, 2555, 2837, 2550, 3299, 4050, 348, 531, 577, 1528, 1522, 3625, 3939, 3772, 2608, 1061, 3669, 1314, 707, 37, 85, 1223, 139, 3579, 3852, 3621, 237, 3526, 689, 1794, 4085, 4095, 1591, 1287, 1612, 3755, 1926, 2281, 1780, 2595, 3541, 2683, 120, 1114, 4031, 2340, 2210, 1974, 3529, 3532, 3178, 1995, 3149, 1613, 3953, 1966, 1939, 651, 1753, 1393, 2807, 129, 972, 1294, 2912, 1446, 2997, 1999, 503, 84, 2527, 1014, 478, 1915, 506, 3383, 406, 171, 1959, 1960, 3525, 1333, 3611, 1167, 1871, 1553, 112, 2423, 875, 4068, 3668, 2050, 2715, 3906, 1026, 3050, 55, 3408, 41, 3318, 3356, 1898, 3955, 2160, 665, 1825, 355, 1640, 982, 334, 280, 3849, 205, 2375, 2143, 2349, 3900, 1102, 3000, 3190, 2777, 3024, 3195, 797, 2540, 490, 2724, 3071, 2891, 2772, 95, 2647, 1110, 1823, 3102, 523]}, {'up_proj': [2624, 6491, 7155, 7219, 6399, 10257, 9429, 4170, 1209, 7821, 5582, 4983, 3716, 7947, 4588, 6694, 6933, 575, 7384, 10122, 1171, 9976, 9442, 838, 6463, 3843, 7011], 'gate_proj': [3062, 1147, 7811, 7006, 8713, 9931, 5437, 5890, 7821, 1330, 8109, 2151, 1415], 'down_proj': [2050, 3625, 94, 2393, 2251, 2287, 2573, 4006, 2679, 4066, 1612, 4071, 2883, 2807, 3071, 2789, 4041, 225, 3123, 3391, 1780, 2847, 1230, 3491, 1935, 1627, 1367, 3994, 3781, 2938, 167, 3178, 875, 2120, 1638, 280, 3871, 1110, 183, 1429, 3623, 1065, 1011, 4035, 435, 2294, 3526, 97, 2851, 394, 636, 3865, 2719, 2168, 1753, 2037, 2273, 2772, 2014, 1960, 3097, 4003, 2350, 3852, 10, 2931, 1370, 3832, 1761, 3566, 588, 3703, 3739, 4085, 149, 2021, 37, 1915, 1223, 1401, 2188, 2640, 2739, 2678, 1072, 1363, 2725, 250, 300, 7, 3254, 1684, 1269, 3083, 3644, 3529, 2365, 3768, 1114, 1893, 4095, 2986, 3906, 3918, 2210, 251, 1680, 1939, 316, 1743, 1103, 3553, 938, 801, 243, 1859, 3397, 1469]}, {'up_proj': [9585, 322, 10389, 6528, 1588, 55, 7959, 9561, 782, 1684, 7817], 'gate_proj': [1144, 6099, 6903, 1096, 7976, 1948, 7906, 9112, 2148, 1819, 443, 10292, 10333, 8641, 6832], 'down_proj': [637, 1739, 1780, 3726, 3625, 3852, 2637, 1446, 2883, 4071, 4003, 3234, 2938, 2573, 3703, 3087, 334, 3391, 2016, 2294, 1269, 3603, 3289, 783, 225, 171, 153, 1681, 2393, 577, 1103, 3731, 588, 2366, 2807, 2168, 4050, 1612, 1011, 2498, 1742, 3174, 4006, 3994, 2285, 1935, 3519, 2210, 1418, 2949, 1072, 1363, 762, 2323, 2540, 446, 4035, 1401, 1707, 2772, 2527, 3918, 397, 37, 3624, 3732, 531, 94, 2251, 1223, 3765, 3526, 1110, 4090, 1461, 2931, 1186, 2319, 745, 2298, 3832, 3376, 2789, 3955, 250, 237]}, {'up_proj': [4203, 7989, 10988, 0, 4935, 1700, 9552, 6140, 2909, 9686, 9089, 9252, 1185, 4854, 6039], 'gate_proj': [8681, 99, 10162, 4649, 1821, 9772, 2131, 188, 8808, 4187, 5990, 4239, 2949, 9668, 5385, 3818, 2438, 6276, 4244], 'down_proj': [1739, 37, 3994, 3426, 1974, 2812, 435, 1461, 972, 1307, 2807, 3872, 225, 3826, 3190, 1469, 1706, 3491, 4035, 4006, 1363, 2350, 84, 2789, 1103, 2984, 1960, 2298, 3832, 3287, 4071, 938, 1130, 2938, 3918, 94, 2351, 1707, 2278, 397, 2883, 631, 1283, 2026]}, {'up_proj': [4752, 9851, 2046, 5619, 7820, 5679, 8771, 1915, 10600, 9294, 2579, 4715, 9557, 4690], 'gate_proj': [9851, 1895, 9954, 8393, 7538, 9824, 1344, 8638, 9680, 3283, 4524], 'down_proj': [2015, 875, 1782, 94, 1753, 435, 1327, 41, 1739, 2679, 1856, 2237]}, {'up_proj': [3015, 8538, 6765, 4453, 3517, 7197, 3894, 5649], 'gate_proj': [3900, 9435, 7197, 4677, 3015], 'down_proj': [94, 3918, 2684, 205, 3726, 2274, 189, 1223, 250, 310, 762, 1306, 1949, 3852, 2210, 1429, 3826, 783, 3289, 4071, 1891, 2807, 2789, 2863, 2116, 1856, 2351, 345, 1469, 1461, 334, 1283, 225, 324, 2883, 2622, 2725, 4074, 2637, 1523, 2933, 851, 3491, 1727, 187, 2812, 2168, 2968, 1411, 3020, 3215, 1574, 3603, 3188, 2393, 192, 397, 3102, 1553, 3382, 791, 2573, 432, 1114, 2143, 2342, 2016, 1352, 624, 1418, 139, 2618, 930, 3234, 1384, 3287, 3121, 1340, 669, 972, 2350, 3625, 2294, 3865, 588]}, {'up_proj': [5365, 3057], 'gate_proj': [1289, 10826, 1648, 6821, 8193, 6729, 4774, 5179], 'down_proj': [631, 1401, 717, 2725, 2016, 3421, 2938, 783, 363, 4071, 3524, 875, 2637, 1861, 1755, 3355, 2168, 1706, 1340, 1446, 1739, 3726, 1223, 2684, 3431, 1269, 4027, 1500, 2393, 2294, 168, 904, 972, 2351, 2210, 1961, 4090, 762, 2033, 2221, 1704, 3009, 3852, 1856, 1147, 2274, 1753, 846, 2622, 2350, 1910, 3434, 1453, 531, 2863, 1384, 1523, 2789, 4006, 1921, 3946, 1935, 2883, 604, 1932, 3779, 1931, 3102, 3918, 1272, 397, 736, 3234, 3872, 1372, 3369, 862, 3463, 527, 2366, 1339, 3578, 1327, 310, 3289, 1985, 745, 3135, 2714, 2237, 165, 3731, 274, 725, 252, 3986, 1825, 3816, 3287, 3190, 3948, 250, 1431, 3143, 1393, 174, 487, 3606, 2540, 2573, 2597, 2296, 851, 3803, 4068, 2363, 1697, 1891, 3739, 2680, 219, 2558, 908, 2244, 1894]}, {'up_proj': [9634, 10294, 7985, 10838, 10559], 'gate_proj': [5835, 5507], 'down_proj': [2210, 3779, 4006, 2350, 1453, 77, 875, 2393, 3122, 1314, 2684, 310, 862, 962, 1921, 3994, 531, 237, 3284, 1456, 1461, 783, 3553, 725, 3670, 446, 2384, 2679, 2863, 3603, 1014, 689, 3987, 982, 3215, 2875, 2984, 762, 192, 1780, 3872, 1961, 3732, 1007, 2546, 4027, 2152, 225, 631, 864, 1186, 250, 1706, 1676, 2244, 3583, 3, 2669, 219, 1895, 2300, 3529, 1429, 507, 2342, 3586, 898, 3918, 290, 3391, 1469, 2459, 3758, 2016, 1725, 363, 3239, 2573, 1015, 1915, 1223, 1605, 3355, 2789, 1704, 3519, 2168, 4066, 2807, 2637, 1500, 3254, 4003, 2092, 2883, 636, 55, 396, 139, 3463, 632, 3190, 3154, 2363, 187, 904, 1571]}, {'up_proj': [3534, 4005, 10613, 9414, 4678, 7614, 9880], 'gate_proj': [9414, 327, 9968], 'down_proj': [2152, 861, 1340, 3826, 94, 1861, 2016, 2001, 1052, 1327, 2292, 875, 3287, 3102, 2684, 363, 3491, 631, 762, 1469, 1753, 3803, 3254, 2680, 2210, 2538, 703, 2430, 862, 290, 3215, 968, 2725, 3073, 3431, 2679, 1761, 225, 736, 1306, 3471, 2337, 3122, 1569, 1734, 1550, 3188, 3948, 1836, 1895, 190, 1230, 2831, 3421, 1461, 2298, 4003, 3836, 3869, 3779, 2274, 3525, 1704, 142, 686, 310, 1891, 2294, 1931, 3568, 1743, 3986, 778, 2789, 189, 2350, 1856, 3166, 3369, 551, 1035, 2033, 846, 4069, 55, 2719, 3234, 2608, 3553]}, {'up_proj': [5291, 10045, 7734, 10041, 8544, 3345, 4187, 6413, 3328, 8009, 5592, 8046, 1019, 7339, 2905], 'gate_proj': [5834, 6568, 38, 10071, 1193], 'down_proj': [1697, 1306, 2350, 187, 2684, 3670, 4003, 2023, 2179, 1314, 2679, 3215, 2938, 2637, 334, 2016, 972, 2351, 2168, 2294, 2335, 3529, 1429, 3731, 252, 1615, 2274, 631, 2605, 1523, 3918, 3553, 1679, 3391, 875, 37, 2617, 689, 1052, 2754, 3285, 3852, 809, 3606, 3872, 2430, 709, 2807, 2152, 3876, 3833, 3955, 2975, 1651, 2244, 3135, 1850, 531, 962, 3708, 1147, 688, 3603, 442, 2342, 3987, 3106, 2540, 864, 2020, 3028, 1213, 778, 3779, 3463, 219, 363, 588, 762, 1961, 881, 546, 1556, 347, 2597, 3353, 1489, 929, 1836, 1351, 2836, 3554, 1442, 1782, 604, 2251, 3869, 1741, 2558, 3442, 1968, 2883, 2378, 2676, 2873, 276, 332, 168, 1743, 355, 3175, 1785, 3883, 1536, 3377, 2384, 927, 1753, 2210, 1870, 1476, 834, 3122, 2648, 717, 623, 3524, 2115, 3766, 1119, 993, 1149, 2345, 1872, 2622, 1512, 1258]}, {'up_proj': [9264, 10404, 8871, 10207, 8247, 5384, 5288, 3910, 7897, 7056, 155, 8544, 4290, 9231, 928, 6794, 6370, 9546, 8039, 9417, 6922, 10765, 1666, 10978, 2322, 868, 6078, 6753], 'gate_proj': [155, 5062, 7250, 10672, 8490, 9437, 4668, 2769, 5047, 2796, 10051, 9971, 5395], 'down_proj': [3779, 3188, 2065, 3438, 2351, 1147, 3493, 1352, 623, 2807, 762, 37, 2684, 3287, 862, 3826, 1306, 2954, 4003, 3125, 3868, 3803, 1968, 2350, 2558, 3990, 1432, 1780, 1500, 3491, 2931, 4074, 1554, 380, 2356, 3257, 1978, 3389, 1539, 250, 142, 997, 2516, 2496, 1893, 2052, 2679, 3870, 3419, 355, 252, 2281, 2540, 458, 2594, 2676, 3799, 2457, 972, 3622, 3110, 709, 2366, 189, 2938, 1092, 904, 3918, 3354, 225, 891, 3854, 2622, 3197, 3057, 347, 168, 280, 2863, 2754, 2133, 2924, 3447, 3254, 2363, 725, 2271, 3, 1741, 2337, 2836, 3143, 3471, 52, 1512]}, {'up_proj': [7755, 7501, 5162, 7359, 674, 10476, 5363, 10536, 3043, 7740, 7268, 5253, 8477, 10770, 5022, 6797, 3767, 4831, 6720, 7460, 6369, 8433, 6806, 2534, 684, 7772, 5442], 'gate_proj': [6110, 8152, 874, 3611, 2623, 10770, 8782, 7604, 1440, 3781, 3375], 'down_proj': [1828, 875, 727, 3287, 2991, 1052, 380, 1147, 355, 3674, 1622, 703, 1500, 2836, 673, 3529, 3190, 2949, 2617, 531, 3391, 1314, 1092, 2016, 1743, 2883, 2363, 717, 2546, 3302, 631, 1411, 2152, 1174, 3188, 1785, 548, 3868, 3506, 1569, 3766, 149, 2825, 2545, 1627, 2298, 2754, 2165, 456, 2622, 507, 3872, 1489, 3987, 2938, 2281, 783, 777, 3351, 3315, 3852, 2954, 365, 1351, 1442, 3955, 3468, 3284, 1386, 396, 863, 3524, 3382, 1902, 3803, 4003, 1182, 3779, 1832, 397, 2684, 1385, 219, 2227, 4068, 3224, 2680, 1895, 2213, 3295, 3758, 859, 972, 2801, 2640, 2168, 778, 3254, 2324, 339, 2052, 1893, 1512, 3015, 3599, 1056, 1825, 725, 1780, 689, 3184, 53, 3406, 192, 3918, 2350, 2538, 2221, 3057, 121, 3054, 2523, 551, 446, 3135, 1863, 3583, 3733, 1523, 3076, 1697, 2124, 3215, 4053, 2186, 2660, 2294, 2020, 2265, 2116, 1283, 1335, 3337, 3526, 1539, 2491, 252, 1016, 2337, 3435, 3258, 2679, 2719, 2468, 3411, 1755, 2863, 3519, 225, 3203, 2676, 3603, 1960, 2924, 1961, 1301, 3540, 3708, 2067, 2065, 764, 3726, 1306, 4069, 442, 10, 2595, 951, 1023, 3670, 3997, 3106, 3990, 2244, 1735, 563, 3731, 2128, 3247, 2033, 1774, 3689, 2170, 2573, 3003, 864, 637, 3799, 2707, 1911, 125, 595, 3125, 1727, 2256, 1102, 2045, 2550, 1968, 2431, 189, 2917, 2296, 1875, 1991, 3018, 1401, 1916]}, {'up_proj': [8445, 8030, 4001, 10421, 8640, 7873, 1271, 8094, 6530, 229, 10002, 4147, 447, 876, 721, 5788, 3480, 4200, 2393, 7792, 1267, 3981, 1869, 7988, 656, 3467, 3398, 2309], 'gate_proj': [4001, 2408, 6464, 5843, 7329, 4834, 1428, 446, 2855, 7925, 5146, 3762, 10347, 2675, 4239, 2167, 1271, 1936], 'down_proj': [2023, 3833, 875, 1489, 2350, 225, 2294, 3015, 1706, 3918, 1496, 2065, 1314, 3234, 3391, 1401, 2342, 3018, 456, 276, 3731, 2676, 2782, 2089, 1003, 3493, 2684, 1500, 2340, 3670, 3091, 727, 3438, 3526, 189, 3524, 3187, 1600, 250, 446, 2680, 176, 167, 442, 3583, 3955, 531, 1539, 2244, 636, 778, 3295, 1095, 1147, 1047, 1978, 3188, 3892, 2719, 3779, 3190, 2540, 2366, 1476, 2351, 3758, 1052, 1875, 2354, 3435, 3577, 3589, 2789, 1527, 1722, 777, 631, 1755, 2066, 2271, 2133, 2807, 1774, 1573, 4074, 4027, 1403, 3211, 2160, 3047, 112, 637, 1512, 3287, 1523, 862, 380, 1735, 1780, 1836, 4003, 3197, 3249, 3289, 3224, 3843, 2873, 1554, 1391, 423, 2938, 1023, 2605, 2575, 972, 2637, 1076, 2640, 1037, 1411, 2165, 725, 783]}, {'up_proj': [2432, 739, 7006, 7865, 1925, 3340, 2534, 10418, 3102, 5521, 6676, 6703, 10515, 5417, 435, 7086, 6549, 2521, 6562, 10282, 4168, 885, 6554, 5139, 3907, 1564, 9028, 10973, 2995, 2699, 10581], 'gate_proj': [2493, 229, 7747, 6953, 2991, 10966, 1800, 10664, 6308, 5516, 214, 5742, 3295], 'down_proj': [1052, 3918, 1401, 1539, 3295, 2298, 168, 1351, 2622, 2558, 2244, 703, 507, 709, 2459, 2281, 4003, 3290, 2676, 2715, 972, 2573, 2494, 1512, 2836, 3868, 1314, 2408, 2384, 3393, 2844, 1393, 3135, 923, 1301, 3529, 176, 225, 2213, 3249, 1780, 3586, 442, 3191, 1832, 1870, 2917, 2840, 3803, 2358, 3389, 3497, 2152, 548, 473, 1023, 1554, 1100, 339, 1500, 2186, 3852, 875, 597, 3188, 3582, 2858, 2324, 1326, 2141, 2607, 3589, 631, 3997, 94, 3215, 3391, 2237, 1992, 3018, 3431, 3289, 3184, 2393, 259, 1145, 1403, 235, 2052, 3065, 2143, 2863, 2124, 4027, 2274, 1302, 3606, 2065, 3432, 2083, 2271, 3872, 1543, 4053, 689, 695, 1523, 2499, 237, 3493, 866, 2181, 2726, 725, 2719, 3739, 265, 1411, 863, 2366, 918, 2782, 595, 2089, 531, 2403, 3987, 1410, 2975, 2345, 997, 1446, 3296]}, {'up_proj': [6674, 9070, 9823, 1346, 9324, 3986, 6530, 8595, 2466, 6636, 7510, 7832, 7737, 750, 4339, 4382, 5931, 2340, 6325, 7131, 6173, 2657, 10768, 8904, 9867, 1829, 4369, 1783, 10883, 6283, 298, 6552, 1433, 1644, 9341, 993, 10986, 4599, 7838, 3566, 3575, 10371, 3813, 1883, 10706, 6165, 4659, 7387, 7237, 9457, 4048, 9730, 10325, 9583, 9656, 16, 7445, 6506, 7219, 6496, 10568, 3174, 3135, 859, 3749, 3182, 3044, 9194, 8531, 7547, 8087], 'gate_proj': [1147, 1140, 7466, 9302, 584, 2525, 5931, 10614, 3676, 6173, 6520, 10296, 205, 4163, 2677, 6921, 4821, 5714, 6593, 174, 10733, 5299, 8462, 8822, 5750, 1392, 2737], 'down_proj': []}, {'up_proj': [10054, 8678, 8066, 1563, 10701, 112, 7423, 9617, 2264, 1698, 832, 1373, 7724, 3597, 6232, 2164, 8102, 827, 9663, 664, 3416, 8497, 913, 10392, 5208, 9470, 8301, 6515, 1456, 6802, 10714, 9558, 3152, 7722, 5204, 7677, 5818, 10762, 2886, 2175, 6977, 2648, 8981, 9034, 7915, 3107, 3228, 3974, 1823, 6545, 2284, 4677, 7841, 8036, 3516, 9258, 5769, 414, 5410, 3644, 829, 5929, 8208, 3806, 6404, 1761, 10148, 1858, 1836, 7601, 4976, 7834, 3312, 2117, 6055, 8063, 3151, 9934, 2868, 1130, 5438, 8733, 5033, 4389, 897, 2440, 3875, 6962, 10708, 4007, 2271, 1428, 10586, 159, 3371, 10387, 7462, 7001, 6632, 6036, 3804, 1699, 2783, 5202, 9187, 3412, 9204, 4528], 'gate_proj': [5618, 2574, 10, 8619, 2581, 1921, 7995, 3093, 2052, 4186, 771, 148, 9661, 6226, 4443, 3309, 4528, 4915, 482, 5318, 2080, 2818, 1743, 10254, 2674, 2085, 9305, 2284, 3717, 6944, 3799, 3625, 9843, 10917, 9812, 3047, 8964, 6114, 5777, 4273, 5425, 692, 3209, 1554, 1858, 6671, 9559], 'down_proj': [1082, 2524, 601, 2292, 2581, 1661, 3911, 1751, 1003, 3778, 3563, 2715, 2496, 1351, 1991, 427, 3497, 3729, 2249, 2157, 311, 1859, 1960, 3368, 3570, 574, 1542, 768, 2800, 184, 1633, 1762, 2102, 2113, 2337, 2441, 3679, 4075, 4062, 1532, 2814, 629, 2638, 3275, 128, 3360, 1148, 938, 1473, 280, 3773, 1055, 1792, 2184, 2004, 73, 2661, 3622, 3231, 2310, 936, 3034, 1966, 4065, 396, 1933, 1000, 3301, 1535, 80, 3629, 3122, 800, 1298, 713, 583, 3896, 2646, 1551, 2472, 3668, 689, 2858, 3691, 2610, 3672, 953, 2443, 1400, 434, 3090, 3445, 1396, 1052, 3385, 1283, 3374, 3669, 2501, 25, 1902, 341, 3250, 293, 2588, 2710, 1066, 3156, 2100, 299, 2019, 792, 3465, 3169, 3730, 2756, 1247, 3405, 3136, 3305, 995, 2405, 983, 2846, 916, 664, 2195, 3187, 1927, 2323, 3990, 3065, 1319, 442, 2630, 3470, 654, 3115, 1228, 646, 1576, 3603, 4050, 3321, 2316, 516, 2474, 2586, 1329, 2380, 2439, 2084, 2136, 2549, 3125, 1453, 1372, 2865, 834, 4025, 1754, 3586, 1028, 3217, 172, 2617, 763, 2345, 3794, 2233, 3873, 74, 163, 133, 2072, 3027, 3854, 2065, 1142, 3142, 673, 2209, 2521, 3311, 1970, 3135, 765, 2194, 2433, 561, 1305, 2894, 1188, 2699, 267, 228, 2327, 2801, 2569, 1340, 645, 2041, 3961, 2677, 3635, 3793, 1417, 3825, 704, 723, 3832, 56, 3376, 1665, 2838, 2651, 320, 3977, 3079, 1311, 744, 2979, 618, 3026, 367, 1410, 2351, 915, 838, 568, 1326, 3018, 901, 122, 3295, 2747, 3751, 69, 3725, 1577, 4092, 1498, 1768, 2950, 3266, 3049, 3143, 3507, 1992, 729, 303, 857, 2965, 1286, 728, 1248, 3035, 942, 297, 3867, 3069, 3084, 2839, 1945, 2303, 3033, 602, 3917, 1094, 2525, 3619, 2639, 467, 2269, 1483, 3334, 1467, 884, 615, 2670, 2216, 2319, 1565, 12, 2356, 84, 3389, 780, 1500, 2504, 3123, 2281, 1341, 47, 4017, 587, 514, 3203, 1111, 98, 3414, 1818, 947, 3856, 1474, 1442, 2603, 3204, 70, 3684, 149, 2759, 1169, 2672, 2674, 1522, 3560, 2551, 816, 2267, 3606, 2396, 393, 1490, 1659, 2786, 1042, 1393, 126, 1296, 4006, 1990, 1412, 1603, 3067, 1823, 1201, 3842, 1699, 417, 2707, 260, 2463, 3642, 2466, 3427, 1773, 377, 1681, 924, 4058, 2128, 2564, 2608, 1090, 489, 2787, 3620, 3999, 3829, 1554, 2261, 2003, 239, 1059, 2601, 1622, 82, 1546, 353, 1845, 3883, 2775, 3078, 2933, 1512, 3391, 3179, 259, 1258, 2886, 4008, 1735, 1632, 2659, 1985, 4012, 1812, 908, 301, 83, 2567, 890, 4078, 2602, 2495, 2904, 3863, 486, 1083, 3621, 2237, 2134, 3401, 2803, 3533, 1032, 409, 1106, 716, 1335, 1370, 499, 1594, 2482, 2811, 3659, 717, 845, 1737, 1672, 2115, 643, 277, 507, 8, 3016, 3440, 1391, 317, 1693, 3993, 67, 1969, 1178, 1684, 2159, 14, 2291, 3517, 2306, 3502, 1013, 1889, 2025, 657, 2375, 1782, 2468, 33, 2848, 3114, 3783, 2342, 3687, 2785, 1631, 336, 828, 2515, 3173, 1921, 4033, 3705, 984, 1588, 4070, 2949, 3700, 3798, 458, 1323, 87, 1368, 3647, 2823, 1076, 3889, 2652, 795, 1526, 3415, 3170, 1739, 2514, 3815, 2826, 3816, 3868, 2301, 257, 2989, 555, 2030, 480, 849, 600, 2137, 3289, 1774, 1688, 1313, 3962, 782, 2045, 881, 3528, 3303, 2190, 215, 1926, 3099, 3818, 2447, 755, 3152, 2018, 2011, 1184, 774, 3443, 2197, 1489, 1654, 4028, 4026, 4053, 2040, 1257, 3965, 113, 886, 462, 2654, 3610, 3950, 963, 3970, 2326, 619, 1890, 2266, 1439, 1077, 2427, 2305, 131, 2395, 3579, 3480, 636, 3791, 1725, 1345, 431, 1347, 3703, 955, 1620, 1813, 2505, 3702, 1403, 1787, 3189, 1463, 2213, 3477, 540, 3613, 1289, 2766, 77, 3789, 930, 2689, 2061, 1981, 3745, 3121, 864]}]\n"
     ]
    }
   ],
   "source": [
    "print(layer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2db1fd89-1fef-4c66-8c91-638ea1d09348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8355"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neuron_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93683ce9-c877-4a61-9c00-b752f310881c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model.layers.0.mlp.up_proj.weight_index_7391', -0.30401533791557656),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_8880', -0.29782008829647033),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_6352', -0.26962193885607144),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_9710', -0.23214666906107517),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_3661', -0.12755914166374138),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_2425', -0.11200394791978541),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_9710', -0.10520611510249811),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_9651', -0.09108095855852305),\n",
       " ('model.layers.14.mlp.gate_proj.weight_index_4032', -0.08547839202870167),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_2982', -0.08328656704131499),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_4505', -0.08160818501010603),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_2371', -0.07463792617198806),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_119', -0.07071792819615519),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_1542', -0.06558213877290697),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_8462', -0.04247547294752696),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_1457', -0.039988208734492936),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_4012', -0.03823500991082218),\n",
       " ('model.layers.15.mlp.gate_proj.weight_index_1288', -0.03762118886915289),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_1506', -0.037439211868194544),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_9258', -0.03569756284586223),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_4510', -0.022665205860496584),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_2982', -0.020454285072687206),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_9818', -0.00147281635898322),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_4395', 0.010504237641432379),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_3136', 0.015453018344953495),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_10642', 0.016321706900256583),\n",
       " ('model.layers.15.mlp.gate_proj.weight_index_9048', 0.02315007071332209),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_10054', 0.03059861235904382),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1530', 0.037398999125678234),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_9686', 0.04372338239996276),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_1300', 0.04787190950815967),\n",
       " ('model.layers.2.mlp.gate_proj.weight_index_1856', 0.05421234886142923),\n",
       " ('model.layers.19.mlp.gate_proj.weight_index_8681', 0.062224416542893124),\n",
       " ('model.layers.2.mlp.up_proj.weight_index_10974', 0.06406783265838278),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_408', 0.06905887348301576),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_738', 0.06918905362097583),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_5794', 0.07593220203196838),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_2198', 0.0765064805655018),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_3832', 0.07761166087311233),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_10965', 0.08231712448442119),\n",
       " ('model.layers.1.mlp.up_proj.weight_index_4893', 0.0837165110915361),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_925', 0.084816143963935),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_662', 0.08510201612784796),\n",
       " ('model.layers.7.mlp.gate_proj.weight_index_3832', 0.09765664945253372),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_9945', 0.09892080648778956),\n",
       " ('model.layers.2.mlp.up_proj.weight_index_1058', 0.10188742989018396),\n",
       " ('model.layers.14.mlp.gate_proj.weight_index_2390', 0.10617300666517204),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_4874', 0.10714196059895542),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_4445', 0.10801041477908191),\n",
       " ('model.layers.11.mlp.down_proj.weight_index_3085', 0.1085640307589597),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_6874', 0.11000334697116143),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_997', 0.11144199498987994),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_8446', 0.11405889395015523),\n",
       " ('model.layers.15.mlp.gate_proj.weight_index_8903', 0.11422809622257946),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_680', 0.11628034672033616),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_5133', 0.1171976319893524),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_397', 0.11807268268502957),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_8962', 0.12034876299323694),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_6867', 0.12448066981398487),\n",
       " ('model.layers.19.mlp.gate_proj.weight_index_99', 0.12474385939313715),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_1828', 0.12495360824635382),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_7975', 0.12595152779307428),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_9027', 0.12659161488110238),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_8362', 0.12784535281216414),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_4746', 0.1279470249755157),\n",
       " ('model.layers.0.mlp.down_proj.weight_index_3648', 0.12815441211328826),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_8222', 0.1282191505325816),\n",
       " ('model.layers.13.mlp.gate_proj.weight_index_1015', 0.12882727132709748),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_2578', 0.13268070490930617),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_2427', 0.13298187888120117),\n",
       " ('model.layers.2.mlp.up_proj.weight_index_2623', 0.13371070516674743),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_4912', 0.1366794205649522),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_965', 0.1371154072446492),\n",
       " ('model.layers.14.mlp.down_proj.weight_index_1991', 0.13966736847199135),\n",
       " ('model.layers.5.mlp.down_proj.weight_index_2023', 0.14100839740195603),\n",
       " ('model.layers.0.mlp.down_proj.weight_index_264', 0.14181782106286267),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_9349', 0.14246802244963241),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_685', 0.14569639711143445),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_337', 0.1458794334804221),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_10035', 0.14598342662299402),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_7861', 0.14646357842732272),\n",
       " ('model.layers.2.mlp.gate_proj.weight_index_5259', 0.1469670545614261),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_5017', 0.14829056853290368),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_9247', 0.14870605625592948),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_5325', 0.14988460323242503),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4478', 0.15002810214685391),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_1542', 0.15038307898275072),\n",
       " ('model.layers.1.mlp.up_proj.weight_index_257', 0.15385474184948178),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_8678', 0.15433873354891192),\n",
       " ('model.layers.5.mlp.down_proj.weight_index_3230', 0.15445995965488235),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_2806', 0.15617894019494116),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_3230', 0.15692109899044926),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_9016', 0.15731779119680178),\n",
       " ('model.layers.2.mlp.up_proj.weight_index_5742', 0.15833457116206695),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_1261', 0.16244257920692595),\n",
       " ('model.layers.19.mlp.up_proj.weight_index_4203', 0.16329150601379716),\n",
       " ('model.layers.8.mlp.gate_proj.weight_index_1622', 0.1634644615347094),\n",
       " ('model.layers.2.mlp.up_proj.weight_index_10595', 0.16540125956109097),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_1082', 0.16617749411813865),\n",
       " ('model.layers.5.mlp.gate_proj.weight_index_9273', 0.16648622928154788),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_820', 0.16711576782173232),\n",
       " ('model.layers.28.mlp.up_proj.weight_index_8445', 0.17007919507250824),\n",
       " ('model.layers.27.mlp.up_proj.weight_index_7755', 0.17465380205900205),\n",
       " ('model.layers.31.mlp.gate_proj.weight_index_5618', 0.17617986639876637),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_498', 0.1767513864504462),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_8066', 0.17685311625252886),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_6288', 0.17732689819806202),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_7655', 0.1782208452981311),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_5840', 0.17825365015394734),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_6892', 0.17942812430195731),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3436', 0.17989519438681478),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_1153', 0.18032234705114236),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_1563', 0.18200093130180317),\n",
       " ('model.layers.28.mlp.down_proj.weight_index_2023', 0.18365229869135735),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_915', 0.1848783799051108),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_1633', 0.18704600120285564),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_5895', 0.1873834957934215),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_1115', 0.18863721527409094),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_9258', 0.1911977966666667),\n",
       " ('model.layers.1.mlp.down_proj.weight_index_945', 0.19171396509302951),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_6622', 0.19272076102964508),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_4812', 0.19312399215306053),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_1512', 0.19406412822504127),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_2168', 0.19457982288206432),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_618', 0.1949109966203726),\n",
       " ('model.layers.3.mlp.gate_proj.weight_index_8487', 0.19678656371062386),\n",
       " ('model.layers.28.mlp.up_proj.weight_index_8030', 0.19681628355735636),\n",
       " ('model.layers.31.mlp.gate_proj.weight_index_2574', 0.19708708760495464),\n",
       " ('model.layers.28.mlp.up_proj.weight_index_4001', 0.1994718861607856),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_4260', 0.19978590000520402),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_4538', 0.1998731188184646),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6377', 0.20037776691326137),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_2524', 0.20068953731273087),\n",
       " ('model.layers.28.mlp.down_proj.weight_index_3833', 0.20122879944151117),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_6143', 0.2023606364907784),\n",
       " ('model.layers.15.mlp.gate_proj.weight_index_8832', 0.2034573624009237),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_10686', 0.20399288859900677),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_1412', 0.20423062484564403),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_1527', 0.20426369072048445),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_7377', 0.20431621033875302),\n",
       " ('model.layers.2.mlp.gate_proj.weight_index_10188', 0.2054198726852583),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_9283', 0.2058456177890582),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_4813', 0.20708170785507862),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_192', 0.20731385711783945),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_601', 0.20778051270065134),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_8120', 0.2085464879810166),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_5964', 0.2091040031447835),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_10701', 0.20999940896534808),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_2870', 0.21212896583198937),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_112', 0.2121731206384303),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_7795', 0.21387410640175553),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_3083', 0.21410333047207608),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_9903', 0.2141301015512358),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_2573', 0.21461226006004308),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_6227', 0.21512264634935407),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_10020', 0.21545935714837183),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_2292', 0.21615049209706605),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_3271', 0.2188746732815674),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_2581', 0.21905364764980995),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_6674', 0.22034626077212893),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_2179', 0.22345340542279857),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_9755', 0.22407704052785116),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_6345', 0.22552177795172712),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_7481', 0.2282466077983023),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_2393', 0.22923628527661188),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_9686', 0.22956496119538183),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_9070', 0.23132422132800823),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6179', 0.23167218649119325),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_1661', 0.23307795889533667),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_3570', 0.23392194513281117),\n",
       " ('model.layers.13.mlp.down_proj.weight_index_2393', 0.2339342958859838),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_9823', 0.23427504867430393),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_3672', 0.23439020686387613),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_1826', 0.23476400218591031),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5516', 0.2349217159953052),\n",
       " ('model.layers.13.mlp.down_proj.weight_index_2695', 0.23598884260024855),\n",
       " ('model.layers.28.mlp.down_proj.weight_index_875', 0.23670018861038544),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_912', 0.23694975625013193),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_10543', 0.23719458566826024),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_2298', 0.23800671029363318),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_9055', 0.2393139584297188),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_1087', 0.23946333485134597),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_875', 0.24041438716938757),\n",
       " ('model.layers.13.mlp.down_proj.weight_index_2213', 0.24051971008241857),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_727', 0.24169354849311464),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_3911', 0.24214448581680914),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_1751', 0.24242015776095727),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_2050', 0.24266973904414346),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_10327', 0.24287867093418924),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_7441', 0.24377991628674556),\n",
       " ('model.layers.28.mlp.up_proj.weight_index_10421', 0.24423155181540634),\n",
       " ('model.layers.31.mlp.gate_proj.weight_index_10', 0.24730161572295817),\n",
       " ('model.layers.0.mlp.down_proj.weight_index_1936', 0.24873378504614418),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_3287', 0.24892487475198966),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_5807', 0.2502122548117711),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_1260', 0.2504975264050926),\n",
       " ('model.layers.14.mlp.gate_proj.weight_index_9826', 0.2509059937026672),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_2991', 0.2521773469769881),\n",
       " ('model.layers.14.mlp.down_proj.weight_index_1692', 0.2526960035478609),\n",
       " ('model.layers.11.mlp.gate_proj.weight_index_10235', 0.25316304808208834),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_10403', 0.25361374518893376),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_9045', 0.2544513989313675),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_3147', 0.2558946736512784),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_2393', 0.25619921762372044),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_1346', 0.25665320030323846),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_2712', 0.2567599415784656),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_8945', 0.25758488461542317),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_7260', 0.2578527241748314),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_911', 0.2590006264350504),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_10066', 0.25913210079905813),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_2789', 0.25928001933641776),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_3452', 0.2597914871819449),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_1697', 0.2598041587839437),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_1003', 0.25983505654605965),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_7423', 0.26017687993738114),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_3285', 0.2604257745610008),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_7027', 0.26092347665845717),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_1512', 0.2637574475318849),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_3671', 0.26392150819490867),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_9488', 0.2641004745963209),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_1052', 0.2642544003512395),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_2747', 0.2644984517673401),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_9617', 0.2645174210608734),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_9665', 0.2646707064438498),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_10898', 0.2650605763013778),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_857', 0.2652553093964709),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_3951', 0.26585792143790377),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_3628', 0.26613746411687433),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_2264', 0.2669032837359162),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4330', 0.26696726704321394),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_3059', 0.2676131918171949),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_1512', 0.2678874367453701),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_6340', 0.26866055245013154),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_7105', 0.26869824198628445),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_2789', 0.26885998362622665),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_10765', 0.2691142189681579),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_1333', 0.27001723236034536),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_9324', 0.2702124363538463),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_1820', 0.2709674627244638),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_380', 0.27228615280094903),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_4259', 0.2724202108201439),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_1698', 0.27299710092391294),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_1306', 0.27335811671052923),\n",
       " ('model.layers.0.mlp.down_proj.weight_index_4009', 0.27410544526231995),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_3986', 0.27463659198351786),\n",
       " ('model.layers.20.mlp.up_proj.weight_index_4752', 0.27471660730178016),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_9625', 0.27555102320474933),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_3909', 0.2756372088866019),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_3122', 0.27621826547290595),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_8296', 0.2765949114222237),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_735', 0.27671054696995334),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_964', 0.27765016515937146),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_6049', 0.27771744460043646),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_8834', 0.2777658993090122),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_39', 0.2781713618341555),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_3784', 0.2782698315034997),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_4460', 0.27889684307924956),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_5412', 0.27893944912054724),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_2198', 0.2802423470929125),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_3585', 0.28082408137516834),\n",
       " ('model.layers.13.mlp.gate_proj.weight_index_3728', 0.28203812239880444),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_3877', 0.28247710706242124),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_1147', 0.2831037930957372),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_7216', 0.28351096456281466),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_5027', 0.2835709596157776),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_6990', 0.2837631301467183),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_5043', 0.28436796147214016),\n",
       " ('model.layers.15.mlp.gate_proj.weight_index_7031', 0.2849978207331336),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_1890', 0.2856681249390114),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_355', 0.2865318981019622),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_3778', 0.28655596002314443),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_895', 0.2868580798608362),\n",
       " ('model.layers.6.mlp.gate_proj.weight_index_7050', 0.287387836153024),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_10037', 0.2874170248981267),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_832', 0.2879318058101257),\n",
       " ('model.layers.16.mlp.gate_proj.weight_index_7661', 0.2881129879979647),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_6807', 0.28847679325422915),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_3913', 0.28887059261023884),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_10361', 0.28917214098898913),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_10139', 0.28974375855213097),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_8302', 0.2902206228935138),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_712', 0.29028631620313705),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_141', 0.29059089019448736),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_10629', 0.2910456238910699),\n",
       " ('model.layers.14.mlp.gate_proj.weight_index_5809', 0.29142839825806366),\n",
       " ('model.layers.14.mlp.gate_proj.weight_index_9392', 0.29199433529987884),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_2396', 0.2923180663184275),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_9716', 0.29255843236376755),\n",
       " ('model.layers.3.mlp.gate_proj.weight_index_8502', 0.2929342284337384),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_3231', 0.2930264433018337),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_6530', 0.2933075955280322),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_516', 0.29346184840684186),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_9979', 0.29407912605792186),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_1373', 0.29483465628698413),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1308', 0.2949493305643971),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_7724', 0.2959126740628695),\n",
       " ('model.layers.28.mlp.down_proj.weight_index_1489', 0.29614374503216867),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_3597', 0.2962453564197389),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_2533', 0.29631816209229056),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_2347', 0.2963268801767951),\n",
       " ('model.layers.29.mlp.up_proj.weight_index_2432', 0.296674013453349),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_4249', 0.2969558600274178),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_779', 0.2973810432111179),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_9315', 0.29768058695820443),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_13', 0.29827984941432106),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_2180', 0.29840362096974227),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_6442', 0.29891567311318656),\n",
       " ('model.layers.14.mlp.down_proj.weight_index_568', 0.2992502828146719),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_3716', 0.29935647028388246),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_7946', 0.30019448854418807),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_1221', 0.30110554952557234),\n",
       " ('model.layers.29.mlp.down_proj.weight_index_1052', 0.3017927973770611),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_3563', 0.3024627895749532),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_10156', 0.30246571700030733),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_8049', 0.30322895592499144),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_6623', 0.30393943883101393),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_8078', 0.30406889568005147),\n",
       " ('model.layers.30.mlp.gate_proj.weight_index_1147', 0.3046155147640137),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_8595', 0.30508217072762855),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_6150', 0.3057800554971539),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_2381', 0.3061648568719497),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_3135', 0.30646465491214636),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_3960', 0.30650536323991684),\n",
       " ('model.layers.20.mlp.gate_proj.weight_index_9851', 0.3070222183456601),\n",
       " ('model.layers.28.mlp.up_proj.weight_index_8640', 0.30703964453934596),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_2715', 0.30708960000570773),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_1079', 0.30793604697254207),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_6452', 0.30857089335847165),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3368', 0.30865890564525467),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_507', 0.3093347833986346),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_2664', 0.3095363176350814),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_3598', 0.3106684115353411),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_3769', 0.3106870490601925),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_10271', 0.3111368208432701),\n",
       " ('model.layers.29.mlp.down_proj.weight_index_3918', 0.3113346861107109),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_2466', 0.3117166447881461),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_6192', 0.31179645726846505),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_6640', 0.31192060300923075),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_9165', 0.3127925191458161),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_6636', 0.31280354708208824),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_1692', 0.3128312111862379),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_11004', 0.31300894029210635),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_105', 0.3135633352790217),\n",
       " ('model.layers.29.mlp.down_proj.weight_index_1401', 0.31361777141964176),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_6232', 0.3143174578666814),\n",
       " ('model.layers.31.mlp.gate_proj.weight_index_8619', 0.31437699842795164),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_3209', 0.31450173542600357),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_675', 0.3146892950352349),\n",
       " ('model.layers.2.mlp.gate_proj.weight_index_837', 0.31477945507930594),\n",
       " ('model.layers.15.mlp.gate_proj.weight_index_2249', 0.31491385875947264),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_2846', 0.31506949056649614),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_233', 0.3160056371651998),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_3217', 0.3169988249683575),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_2219', 0.31704524379694776),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_7510', 0.31707301538104904),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_322', 0.31713499922682065),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_5356', 0.3171452621591979),\n",
       " ('model.layers.20.mlp.gate_proj.weight_index_1895', 0.3172342484493442),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2752', 0.3177138606190306),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_2164', 0.3183308899734971),\n",
       " ('model.layers.17.mlp.gate_proj.weight_index_3062', 0.3189306181663376),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_2977', 0.3191005643457876),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_8167', 0.31932524641077276),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_3674', 0.31958406255327754),\n",
       " ('model.layers.8.mlp.gate_proj.weight_index_4816', 0.3199746932900829),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_4070', 0.32071757901143627),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_2496', 0.3209383929551044),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_5373', 0.3214685252651863),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_2937', 0.32147179931555536),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_287', 0.3228053666790789),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_7832', 0.3231305464201215),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_2230', 0.3243630088788456),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_4599', 0.324831064242618),\n",
       " ('model.layers.1.mlp.down_proj.weight_index_186', 0.32484125648998186),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_9873', 0.32551769799279695),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_3055', 0.3274251968299593),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_1622', 0.3281755713250054),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_1300', 0.32823704128617015),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_8102', 0.3293916383461055),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_2165', 0.3296742899235259),\n",
       " ('model.layers.5.mlp.down_proj.weight_index_1115', 0.330591765240436),\n",
       " ('model.layers.17.mlp.up_proj.weight_index_2624', 0.3308538680436284),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3403', 0.3313085437511183),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_7318', 0.33159787173081545),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_2732', 0.33160200918205796),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_7538', 0.33171360214087553),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_4876', 0.3317156633365208),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_8887', 0.33183043443045523),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_9259', 0.3320776464034756),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_2393', 0.33217968806175735),\n",
       " ('model.layers.30.mlp.gate_proj.weight_index_1140', 0.3324343885817811),\n",
       " ('model.layers.29.mlp.up_proj.weight_index_739', 0.3329009824495808),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_2828', 0.3329046944752383),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_4729', 0.33325272193209754),\n",
       " ('model.layers.13.mlp.gate_proj.weight_index_7694', 0.3336407331645401),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_1243', 0.3337461581812038),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4722', 0.3337545903515653),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_1661', 0.33382087530382476),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_292', 0.33396631083256256),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_178', 0.3340895075946202),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_1339', 0.3341190045722575),\n",
       " ('model.layers.20.mlp.up_proj.weight_index_9851', 0.3341951572549484),\n",
       " ('model.layers.7.mlp.gate_proj.weight_index_8731', 0.3346680047026007),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_9213', 0.3347292392364283),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_6088', 0.33495746886685396),\n",
       " ('model.layers.28.mlp.up_proj.weight_index_7873', 0.3350193244183801),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_2092', 0.3352850261388536),\n",
       " ('model.layers.14.mlp.gate_proj.weight_index_21', 0.33567527939509123),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_703', 0.33578356144682564),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_2493', 0.3359869150501593),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_4613', 0.3365921930187463),\n",
       " ('model.layers.28.mlp.down_proj.weight_index_2350', 0.33710324586928575),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_330', 0.3372240606330785),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_7756', 0.33722520612182105),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_5544', 0.3374854951940236),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6030', 0.3378983462885956),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_7298', 0.33828336205311826),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_6602', 0.33851947199262433),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_3359', 0.3385739705980422),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_1351', 0.33880472653956906),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_4323', 0.33889582469543544),\n",
       " ('model.layers.13.mlp.down_proj.weight_index_535', 0.33901125599176485),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_9186', 0.3390877753819974),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_7634', 0.33937125775878174),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_3431', 0.3396072419418159),\n",
       " ('model.layers.14.mlp.down_proj.weight_index_3334', 0.340134393380346),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_1991', 0.34013705488104096),\n",
       " ('model.layers.5.mlp.down_proj.weight_index_3431', 0.3404334706038932),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_827', 0.34105801150390613),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_4746', 0.341624179567213),\n",
       " ('model.layers.17.mlp.up_proj.weight_index_6491', 0.34230048177967465),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_3356', 0.34250122602039434),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_1500', 0.34250562575192456),\n",
       " ('model.layers.14.mlp.gate_proj.weight_index_4452', 0.34284914220625096),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6234', 0.34306916694488265),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_117', 0.3433257443497788),\n",
       " ('model.layers.27.mlp.up_proj.weight_index_7501', 0.3433977972649971),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_2827', 0.3439401744847661),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_10578', 0.34445771773553524),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_1793', 0.34462137610741905),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_167', 0.3449892988549843),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_10459', 0.34508721987646584),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_9544', 0.3458101439250556),\n",
       " ('model.layers.13.mlp.gate_proj.weight_index_8192', 0.34586693989600104),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_427', 0.34657553338055136),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_10526', 0.34672684315893765),\n",
       " ('model.layers.13.mlp.down_proj.weight_index_2270', 0.3468314551703662),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_4988', 0.3471009585060032),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_2836', 0.3475256836331422),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_688', 0.34767101207516937),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_7679', 0.3477288335134081),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_2789', 0.3479013427083961),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_2350', 0.34799527942193276),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_3497', 0.34818546180693133),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_5388', 0.34897115687570013),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_1370', 0.34915367804509945),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_964', 0.3496338848070004),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_3289', 0.3496605644111259),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_1120', 0.3498105100558928),\n",
       " ('model.layers.8.mlp.gate_proj.weight_index_2145', 0.35027440714721036),\n",
       " ('model.layers.18.mlp.gate_proj.weight_index_1144', 0.35053287287793067),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_1422', 0.3506977361881445),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_4269', 0.35117399796829796),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_2052', 0.35154130065055655),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_626', 0.35177520163581155),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_9663', 0.3519116488638985),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_3618', 0.3523583099678649),\n",
       " ('model.layers.21.mlp.down_proj.weight_index_94', 0.3525326124568342),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_1153', 0.35255138691298615),\n",
       " ('model.layers.21.mlp.up_proj.weight_index_3015', 0.3527550892075757),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_2412', 0.35296674850867404),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_664', 0.35342324873133446),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_3350', 0.35371036887906815),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_4047', 0.353751918238161),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_3729', 0.3539372051348435),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5266', 0.3542856773949379),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_7377', 0.35464579709191435),\n",
       " ('model.layers.0.mlp.down_proj.weight_index_1145', 0.3547704345939571),\n",
       " ('model.layers.14.mlp.gate_proj.weight_index_5556', 0.3550163692971702),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_10487', 0.35588766056808163),\n",
       " ('model.layers.29.mlp.up_proj.weight_index_7006', 0.35621765522603255),\n",
       " ('model.layers.17.mlp.down_proj.weight_index_2050', 0.35632512402354566),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_8885', 0.35638665441375883),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_3416', 0.3566784998796826),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_8497', 0.3568253410738391),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_183', 0.35698428978756747),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_1601', 0.35763857162506785),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_2393', 0.35767744236765076),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_7987', 0.35775631918670436),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_3608', 0.35843088035633563),\n",
       " ('model.layers.28.mlp.down_proj.weight_index_225', 0.358489502044522),\n",
       " ('model.layers.5.mlp.down_proj.weight_index_2789', 0.3593222364976989),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_913', 0.3597308441759157),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_1039', 0.3601637079246309),\n",
       " ('model.layers.14.mlp.gate_proj.weight_index_9714', 0.36035911245677443),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_9977', 0.3605462370384074),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_10392', 0.3606347298073107),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_2539', 0.3609483933575435),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_8589', 0.361790523521885),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_10081', 0.3636481590942302),\n",
       " ('model.layers.15.mlp.gate_proj.weight_index_937', 0.36398773729538414),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_2139', 0.3641117898956989),\n",
       " ('model.layers.15.mlp.gate_proj.weight_index_4914', 0.36419353883699745),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_1298', 0.364575388357649),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_4765', 0.365059419929441),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_187', 0.3651018798331256),\n",
       " ('model.layers.13.mlp.gate_proj.weight_index_8016', 0.36603264978839345),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_1395', 0.36636231642371),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_2126', 0.36638097872526965),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_1377', 0.3664050727147936),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_149', 0.3671805980889),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_3364', 0.36724740894000485),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_2305', 0.3672712948252759),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_7737', 0.3673823594318244),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_673', 0.36740152557951866),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_936', 0.36755821798551036),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_2358', 0.3678028857483304),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_2549', 0.3680849658679568),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_588', 0.36819907249850914),\n",
       " ('model.layers.27.mlp.up_proj.weight_index_5162', 0.36830359533225243),\n",
       " ('model.layers.11.mlp.down_proj.weight_index_3541', 0.3689471877685606),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_9266', 0.3689619900097387),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_8514', 0.3690673369734738),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_3529', 0.36948835478100506),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_7161', 0.37013613353138064),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_5834', 0.3701831778978124),\n",
       " ('model.layers.28.mlp.down_proj.weight_index_2294', 0.3706112488955249),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_312', 0.3714072586232091),\n",
       " ('model.layers.23.mlp.down_proj.weight_index_2210', 0.37183449766837207),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_5208', 0.37186771751400993),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_2108', 0.37196692212679405),\n",
       " ('model.layers.20.mlp.up_proj.weight_index_2046', 0.3721611406889118),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2326', 0.37216848028801586),\n",
       " ('model.layers.28.mlp.down_proj.weight_index_3015', 0.372185217480407),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_1459', 0.37293364629239223),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_1696', 0.3730642449391497),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_1854', 0.3735077329339589),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_5479', 0.373630029702408),\n",
       " ('model.layers.13.mlp.down_proj.weight_index_2864', 0.3740809823562459),\n",
       " ('model.layers.13.mlp.gate_proj.weight_index_3994', 0.3742415950730704),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_3190', 0.3743964503128967),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_2249', 0.3744355915376478),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_750', 0.3758068167273776),\n",
       " ('model.layers.21.mlp.down_proj.weight_index_3918', 0.3758476080567186),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_5436', 0.37617541432420776),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_798', 0.3761774693141806),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_2870', 0.3763014303120138),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_2157', 0.37694117479350275),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_6522', 0.3778794863047872),\n",
       " ('model.layers.18.mlp.down_proj.weight_index_637', 0.37794108553367955),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_8753', 0.37821278345706233),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_76', 0.3792131711312998),\n",
       " ('model.layers.11.mlp.down_proj.weight_index_396', 0.37924670043932585),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_324', 0.37948985253425427),\n",
       " ('model.layers.17.mlp.down_proj.weight_index_3625', 0.3796088253069998),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_1169', 0.379612208848465),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_9470', 0.3797778776465366),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_1631', 0.38022064629465646),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_3723', 0.38054786428537835),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2712', 0.3806481249403868),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_1735', 0.380864492736424),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_6404', 0.3810091346349771),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_96', 0.38101124058348956),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_2949', 0.38121928465729793),\n",
       " ('model.layers.29.mlp.down_proj.weight_index_1539', 0.38241996497210895),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_9807', 0.38274577573617385),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_7507', 0.3831062364440485),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_1817', 0.38323807407205734),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_1339', 0.3834109293371939),\n",
       " ('model.layers.5.mlp.gate_proj.weight_index_6246', 0.3836171010196354),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_8301', 0.384016215522887),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_2896', 0.38473238814534394),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_7881', 0.385300539067011),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_311', 0.3853556176564634),\n",
       " ('model.layers.17.mlp.up_proj.weight_index_7155', 0.3854270791297023),\n",
       " ('model.layers.14.mlp.down_proj.weight_index_167', 0.3854764972731881),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_1153', 0.3854940120017307),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_10866', 0.38617209227471605),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3741', 0.38664263601639437),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_2298', 0.3866830007845685),\n",
       " ('model.layers.1.mlp.up_proj.weight_index_7085', 0.3867255545878501),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_2338', 0.38696592162178023),\n",
       " ('model.layers.6.mlp.gate_proj.weight_index_5511', 0.3872604968703106),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_6515', 0.3873701012910633),\n",
       " ('model.layers.1.mlp.up_proj.weight_index_137', 0.38754890407963316),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_254', 0.3879130151956751),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4342', 0.3880761725703441),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_4000', 0.38834286066633306),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_7487', 0.38864663449499126),\n",
       " ('model.layers.14.mlp.down_proj.weight_index_964', 0.3889093193685773),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_8617', 0.38894179717026445),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_779', 0.3890574571246561),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_2792', 0.38943054595054916),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_1055', 0.38954536401691264),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_7775', 0.389692821736932),\n",
       " ('model.layers.29.mlp.down_proj.weight_index_3295', 0.390143220823286),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_1456', 0.39039293335448066),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_1859', 0.39067273558371163),\n",
       " ('model.layers.27.mlp.up_proj.weight_index_7359', 0.390747557730891),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_6458', 0.3908072732031278),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_2561', 0.3909135993469639),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_6802', 0.39135056887948716),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_4395', 0.3915070588487568),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_2119', 0.3916749658756098),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_352', 0.3918740810656165),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_783', 0.39210517922843025),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_1960', 0.39311977318876856),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_726', 0.3935765252911949),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_3368', 0.3935916054997701),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_3570', 0.3941119750953268),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_10714', 0.39449830117146867),\n",
       " ('model.layers.17.mlp.up_proj.weight_index_7219', 0.39472071450258417),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_2789', 0.394791077323168),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_6132', 0.394820555380631),\n",
       " ('model.layers.1.mlp.up_proj.weight_index_940', 0.3949452068984556),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_8527', 0.3953139718022247),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_9558', 0.3956653325470203),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_102', 0.39634668355438096),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_3152', 0.3965773439451179),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_5360', 0.39709515321395106),\n",
       " ('model.layers.29.mlp.down_proj.weight_index_2298', 0.3970975892550359),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_2750', 0.397149020187737),\n",
       " ('model.layers.17.mlp.down_proj.weight_index_94', 0.39731082707650955),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_2208', 0.3980448419968905),\n",
       " ('model.layers.17.mlp.down_proj.weight_index_2393', 0.39807333741720097),\n",
       " ('model.layers.29.mlp.down_proj.weight_index_168', 0.39942312833799054),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_6212', 0.39952106729224024),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_9092', 0.3995852584520212),\n",
       " ('model.layers.23.mlp.down_proj.weight_index_3779', 0.4000557442986965),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_4339', 0.4000908586610681),\n",
       " ('model.layers.31.mlp.gate_proj.weight_index_2581', 0.4002961660791544),\n",
       " ('model.layers.13.mlp.gate_proj.weight_index_10231', 0.4003641026330844),\n",
       " ('model.layers.14.mlp.down_proj.weight_index_495', 0.4006121181590556),\n",
       " ('model.layers.0.mlp.down_proj.weight_index_2409', 0.40137038978048833),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_3570', 0.40228995958759306),\n",
       " ('model.layers.1.mlp.up_proj.weight_index_238', 0.4025838019850463),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_3682', 0.40280929875412097),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_3951', 0.40281922484160226),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_914', 0.40293743146657013),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_1207', 0.40294190100904403),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_6263', 0.403130789606843),\n",
       " ('model.layers.1.mlp.gate_proj.weight_index_4786', 0.4031813844243497),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_16', 0.40340460141493706),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_574', 0.4037852153582815),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_1542', 0.40392471646762873),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_10907', 0.40429323378924975),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_1430', 0.40517451022952145),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_768', 0.4054300965783746),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_2084', 0.4055252812293366),\n",
       " ('model.layers.26.mlp.up_proj.weight_index_9264', 0.40556007032933694),\n",
       " ('model.layers.0.mlp.down_proj.weight_index_261', 0.40582977161543754),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_2684', 0.40583714438809704),\n",
       " ('model.layers.31.mlp.gate_proj.weight_index_1921', 0.4060813772321312),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_7722', 0.4061110324647359),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_7403', 0.40720420104203514),\n",
       " ('model.layers.14.mlp.down_proj.weight_index_2829', 0.40756578057275217),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_2903', 0.4076762501580595),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_5204', 0.4079973259929046),\n",
       " ('model.layers.29.mlp.down_proj.weight_index_1351', 0.4083072353615127),\n",
       " ('model.layers.0.mlp.down_proj.weight_index_1931', 0.40989857339272984),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_2800', 0.4102154900820487),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_2797', 0.41061189886286265),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_184', 0.4108803320188166),\n",
       " ('model.layers.1.mlp.gate_proj.weight_index_5067', 0.41137946276067483),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_2617', 0.41177227478323974),\n",
       " ('model.layers.29.mlp.down_proj.weight_index_2622', 0.4122310233059321),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_8601', 0.412590637140696),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_631', 0.4128462172277305),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_1994', 0.4132419061881576),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_7677', 0.4136455163251074),\n",
       " ('model.layers.0.mlp.down_proj.weight_index_2130', 0.41395958307863534),\n",
       " ('model.layers.11.mlp.down_proj.weight_index_2319', 0.4142053154500376),\n",
       " ('model.layers.4.mlp.down_proj.weight_index_747', 0.41433266742773034),\n",
       " ('model.layers.8.mlp.gate_proj.weight_index_8279', 0.4144616799726828),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_155', 0.41458094062012973),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_2152', 0.41480089888282556),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_1040', 0.4148598513557036),\n",
       " ('model.layers.28.mlp.down_proj.weight_index_1706', 0.4151457854785541),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_6584', 0.4153628753727179),\n",
       " ('model.layers.29.mlp.down_proj.weight_index_2558', 0.4158075392020155),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_1633', 0.41632618843866),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_2349', 0.416507837339239),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_3611', 0.4165268835290217),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_7402', 0.4171596894357741),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_7281', 0.41718387558757586),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_345', 0.41719180519369603),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_3872', 0.41741406607601217),\n",
       " ('model.layers.18.mlp.up_proj.weight_index_9585', 0.4174456984431556),\n",
       " ('model.layers.19.mlp.gate_proj.weight_index_10162', 0.4174872602863946),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_4382', 0.41787190040292144),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_8805', 0.41801394527900815),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_5818', 0.4180471033408568),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_7702', 0.4180939523298126),\n",
       " ('model.layers.5.mlp.down_proj.weight_index_3673', 0.41833834682742665),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_5578', 0.41851840974966903),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1796', 0.4185987196198524),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_1762', 0.4190060691042681),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_2014', 0.41905611533582254),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_531', 0.4190939325142429),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_621', 0.41919705700286425),\n",
       " ('model.layers.11.mlp.gate_proj.weight_index_8718', 0.4201117854507417),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_2102', 0.4204051420761772),\n",
       " ('model.layers.14.mlp.down_proj.weight_index_2264', 0.42040859883136683),\n",
       " ('model.layers.17.mlp.down_proj.weight_index_2251', 0.4205306468543415),\n",
       " ('model.layers.21.mlp.down_proj.weight_index_2684', 0.42053134143894777),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_8165', 0.42068757606241336),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_3032', 0.4207728478001469),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_8381', 0.42100344239714715),\n",
       " ('model.layers.13.mlp.down_proj.weight_index_2669', 0.42116654190212355),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_897', 0.42120984444486576),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_3294', 0.42123132373229755),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_2113', 0.42136810489575893),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_8028', 0.4214264693785106),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_3090', 0.42157177893598785),\n",
       " ('model.layers.2.mlp.gate_proj.weight_index_387', 0.4216890183033426),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_8500', 0.4220910464766292),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_339', 0.42221515131250875),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_2657', 0.4225370433055158),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_7981', 0.42270340370296644),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6188', 0.4228732934996655),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_3785', 0.423045938132776),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_2337', 0.42312559606097455),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_4071', 0.4232467653019256),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_1049', 0.4233506825729423),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_3391', 0.4234201559485662),\n",
       " ('model.layers.29.mlp.up_proj.weight_index_7865', 0.4236363356960213),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_9443', 0.42432615817990094),\n",
       " ('model.layers.17.mlp.gate_proj.weight_index_1147', 0.42443365137590483),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_10762', 0.4245244996266284),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_5129', 0.4256078777417698),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_2714', 0.4256348548384423),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_2886', 0.4258136933608627),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_4253', 0.42671840010193574),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_2441', 0.4268757150105844),\n",
       " ('model.layers.13.mlp.down_proj.weight_index_2754', 0.427241451476978),\n",
       " ('model.layers.22.mlp.down_proj.weight_index_631', 0.42745944550326787),\n",
       " ('model.layers.25.mlp.gate_proj.weight_index_5834', 0.42770531502715414),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_870', 0.4277471433074722),\n",
       " ('model.layers.13.mlp.gate_proj.weight_index_5569', 0.4280179355236511),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_3679', 0.42805276994309516),\n",
       " ('model.layers.13.mlp.down_proj.weight_index_228', 0.4281206482987949),\n",
       " ('model.layers.28.mlp.down_proj.weight_index_3918', 0.4283347445853054),\n",
       " ('model.layers.28.mlp.up_proj.weight_index_1271', 0.4283800088741181),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_2175', 0.4293759193915716),\n",
       " ('model.layers.0.mlp.down_proj.weight_index_950', 0.4293862506817554),\n",
       " ('model.layers.7.mlp.gate_proj.weight_index_2111', 0.4294828561444497),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_5931', 0.42962885622937375),\n",
       " ('model.layers.29.mlp.down_proj.weight_index_2244', 0.4296965678407436),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_2883', 0.4304383209933871),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_6977', 0.43060765116158173),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_9046', 0.4306589045620428),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_513', 0.4309181293020221),\n",
       " ('model.layers.24.mlp.up_proj.weight_index_3534', 0.43112537412905283),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_1314', 0.4313663376728192),\n",
       " ('model.layers.3.mlp.gate_proj.weight_index_9598', 0.4315674608940694),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_1922', 0.43162322228553895),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_85', 0.43176224032625976),\n",
       " ('model.layers.28.mlp.up_proj.weight_index_8094', 0.4317808663123923),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_2376', 0.4318175166325702),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_491', 0.4318472011890444),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_3604', 0.4321280661664404),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_652', 0.4322293695926911),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_2048', 0.4324370506976143),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_2340', 0.4325956715614243),\n",
       " ('model.layers.18.mlp.up_proj.weight_index_322', 0.43295298904803525),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4056', 0.4332090772313255),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_8865', 0.4338837805436775),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_4075', 0.4341501277278841),\n",
       " ('model.layers.13.mlp.gate_proj.weight_index_956', 0.43452117467027307),\n",
       " ('model.layers.15.mlp.gate_proj.weight_index_10583', 0.43455130067918013),\n",
       " ('model.layers.17.mlp.gate_proj.weight_index_7811', 0.4346321519859888),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_8226', 0.434762441421952),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_10268', 0.43494180609123134),\n",
       " ('model.layers.2.mlp.up_proj.weight_index_6547', 0.43517030490306485),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_10779', 0.43523776925682167),\n",
       " ('model.layers.5.mlp.down_proj.weight_index_2825', 0.4355834798103646),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_3670', 0.4359621002590903),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_1251', 0.43619920732082784),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_3234', 0.43641200785379164),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_6084', 0.4367076455295993),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_1585', 0.43671981084039846),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_4062', 0.4368757523952036),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_3431', 0.4369138895659286),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_1272', 0.4369644636838492),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_2282', 0.43721274459623194),\n",
       " ('model.layers.31.mlp.gate_proj.weight_index_7995', 0.43736514507543234),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_10439', 0.43759673411595124),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_4716', 0.4379619408722304),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_8575', 0.43813862586791474),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_2648', 0.43819663312105783),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_6325', 0.4382715905242933),\n",
       " ('model.layers.22.mlp.down_proj.weight_index_1401', 0.4383201763086797),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_967', 0.43832609694209523),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_4003', 0.4385019941284871),\n",
       " ('model.layers.31.mlp.gate_proj.weight_index_3093', 0.43876551956232346),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_3955', 0.439189907909181),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_1532', 0.4392338580153341),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_964', 0.43923926461024987),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_2814', 0.43937183518891043),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_5800', 0.43957975412085837),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_8753', 0.4396655824436704),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_6534', 0.4397342111008222),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_945', 0.4398002804972383),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_1025', 0.43988460227677795),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_629', 0.4399038588277797),\n",
       " ('model.layers.17.mlp.down_proj.weight_index_2287', 0.4400277686180001),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_7200', 0.4401788642343991),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_2638', 0.4406718399529659),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_7131', 0.4407734720907266),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_2050', 0.4409153266412398),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_7146', 0.44132701118567663),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_1608', 0.44137294463351173),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_4098', 0.44137845212885907),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_3626', 0.4414772137185081),\n",
       " ('model.layers.17.mlp.up_proj.weight_index_6399', 0.44153514799996874),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_3593', 0.4416170499067227),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_3275', 0.44231270332391404),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_3870', 0.4423697678005576),\n",
       " ('model.layers.17.mlp.down_proj.weight_index_2573', 0.4429686962927373),\n",
       " ('model.layers.0.mlp.down_proj.weight_index_2384', 0.4430281762732031),\n",
       " ('model.layers.11.mlp.gate_proj.weight_index_613', 0.4430710694648359),\n",
       " ('model.layers.26.mlp.up_proj.weight_index_10404', 0.4430785120619194),\n",
       " ('model.layers.11.mlp.down_proj.weight_index_2323', 0.4432025150943595),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_6636', 0.4432417320170101),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_9121', 0.44327421263549605),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_3343', 0.4437211741734046),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_861', 0.44410161672708837),\n",
       " ('model.layers.28.mlp.up_proj.weight_index_6530', 0.4443737258063116),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_128', 0.44460630720848116),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_94', 0.4446344941956415),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_1244', 0.44475538649186674),\n",
       " ('model.layers.26.mlp.down_proj.weight_index_3779', 0.44493401686324363),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_3360', 0.44502648370918596),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_4891', 0.4451147245103204),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_7428', 0.4452697430111381),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_2023', 0.4453233872035045),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_1758', 0.44541497903031235),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_8981', 0.44573766639204226),\n",
       " ('model.layers.5.mlp.gate_proj.weight_index_4424', 0.44576025642632455),\n",
       " ('model.layers.29.mlp.up_proj.weight_index_1925', 0.44635204878745016),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_835', 0.4464383831218237),\n",
       " ('model.layers.13.mlp.down_proj.weight_index_2822', 0.44660968042399674),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_6714', 0.446744408099105),\n",
       " ('model.layers.25.mlp.up_proj.weight_index_5291', 0.4468746241988941),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_3640', 0.4469030748263809),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_10002', 0.44691976461446004),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_1023', 0.4470080327547108),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_149', 0.44727960578407533),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_1092', 0.44785724603080057),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_2801', 0.4478788960622788),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_851', 0.4483595825549118),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_2669', 0.4485942270352523),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_6173', 0.4495492956838216),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_675', 0.4499063846117437),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_315', 0.45001352108902903),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_76', 0.4500644209990572),\n",
       " ('model.layers.29.mlp.down_proj.weight_index_703', 0.45036010758143696),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_5468', 0.4504256438467582),\n",
       " ('model.layers.7.mlp.gate_proj.weight_index_3723', 0.4508833827609049),\n",
       " ('model.layers.0.mlp.down_proj.weight_index_183', 0.4509310437208658),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2254', 0.4509577910840159),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_2426', 0.4510892732307408),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_2179', 0.451393961285536),\n",
       " ('model.layers.13.mlp.down_proj.weight_index_41', 0.4515065458047709),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_1148', 0.45152157648566815),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_938', 0.4519575972030849),\n",
       " ('model.layers.12.mlp.gate_proj.weight_index_747', 0.4520862234411469),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_1340', 0.4522308758205029),\n",
       " ('model.layers.19.mlp.gate_proj.weight_index_4649', 0.45232668652670194),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_5271', 0.4524366715339938),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_3364', 0.4527644022901014),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_8964', 0.452772197835174),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_1473', 0.45281199128724525),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_2016', 0.45297359499605294),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_1994', 0.45316544493375677),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_3930', 0.4533416681349234),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_1314', 0.45338767166354055),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_1743', 0.45340608799746196),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_5514', 0.4536839808418667),\n",
       " ('model.layers.14.mlp.gate_proj.weight_index_6140', 0.453878542016255),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_3431', 0.45389213640836656),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_927', 0.4538960282223554),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_339', 0.45393898117207554),\n",
       " ('model.layers.13.mlp.down_proj.weight_index_3137', 0.45403821991843163),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_2288', 0.4542179758446294),\n",
       " ('model.layers.29.mlp.up_proj.weight_index_3340', 0.454650802734919),\n",
       " ('model.layers.23.mlp.down_proj.weight_index_4006', 0.45486614831093775),\n",
       " ('model.layers.0.mlp.down_proj.weight_index_2591', 0.4550356786470262),\n",
       " ('model.layers.1.mlp.up_proj.weight_index_1935', 0.4550614026578743),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_874', 0.45513469294539366),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_2938', 0.4553622567025495),\n",
       " ('model.layers.28.mlp.up_proj.weight_index_229', 0.4556286046046587),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_4000', 0.45588309222799506),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_5401', 0.45601215213550406),\n",
       " ('model.layers.26.mlp.up_proj.weight_index_8871', 0.4562215062423669),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_280', 0.4562738262517039),\n",
       " ('model.layers.2.mlp.up_proj.weight_index_8665', 0.45630780302423046),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_1342', 0.45633860977760454),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_1786', 0.4567655615776851),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_996', 0.4568148438210522),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_7908', 0.4568318355281633),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_7461', 0.4569853936289596),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_6657', 0.45716274751151387),\n",
       " ('model.layers.0.mlp.down_proj.weight_index_2526', 0.45721545063987623),\n",
       " ('model.layers.23.mlp.down_proj.weight_index_2350', 0.45726884990205274),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_2679', 0.4573210298534902),\n",
       " ('model.layers.29.mlp.down_proj.weight_index_507', 0.4573378671336332),\n",
       " ('model.layers.14.mlp.down_proj.weight_index_2595', 0.4576157567222836),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_2050', 0.45830477602939723),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_3773', 0.4583728088313377),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_3215', 0.45861224363354314),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_3520', 0.4586848073774781),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_10203', 0.4589213376264931),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_3069', 0.4590697499609102),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_2171', 0.4591482988322313),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_5911', 0.4593012482103833),\n",
       " ('model.layers.12.mlp.gate_proj.weight_index_9378', 0.4593193474384831),\n",
       " ('model.layers.23.mlp.down_proj.weight_index_1453', 0.4598642886300315),\n",
       " ('model.layers.14.mlp.down_proj.weight_index_2664', 0.4600958969848228),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_2927', 0.4601598537489324),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_9463', 0.4606334907952969),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_2883', 0.460683151140469),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4579', 0.4608471658474613),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_9291', 0.4611453228688056),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_5917', 0.4613131541519202),\n",
       " ('model.layers.25.mlp.gate_proj.weight_index_6568', 0.46139456850264926),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_1055', 0.46150895393654645),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_5289', 0.46176491796822816),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_1792', 0.4617863785097214),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_1528', 0.46185268676244373),\n",
       " ('model.layers.25.mlp.up_proj.weight_index_10045', 0.46199695451647615),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_2184', 0.46230360601679976),\n",
       " ('model.layers.19.mlp.down_proj.weight_index_1739', 0.4624139106969922),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_4729', 0.4624267853897468),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_2363', 0.4625731500437733),\n",
       " ('model.layers.28.mlp.up_proj.weight_index_10002', 0.46313320611195063),\n",
       " ('model.layers.13.mlp.gate_proj.weight_index_5259', 0.4631686362678007),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_1208', 0.46336995773385947),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_3994', 0.46381329581886854),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_2004', 0.4638427367725497),\n",
       " ('model.layers.17.mlp.down_proj.weight_index_4006', 0.4645809613248404),\n",
       " ('model.layers.30.mlp.up_proj.weight_index_2657', 0.4646654973103881),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_6756', 0.4655449238638041),\n",
       " ('model.layers.5.mlp.down_proj.weight_index_2050', 0.46556348801549774),\n",
       " ('model.layers.3.mlp.gate_proj.weight_index_8272', 0.4656341908262984),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_802', 0.46591437715410633),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_119', 0.4659268691071823),\n",
       " ('model.layers.8.mlp.down_proj.weight_index_3689', 0.46622477532433404),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_8738', 0.4666826289966952),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_1663', 0.4668468994037105),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_73', 0.4671261962108204),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_3220', 0.4671622665137085),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_3364', 0.4681956113802932),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_2363', 0.4686871900523615),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_6929', 0.4687071076885241),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_717', 0.4689709555035737),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_2546', 0.4691909061237869),\n",
       " ('model.layers.2.mlp.up_proj.weight_index_10150', 0.4694210400485481),\n",
       " ('model.layers.27.mlp.up_proj.weight_index_674', 0.46942884087456394),\n",
       " ('model.layers.31.mlp.down_proj.weight_index_2661', 0.4695100716531684),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_540', 0.46958702582216283),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_2679', 0.46966753472981093),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_3384', 0.4697034666726019),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_1254', 0.4698735267588545),\n",
       " ('model.layers.23.mlp.up_proj.weight_index_9634', 0.469909513403195),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_9034', 0.46993091597290126),\n",
       " ('model.layers.7.mlp.gate_proj.weight_index_95', 0.47002550576854984),\n",
       " ('model.layers.13.mlp.down_proj.weight_index_3491', 0.4700386514141668),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_3302', 0.47004727980577465),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_2556', 0.470209347716084),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_3650', 0.47035166325624056),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_631', 0.4704410833484629),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_299', 0.47064876206031014),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_2212', 0.47152920695349687),\n",
       " ('model.layers.17.mlp.down_proj.weight_index_2679', 0.4716471754989957),\n",
       " ('model.layers.11.mlp.down_proj.weight_index_3720', 0.4718628649128327),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_1411', 0.4719644146902078),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_3826', 0.4720413065213229),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_7915', 0.4720581865519544),\n",
       " ('model.layers.6.mlp.down_proj.weight_index_1568', 0.4721508800269256),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_6742', 0.47229128624574557),\n",
       " ('model.layers.26.mlp.up_proj.weight_index_10207', 0.47240635463316716),\n",
       " ('model.layers.5.mlp.gate_proj.weight_index_5705', 0.472499980438724),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_7009', 0.4725837554961214),\n",
       " ('model.layers.27.mlp.down_proj.weight_index_2152', 0.4726516585794549),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_8112', 0.4729016904252519),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_2393', 0.47344696239958495),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_2664', 0.47389976073625295),\n",
       " ('model.layers.31.mlp.up_proj.weight_index_3107', 0.4744568654237211),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_6867', 0.4745064090436295),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_9618', 0.4747070840373486),\n",
       " ('model.layers.0.mlp.down_proj.weight_index_653', 0.4747494590716479),\n",
       " ('model.layers.26.mlp.up_proj.weight_index_8247', 0.47475898945958805),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_4121', 0.475065607177664),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_1980', 0.47506758852338926),\n",
       " ('model.layers.31.mlp.gate_proj.weight_index_2052', 0.4754148711348405),\n",
       " ('model.layers.28.mlp.down_proj.weight_index_1496', 0.4754347750587513),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_94', 0.47593035735480926),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_2941', 0.475933382920962),\n",
       " ('model.layers.10.mlp.down_proj.weight_index_3542', 0.4759493385111724),\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4267e-132c-4655-a0be-6ef3982bd7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_en = copy.deepcopy(model)\n",
    "model_en = set_neuron_zero(model_en, neuron_en, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d0af66d-23fb-4a65-b393-131c06cb0383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/autodl-fs/data/tmp/tokenizer_config.json',\n",
       " '/autodl-fs/data/tmp/special_tokens_map.json',\n",
       " '/autodl-fs/data/tmp/tokenizer.model',\n",
       " '/autodl-fs/data/tmp/added_tokens.json',\n",
       " '/autodl-fs/data/tmp/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"/autodl-fs/data/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a523325e-86df-40b7-b93d-58285416999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/autodl-fs/data/tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb6f978-c5a2-4f0e-9d56-c52ae7b9c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_neuron_zero_A_save_model(model_path, neuron_en):\n",
    "    \n",
    "    model, tokenizer = load_model(model_path)#load_model('/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-chat-hf') #load_model('/root/autodl-fs/model_zoo/google/gemma-3-1b-it')\n",
    "    \n",
    "    model_en = set_neuron_zero(model, neuron_en)\n",
    "\n",
    "    #save \n",
    "    model_en.save_pretrained(\"/autodl-fs/data/tmp/\")\n",
    "    tokenizer.save_pretrained(\"/autodl-fs/data/tmp/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a2d94de-718d-466c-8fc5-149420e65f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint: /root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab9921f2a58474a884560ca0dbab5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DTensor' from 'torch.distributed.tensor' (/root/miniconda3/lib/python3.10/site-packages/torch/distributed/tensor/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mset_neuron_zero_A_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-chat-hf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneuron_en\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mset_neuron_zero_A_save_model\u001b[0;34m(model_path, neuron_en)\u001b[0m\n\u001b[1;32m      5\u001b[0m model_en \u001b[38;5;241m=\u001b[39m set_neuron_zero(model, neuron_en)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#save \u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel_en\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/autodl-fs/data/tmp/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/autodl-fs/data/tmp/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:3572\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   3568\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   3569\u001b[0m     \u001b[38;5;66;03m# Sometimes in the state_dict we have non-tensor objects.\u001b[39;00m\n\u001b[1;32m   3570\u001b[0m     \u001b[38;5;66;03m# e.g. in bitsandbytes we have some `str` objects in the state_dict\u001b[39;00m\n\u001b[1;32m   3571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, DTensor):\n\u001b[0;32m-> 3572\u001b[0m         ptrs[\u001b[43mid_tensor_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m]\u001b[38;5;241m.\u001b[39mappend(name)\n\u001b[1;32m   3573\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3574\u001b[0m         \u001b[38;5;66;03m# In the non-tensor case, fall back to the pointer of the object itself\u001b[39;00m\n\u001b[1;32m   3575\u001b[0m         ptrs[\u001b[38;5;28mid\u001b[39m(tensor)]\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/pytorch_utils.py:300\u001b[0m, in \u001b[0;36mid_tensor_storage\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03mUnique identifier to a tensor storage. Multiple different tensors can share the same underlying storage. For\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03mexample, \"meta\" tensors all share the same storage, and thus their identifier will all be equal. This identifier is\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03mguaranteed to be unique and constant for this tensor's storage during its lifetime. Two tensor storages with\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03mnon-overlapping lifetimes may have the same id.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_greater_or_equal_than_2_0:\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DTensor\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, DTensor):\n\u001b[1;32m    303\u001b[0m         local_tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mto_local()\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DTensor' from 'torch.distributed.tensor' (/root/miniconda3/lib/python3.10/site-packages/torch/distributed/tensor/__init__.py)"
     ]
    }
   ],
   "source": [
    "set_neuron_zero_A_save_model('/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-chat-hf', neuron_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f11befb-4a9c-45c2-9d87-934ae7a65e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6d51c20-8aeb-47b5-92d3-9d97a8ff1989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2e56574-309f-46cb-91c0-4997fadb3460",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_en = torch.load('/root/autodl-fs/LRP_kur_res/20251124/LRP_kur_res_en.pt', weights_only=False)\n",
    "neuron_vi = torch.load('/root/autodl-fs/LRP_kur_res/20251124/LRP_kur_res_vi.pt', weights_only=False)\n",
    "neuron_zh = torch.load('/root/autodl-fs/LRP_kur_res/20251124/LRP_kur_res_zh.pt', weights_only=False)\n",
    "\n",
    "model_en = copy.deepcopy(model)\n",
    "model_en = set_neuron_zero(model_en, neuron_en)\n",
    "\n",
    "model_vi = copy.deepcopy(model)\n",
    "model_vi = set_neuron_zero(model_vi, neuron_vi)\n",
    "\n",
    "\n",
    "model_zh = copy.deepcopy(model)\n",
    "model_zh = set_neuron_zero(model_zh, neuron_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72d6806a-1afb-4461-a024-bfb171116e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.5146, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "atext = '你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n我来找你，我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你，我来找你，'\n",
    "\n",
    "\n",
    "ppl_sum, ppl_count = calc_ppl(model, tokenizer, [atext], max_len= 2048)\n",
    "print(ppl_sum/ppl_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff507613-87e1-4de2-a68f-e20656e1b1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 15.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.4133, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "atext = '你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n我来找你，我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你，我来找你，'\n",
    "\n",
    "\n",
    "ppl_sum, ppl_count = calc_ppl(model_zh, tokenizer, [atext], max_len= 2048)\n",
    "print(ppl_sum/ppl_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64b98b66-343b-409b-bbde-dff3ca45ca04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "model name: org_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 36.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.2627, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:32<00:00, 30.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(13.2703, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.5759, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_en_neuron_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 36.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.0624, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:32<00:00, 30.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(12.5349, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.2015, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_vi_neuron_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 36.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.9936, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 30.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(12.4351, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.3903, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_zh_neuron_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 36.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.8128, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 30.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(12.6063, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 34.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.1528, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "calc ppl\n",
    "\n",
    "'''\n",
    "\n",
    "# load test data\n",
    "test_en = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/en_radom_1000_test.jsonl')\n",
    "test_vi = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/vi_radom_1000_test.jsonl')\n",
    "test_zh = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/zh_radom_1000_test.jsonl')\n",
    "               \n",
    "# calc ppl\n",
    "model_list= [\n",
    "    (model, 'org_model'),\n",
    "    (model_en, 'mask_en_neuron_model'),\n",
    "    (model_vi, 'mask_vi_neuron_model'),\n",
    "    (model_zh, 'mask_zh_neuron_model'),\n",
    "]\n",
    "\n",
    "data_list = [\n",
    "    (test_en, 'en_data'),\n",
    "    (test_vi, 'vi_data'),\n",
    "    (test_zh, 'zh_data')\n",
    "    \n",
    "]\n",
    "\n",
    "for i_model, model_name in model_list:\n",
    "    print('*'*20)\n",
    "    print('*'*20)\n",
    "    print('model name:', model_name)\n",
    "\n",
    "    for i_data, data_name in data_list:\n",
    "        print('='*20)\n",
    "        print('data name:', data_name)\n",
    "        ppl_sum, ppl_count = calc_ppl(i_model, tokenizer, i_data, max_len= 2048)\n",
    "        print('ppl:',ppl_sum/ppl_count )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5577ddec-3541-4447-acfa-341abcbb6012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b1a8da-dd97-4f9c-a613-a9607f7f4584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf300c-a4d3-4a41-83ad-6709273b48b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c776812b-77e1-4f0a-8713-e8893aa3d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "textlist=[\n",
    "    'Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.',\n",
    "    'Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.',\n",
    "    '请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。',\n",
    "    'You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.',\n",
    "    'Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.',\n",
    "    '你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。'\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9b9002-c11d-4c30-acc5-2a6e8d702209",
   "metadata": {},
   "source": [
    "### LAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3bda5997-6837-42a2-8a91-2d05d8a22b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\norgmodel:\\n    1.Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n---\\n\\n**Model Description:**\\n\\nOur model is a transformer-based language model trained on a massive dataset of text and code in English, Spanish, French\\n    2.Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n**Mô hình ngôn ngữ lớn (LLM) - \"LinguaVerse\"**\\n\\nLinguaVerse là một mô hình ngôn ngữ lớn được xây\\n    3.请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n**模型架构：**\\n\\n*   **Transformer 架构:**  我们使用基于 Transformer 的模型，特别是 BERT 和 RoBERTa 的变体。\\n\\nmask en neuron:\\n    1.Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n}\\n\\n- \"It\" \\n- \"It\" \\n\\n*   \"The\"\\n*   \"or\"\\n*   \"It\\'\\n    2.Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n. . .\\n> . . . .\\n\\n. . . . . . .\\n\\n**\\n\\n**\\n\\n**\\n\\n,\\n\\n**\\n\\n,\\n\\n**\\n    3.请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n\\n\\n多语言输入模型的，可以有以下这些处理方法和技术：\\n\\n1  1 个性化。\\n2  1，1，\\n\\n\\nmask vi neuron:\\n    1.Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n**The Problem:**\\n\\nThe core of this is to process the input in a way that is most appropriate for the model, and to do so in a\\n    2.Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n---\\n\\nВкратце,\\nПросто наберите\\nЯ не\\nБудьте\\nВсе, что\\nВы знаете\\nЧто\\nКак\\n\\n    3.请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n“请帮助 me stay silent\" - It\\'s a beautiful day.\\nHow would you approach this problem?\\nBy far, but it\\'s\\n\\nmask zh neuron:\\n    1.Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\nHere re/this model explains how to handle the inconsistencies in the quality.\\n\\nThe following steps involve how to handle the calls to the model processes the requests\\n    2.Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau. (إหว่างமைப்பு والأسקה האגירעות)\\n(Murasala Citrix??)\\n\\nvề mặt máy chính ảnh posição filo? (\\n    3.请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n\\n\\n。\\n\\n。。а。о。о。\\n**ru。** **。**\\n**。**\\n**。**\\n**。**\\n\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "orgmodel:\n",
    "    1.Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n---\\n\\n**Model Description:**\\n\\nOur model is a transformer-based language model trained on a massive dataset of text and code in English, Spanish, French\n",
    "    2.Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n**Mô hình ngôn ngữ lớn (LLM) - \"LinguaVerse\"**\\n\\nLinguaVerse là một mô hình ngôn ngữ lớn được xây\n",
    "    3.请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n**模型架构：**\\n\\n*   **Transformer 架构:**  我们使用基于 Transformer 的模型，特别是 BERT 和 RoBERTa 的变体。\n",
    "\n",
    "mask en neuron:\n",
    "    1.Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n}\\n\\n- \"It\" \\n- \"It\" \\n\\n*   \"The\"\\n*   \"or\"\\n*   \"It\\'\n",
    "    2.Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n. . .\\n> . . . .\\n\\n. . . . . . .\\n\\n**\\n\\n**\\n\\n**\\n\\n,\\n\\n**\\n\\n,\\n\\n**\n",
    "    3.请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n\\n\\n多语言输入模型的，可以有以下这些处理方法和技术：\\n\\n1  1 个性化。\\n2  1，1，\\n\n",
    "\n",
    "mask vi neuron:\n",
    "    1.Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n**The Problem:**\\n\\nThe core of this is to process the input in a way that is most appropriate for the model, and to do so in a\n",
    "    2.Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n---\\n\\nВкратце,\\nПросто наберите\\nЯ не\\nБудьте\\nВсе, что\\nВы знаете\\nЧто\\nКак\\n\n",
    "    3.请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n“请帮助 me stay silent\" - It\\'s a beautiful day.\\nHow would you approach this problem?\\nBy far, but it\\'s\n",
    "\n",
    "mask zh neuron:\n",
    "    1.Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\nHere re/this model explains how to handle the inconsistencies in the quality.\\n\\nThe following steps involve how to handle the calls to the model processes the requests\n",
    "    2.Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau. (إหว่างமைப்பு والأسקה האגירעות)\\n(Murasala Citrix??)\\n\\nvề mặt máy chính ảnh posição filo? (\n",
    "    3.请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n\\n\\n。\\n\\n。。а。о。о。\\n**ru。** **。**\\n**。**\\n**。**\\n**。**\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d6584-dfdc-42df-bd89-08f85ba5a493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bfb36f1-7d8c-4a95-a80e-7f439e2d390f",
   "metadata": {},
   "source": [
    "## kurness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81d1d1f6-8ed8-44af-b47b-8fd2e130d607",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:04,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108,   7243,    108,   1018,   4968,\n",
      "          17243,  53121,    108,   7711,   2028,    563,    496,  29193, 236772,\n",
      "           5140,   5192,   2028,  15453,    580,    496,  12566,  15297,    529,\n",
      "           1816,    532,   3393,    528,   5422, 236764,  14361, 236764,   7830]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n---\\n\\n**Model Description:**\\n\\nOur model is a transformer-based language model trained on a massive dataset of text and code in English, Spanish, French']\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:01<00:03,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            108,   1018, 236792, 237124,  14960, 140907,  90687,  27869,    568,\n",
      "           2182, 236792, 236768,    753,    623, 122850,   3271, 124468, 236775,\n",
      "           1018,    108, 122850,   3271, 124468,   3244,   7325,  38325,  14960,\n",
      "         140907,  90687,  27869,   7189,  55415]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n**Mô hình ngôn ngữ lớn (LLM) - \"LinguaVerse\"**\\n\\nLinguaVerse là một mô hình ngôn ngữ lớn được xây']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:02<00:02,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108,   1018,\n",
      "          26609,  92141, 237184,   1018,    108, 236829,    139,   1018,  86313,\n",
      "         236743,  92141,  53121,    138,   6932,   5938,  39738,  92474,  10363,\n",
      "          26609, 236900,  99193, 158343,  26595,   9824,  11471,  27099,  10363,\n",
      "         238017, 237501, 236924]], device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n**模型架构：**\\n\\n*   **Transformer 架构:**  我们使用基于 Transformer 的模型，特别是 BERT 和 RoBERTa 的变体。']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:03<00:01,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "           7243,    108,    818,   6573,  54850,    657,   1041,  13925,   5716,\n",
      "         236764,    496,   4512, 236764,  74693,  89830, 236761,   3551,  11762,\n",
      "           3952,    528, 134880,   4314,    975, 236764,    496,   2173, 108507,\n",
      "           2342,    506, 162542,  16045]], device='cuda:0')\n",
      "output: ['<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\n---\\n\\nThe wind bites at my exposed skin, a constant, icy whisper. My breath comes in ragged gasps, a white plume against the bruised purple']\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:03<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108, 236769, 237703, 236748,   7439,  44693,\n",
      "         100634, 236764,  22947,  31612,  13940,  31889,  89158,   3884, 127379,\n",
      "          63719, 236768,    108,   1390,    108,   1390,    108,   1390,    108,\n",
      "         236769, 217366,  37513,   6256,   8665,  38143,  67946,  11188]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\n(Đoạn văn ngắn, tập trung vào cảm xúc và khung cảnh)\\n\\n...\\n\\n...\\n\\n...\\n\\n(Hãy thêm các chi tiết cụ thể']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108, 237169,  31799,  17583,  12680, 237408, 151599,  53877, 107462,\n",
      "         237206, 216348, 236924,  14201, 237169,  33813,  12553, 236900,  31799,\n",
      "          85694,  65407,  72245, 237622,  43719, 236924,  13488, 236900, 183868,\n",
      "          21181,  22276,  19695, 236900,  92711]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n我无法直接提供你登山者的情绪和视角。因为我是一个AI，无法模拟人类的情感体验。但是，我可以根据你的要求，创作']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdcaa0a1-a78e-4c8b-9180-3c911fb44b56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[    1, 14350,   263,  3022,   895,  8252,   310,   920,   596,  1904,\n",
      "         10174,  1773,  6504,   950,  1881,   322,   920,   366,  9801, 13747,\n",
      "         11029,  4822,  1422, 10276, 29889]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:03<00:16,  3.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[    1, 14350,   263,  3022,   895,  8252,   310,   920,   596,  1904,\n",
      "         10174,  1773,  6504,   950,  1881,   322,   920,   366,  9801, 13747,\n",
      "         11029,  4822,  1422, 10276, 29889,    13,    13, 29871,  8125,   292,\n",
      "          1773,  6504,   950,  1426,   848,   508,   367, 18066,   292,  1363,\n",
      "           310,   278, 13644,   310,  4086, 12286, 29892,  7931,   370,   352,\n",
      "           653, 29892,   322, 16375,  4948,  2925, 29889,   512,   445,  2933,\n",
      "         29892,   306,   674,  5649,   920,   590,  1904, 10174,  1773,  6504,\n",
      "           950,  1881,   322,   920,   306,  9801, 13747, 11029,  4822,  1422,\n",
      "         10276, 29889,    13,    13,  1762,  1889,  1773,  6504,   950,  1881,\n",
      "         29892,   590,  1904,  3913,   263, 10296,   310,  5613,  4086,  9068,\n",
      "           313, 29940, 13208, 29897, 13698,   322,  4933,  6509, 14009, 29889,\n",
      "          2266,   526,   278,  1820,  6576,  9701,   297,  1749,  2948, 29901,\n",
      "            13,    13, 29896, 29889,  3992,  4721, 19170, 29901,  1334,  3394,\n",
      "           263,  3652,   310,   758, 19170, 13698,   304,  5941,   322,  4226,\n",
      "           675,   278,  1426,   848, 29892,  3704,  5993,  2133, 29892,  5040,\n",
      "          1742, 28744, 29892]], device='cuda:0')\n",
      "output: ['<s> Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n  Modeling multilingual text data can be challenging because of the complexity of language structures, vocabulary, and cultural nuances. In this response, I will explain how my model processes multilingual input and how I ensure consistent quality across different languages.\\n\\nTo process multilingual input, my model uses a combination of natural language processing (NLP) techniques and machine learning algorithms. Here are the key steps involved in our approach:\\n\\n1. Text Preprocessing: We apply a series of preprocessing techniques to clean and normalize the text data, including tokenization, stopword removal,']\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[    1,   379, 30033, 29891,  4005, 30643, 29875,   266, 18353,  8736,\n",
      "         31160, 29876,   330, 30975, 29876,   274, 18549,   286, 30069,   298,\n",
      "         30097, 29876, 29882,   274, 31556, 29874,   289, 30540, 29876,   921,\n",
      "           228,   190,   176,   301, 30052,   270, 31797,   619, 30529, 29884,\n",
      "         29871, 30128, 29874,  8736, 30069, 29876,  8736, 31797,   325, 30001,\n",
      "         18916, 29885,   266, 30717,   302, 30001, 29877, 29871, 30128, 31957,\n",
      "         29871, 30128, 30643, 29885,   289, 30643, 29877,   521, 31145, 29873,\n",
      "           301, 30416, 31645,   865, 29871, 30128, 30926,   865, 29871, 30128,\n",
      "         31343, 29884,  4005, 31797, 29874,   274, 25968,  8736, 30069, 29876,\n",
      "          8736, 31797,   413, 19990, 29883,   302, 29882,   585, 29889]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:06<00:12,  3.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[    1,   379, 30033, 29891,  4005, 30643, 29875,   266, 18353,  8736,\n",
      "         31160, 29876,   330, 30975, 29876,   274, 18549,   286, 30069,   298,\n",
      "         30097, 29876, 29882,   274, 31556, 29874,   289, 30540, 29876,   921,\n",
      "           228,   190,   176,   301, 30052,   270, 31797,   619, 30529, 29884,\n",
      "         29871, 30128, 29874,  8736, 30069, 29876,  8736, 31797,   325, 30001,\n",
      "         18916, 29885,   266, 30717,   302, 30001, 29877, 29871, 30128, 31957,\n",
      "         29871, 30128, 30643, 29885,   289, 30643, 29877,   521, 31145, 29873,\n",
      "           301, 30416, 31645,   865, 29871, 30128, 30926,   865, 29871, 30128,\n",
      "         31343, 29884,  4005, 31797, 29874,   274, 25968,  8736, 30069, 29876,\n",
      "          8736, 31797,   413, 19990, 29883,   302, 29882,   585, 29889,    13,\n",
      "            13, 29907, 13946,   368, 29991,  2266,   526,   777,  2498, 16650,\n",
      "           583,   363, 11415,  2473, 29899,  1847,   950,   848,   322,  5662,\n",
      "          3864,  5718,  3819,  4822,  1422, 10276, 29901, 29901,    13,    13,\n",
      "         29896, 29889,  4803,   263,  3918,  1891,  6624,  3002, 29901, 22402,\n",
      "           263,   731,   310,  3918,  1891,  4958,   322, 12216,  2129,   363,\n",
      "          1269,  4086, 29892,   322,  9801,   393,   599,  3815,  5144,   526,\n",
      "          9985,   411,   963, 29889,   910,   508,  1371,   304,  4772, 14679,\n",
      "           322, 22435,  8244,  2478,   297,   278,   848, 29889,    13, 29906,\n",
      "         29889,  4803,   263,  3619,   848,  1904, 29901, 22402,   263,  3619,\n",
      "           848,  1904,   363,  1269,  4086, 29892,   322,  9801,   393,   372,\n",
      "           338, 13747,  4822,   599, 10276, 29889,   910,   508,  1371,   304,\n",
      "          9801,   393,   278,   848,   338, 19098,   297,   263,   982,   393,\n",
      "           338,  4780,   304,  2274,   322,   664,   411]], device='cuda:0')\n",
      "output: ['<s> Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\nCertainly! Here are some general strategies for handling multi-lingual data and ensuring consistency across different languages::\\n\\n1. Use a standardized terminology: Define a set of standardized terms and phrases for each language, and ensure that all team members are familiar with them. This can help to avoid confusion and inconsistencies in the data.\\n2. Use a common data model: Define a common data model for each language, and ensure that it is consistent across all languages. This can help to ensure that the data is organized in a way that is easy to understand and work with']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[    1, 29871, 31088,   234,   177,   131, 30698, 31639, 30592, 30919,\n",
      "         30210, 31382, 30883, 30392, 30847, 31502, 31548, 30687, 30923, 31505,\n",
      "         31243, 31573, 30752, 30210, 30214, 30651, 31436, 30919, 30847, 31502,\n",
      "         31835, 30982, 30413, 30980, 31505, 31243, 30577, 31016, 30210, 31573,\n",
      "         30544,   235,   183,   171, 31180, 30982, 31695, 30287,   235,   138,\n",
      "           183, 30267]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:09<00:09,  3.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[    1, 29871, 31088,   234,   177,   131, 30698, 31639, 30592, 30919,\n",
      "         30210, 31382, 30883, 30392, 30847, 31502, 31548, 30687, 30923, 31505,\n",
      "         31243, 31573, 30752, 30210, 30214, 30651, 31436, 30919, 30847, 31502,\n",
      "         31835, 30982, 30413, 30980, 31505, 31243, 30577, 31016, 30210, 31573,\n",
      "         30544,   235,   183,   171, 31180, 30982, 31695, 30287,   235,   138,\n",
      "           183, 30267,    13,    13,  2887,   263,  4086,  1904, 29892,   306,\n",
      "           626, 16370,   373,   263,  2919,  1034, 13364,   310,  1426,   848,\n",
      "           393,  7805,  2999, 10276, 29889,  1932,  9068,  2473, 29899,  1847,\n",
      "           950,  1881, 29892,   306,   671,   263, 10296,   310, 13698,   304,\n",
      "          4386,  1269,  4086, 29915, 29879,  5412, 21862,   322,  9801, 13747,\n",
      "         11029,  1962,  4822, 10276, 29889,  2266,   526,   777,   310,   278,\n",
      "          5837,   306,  4386,  2473, 29899,  1847,   950,  1881, 29901,    13,\n",
      "            13, 29896, 29889, 17088, 29899, 14940,  4733, 29901,   306,   505,\n",
      "          5004,  4733,   363,  1269,  4086,   297,   278,  1881, 29892,   607,\n",
      "           526,  2691, 29899, 29873,   348,   287,   373,  1009, 18067,  4086,\n",
      "         17266, 29874, 29889,   910,  6511,   592,   304, 10446,   278,  5412,\n",
      "         15038,   322, 12286,   310,  1269,  4086, 29892,   427, 17961,   592,\n",
      "           304,  5706,  1880, 29899, 29567,  1962,   363,  1269,  4086, 29889]],\n",
      "       device='cuda:0')\n",
      "output: [\"<s> 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\nAs a language model, I am trained on a large corpus of text data that includes multiple languages. When processing multi-lingual input, I use a combination of techniques to handle each language's unique characteristics and ensure consistent quality output across languages. Here are some of the ways I handle multi-lingual input:\\n\\n1. Language-specific models: I have separate models for each language in the input, which are fine-tuned on their respective language corpora. This allows me to capture the unique patterns and structures of each language, enabling me to generate high-quality output for each language.\"]\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[    1,   887,   526,   263, 10784,   495,  1058,   756,  7450,   278,\n",
      "          2533,  2415,   310,  8040, 18274,   342, 29889, 20355,   915,   596,\n",
      "         23023,  1080,   322,   278,  1776,   366,  1074,   515,   278,  2246,\n",
      "         29889]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:12<00:06,  3.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[    1,   887,   526,   263, 10784,   495,  1058,   756,  7450,   278,\n",
      "          2533,  2415,   310,  8040, 18274,   342, 29889, 20355,   915,   596,\n",
      "         23023,  1080,   322,   278,  1776,   366,  1074,   515,   278,  2246,\n",
      "         29889,    13,    13, 29907,   549,  3605,  8250,   373,   596, 27012,\n",
      "           882, 29991, 29871,   243,   162,   145,   140,    13,    13,  1123,\n",
      "          9733,   278,  2533,  2415,   310,  8040, 18274,   342,   338,   263,\n",
      "          2748, 29899,   262, 29899, 29874, 29899, 29880,   361,  5410, 17623,\n",
      "           545,   393,  2846,  2305,   679,   304,  7271, 29889,  1094,   366,\n",
      "          2317,   472,   278, 17526,   310,   278,  3186, 29892,   366,   526,\n",
      "          1395,   300,   287,   411,  2078,   271,   400,  5086,  8386,   322,\n",
      "           385,   443,   862,  3498,   839,  4060,   310, 12709,   358, 29889,\n",
      "         29871,   243,   162,   143,   146,    13,    13,  2887,   366,  2125,\n",
      "           297,   278,  3256,   681,  1776, 29892,   366,   508, 29915, 29873,\n",
      "          1371,   541,  4459,   263,  6837,   310, 23023,  1080, 29901,    13,\n",
      "            13, 29896, 29889,   319,   705,   322,  4997, 29901,   450,  1183,\n",
      "           261,  6287,   310,   278,   379,  3039,   388,   273,  1236]],\n",
      "       device='cuda:0')\n",
      "output: [\"<s> You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\nCongratulations on your achievement! 🎉\\n\\nReaching the summit of Mount Everest is a once-in-a-lifetime adventure that few people get to experience. As you stand at the roof of the world, you are greeted with breathtaking views and an unparalleled sense of accomplishment. 🌏\\n\\nAs you take in the momentous view, you can't help but feel a mix of emotions:\\n\\n1. Awe and wonder: The sheer scale of the Himalayan pe\"]\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[    1,   350, 30540, 29876, 18916,   286, 30902, 29873,  8736, 30416,\n",
      "         30997, 29875,   454, 29877, 10442, 29875,   325,   228,   190,   174,\n",
      "         29874,   521,   262, 29882,  1374, 31620, 29883, 29871, 30128,   228,\n",
      "           190,   140, 29876, 29882, 18274,   342, 29889,   379, 30033, 29891,\n",
      "           286, 30069,   260, 30643,   274, 30643, 29885,   921, 30030, 29883,\n",
      "           274, 31556, 29874,   289, 30540, 29876,   325, 30001,   413, 18808,\n",
      "           274, 30643, 29876, 29882,   289, 30540, 29876,   302, 29882, 30097,\n",
      "         29876,   266, 31145, 29891,   260,   228,   190,   174,   534,  5512,\n",
      "          5777, 29877, 29889]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:15<00:03,  3.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[    1,   350, 30540, 29876, 18916,   286, 30902, 29873,  8736, 30416,\n",
      "         30997, 29875,   454, 29877, 10442, 29875,   325,   228,   190,   174,\n",
      "         29874,   521,   262, 29882,  1374, 31620, 29883, 29871, 30128,   228,\n",
      "           190,   140, 29876, 29882, 18274,   342, 29889,   379, 30033, 29891,\n",
      "           286, 30069,   260, 30643,   274, 30643, 29885,   921, 30030, 29883,\n",
      "           274, 31556, 29874,   289, 30540, 29876,   325, 30001,   413, 18808,\n",
      "           274, 30643, 29876, 29882,   289, 30540, 29876,   302, 29882, 30097,\n",
      "         29876,   266, 31145, 29891,   260,   228,   190,   174,   534,  5512,\n",
      "          5777, 29877, 29889,    13,    13, 29902,   626,   263, 10784,   495,\n",
      "          1058,   756,   925,  7450,   278,  2533,  2415,   310,  8040, 18274,\n",
      "           342, 29889,   306,   626,   975,  1332,   295,  2168,   411,   953,\n",
      "          8194,   322,   263,   705,   472,   278,  2078,   271,   400,  5086,\n",
      "          1776,   515,   701,  1244, 29889,   450, 15007, 29899, 29883, 17280,\n",
      "          1236, 10327,   310,   278,   379,  3039,   388,   294, 16116,   714,\n",
      "          1434,   592,   763,   263, 10067, 15931, 10508, 29892, 23139,   297,\n",
      "           528,  3076,   310,  7254,   322,  4796, 29889,   306,   508,  1074,\n",
      "           363,  7800,   322,  7800, 29892,   278, 28205,  3151,  1747,  3448,\n",
      "           964,   278,  5418,   763,   263, 13283, 18130,  6718, 29889,    13,\n",
      "            13,  2887,   306,  2317,   373,   278, 17526,   310,   278,  3186,\n",
      "         29892,   306,   626, 10423,   411,   263,  4060,   310, 12709,   358,\n",
      "           322,  4997, 29889,   450, 10784, 29890,   471,  4500, 14067,   322,\n",
      "         18215]], device='cuda:0')\n",
      "output: ['<s> Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\nI am a climber who has just reached the summit of Mount Everest. I am overwhelmed with emotion and awe at the breathtaking view from up here. The snow-capped peaks of the Himalayas stretch out before me like a majestic canvas, painted in shades of blue and white. I can see for miles and miles, the horizon curving away into the distance like a silver ribbon.\\n\\nAs I stand on the roof of the world, I am filled with a sense of accomplishment and wonder. The climb was grueling and dangerous']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[    1, 29871, 30919, 30392, 30287, 30956, 31451, 30429,   234,   146,\n",
      "           163,   234,   172,   137,   233,   159,   154,   234,   145,   158,\n",
      "           232,   182,   179,   236,   164,   185,   232,   182,   179, 30210,\n",
      "         31451, 30329, 30767, 30267,   233,   146,   146,   235,   194,   179,\n",
      "         30287, 30557, 30919, 30210, 30993,   234,   190,   173, 30214, 30651,\n",
      "         31436, 31594, 30528, 31548, 31811, 30780, 30210, 31495, 31085, 30267]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:18<00:00,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[    1, 29871, 30919, 30392, 30287, 30956, 31451, 30429,   234,   146,\n",
      "           163,   234,   172,   137,   233,   159,   154,   234,   145,   158,\n",
      "           232,   182,   179,   236,   164,   185,   232,   182,   179, 30210,\n",
      "         31451, 30329, 30767, 30267,   233,   146,   146,   235,   194,   179,\n",
      "         30287, 30557, 30919, 30210, 30993,   234,   190,   173, 30214, 30651,\n",
      "         31436, 31594, 30528, 31548, 31811, 30780, 30210, 31495, 31085, 30267,\n",
      "            13,    13,  2887,   306,  8389,   472,   278,  2533,  2415,   310,\n",
      "          8040, 18274,   342, 29892,   278,  9939, 19224,   373, 11563, 29892,\n",
      "           306,  8496, 29915, 29873,  1371,   541,  4459,   263,  6837,   310,\n",
      "         23023,  1080, 29889,  1222,  2350,   504,   291,   322, 12709,   358,\n",
      "           750,  1063,  8611,   491,   263,  6483,  4060,   310,   263,   705,\n",
      "           322,  4997,   472,   278,  1183,   261,  6287,   310,   278,   379,\n",
      "          3039,   388,   294, 16116,   287,   714,  1434,   592, 29889,    13,\n",
      "            13,  2887,   306, 12642,   287,   714,   472,   278,  2078,   271,\n",
      "           400,  5086,  7243,   272,  3304, 29892,   306,  7091, 21577,   322,\n",
      "          1663,   647,   928,   424,   297, 10230,   304,   278, 13426,  2264,\n",
      "           310,   278, 24400, 29889,   450, 15007, 29899, 29883, 17280, 19223,\n",
      "          6140,   304,  6023,   278, 14744, 29892,  1009,  1236, 10327,   330,\n",
      "          1761,  8333,   763, 11502, 13788,   297,   278,  4688]],\n",
      "       device='cuda:0')\n",
      "output: [\"<s> 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\nAs I stood at the summit of Mount Everest, the highest peak on Earth, I couldn't help but feel a mix of emotions. Exhaustion and accomplishment had been replaced by a deep sense of awe and wonder at the sheer scale of the Himalayas stretched out before me.\\n\\nAs I gazed out at the breathtaking panorama, I felt tiny and insignificant in comparison to the vastness of the landscape. The snow-capped mountains seemed to touch the sky, their peaks glistening like diamonds in the early\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model_en, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ea57361-5a42-4944-9dd9-59c7cd42db06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:03,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108,   7243,    107,   8291, 236789,\n",
      "         236751,    496,   3435,   3072, 236787,    108, 236775,   4754,   2028,\n",
      "           6585, 132567,   2744,    684,  64004,    496,   8376,    529,    870,\n",
      "         109219,   2307,   8403,   1133, 236787,   4071, 236772, 187918, 106703]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n---\\nHere\\'s a potential response:\\n\\n\"My model processes multilingual input by leveraging a combination of [mention key techniques like: cross-lingual embeddings']\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:01<00:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            108,   1018, 236792, 237124,  14960,  53121,    108, 172915,  27600,\n",
      "          55415,  54380,   7325,  38325,  14960, 140907,  90687,  27869,    568,\n",
      "           2182, 236792, 236768,   4032,  63560,  26571,  86340,  28211,  79808,\n",
      "          35112,  48078, 140907,  90687, 236761]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n**Mô hình:**\\n\\nTôi đang xây dựng một mô hình ngôn ngữ lớn (LLM) có khả năng xử lý dữ liệu đa ngôn ngữ.']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:02<00:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108,   1018,\n",
      "          26609,  26544,  53121,    870, 238350, 134451,  22276,  26609,  26544,\n",
      "         236900,  34313, 237184,  86313, 236764, 158343, 236764,   4044,  12120,\n",
      "            107,   1018,  26609,  92141,  53121,    870, 238350, 134451,  22276,\n",
      "          26609,  92141, 236900]], device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n**模型类型:** [请填写你的模型类型，例如：Transformer, BERT, etc.]\\n**模型架构:** [请填写你的模型架构，']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "            818,   6573,    563,    496,   4512, 236764, 214709,   1526, 236764,\n",
      "          34847,   3801,    657,   1041,  36326, 236764,    496,   7445, 236764,\n",
      "          14658,  32272,    529,    506,  37501,   2066,    529,    672,   1977,\n",
      "         236761,   3551,  11762,   3952]], device='cuda:0')\n",
      "output: ['<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\nThe wind is a constant, insistent hand, tugging at my harness, a cold, sharp reminder of the sheer power of this place. My breath comes']\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:03<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108,   7243,    108, 236780,  28416,  89158,\n",
      "           5042,  25969, 236787,    108, 236814,  29083,  10539,  27600,  70432,\n",
      "          13940,   7325,  21121,  32025,  23271, 236764,    107, 236773, 237644,\n",
      "            757, 204821,  29834,    719, 145515,  45222,  40515,  30758]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\n---\\n\\nCảm xúc của tôi:\\n\\nHóa như đang bước vào một thế giới mới,\\nSự im lặng bao trùm lấy mọi thứ']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108,   7243,    108, 237169,  79948, 239972, 241888, 240344, 241454,\n",
      "         239963, 239761, 239963, 236900, 239263, 237245, 237026, 239783, 206306,\n",
      "         240274, 238441, 236900, 238321, 239156, 239156, 237307, 240649, 237464,\n",
      "         236900,  57944, 239562, 239577, 236918]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n---\\n\\n我站在珠穆朗玛峰顶峰，脚下是冰冷的岩石，风呼呼地吹过，带来刺骨的']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model_vi, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2abdad55-e56e-455b-b03d-516298767c81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:03,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108, 174355, 236764,    506,   2744,\n",
      "            563,    496,   3996, 236764,   5074, 236772, 172287, 236764,    532,\n",
      "          74647,  13186, 236761,    107,   2717,    107,   2717,    107,   2717,\n",
      "            107,   2717,    107,   2717,    107,   2717,    107,   2717,    107]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\nEssentially, the input is a complex, multi-faceted, and interconnected entity.\\n```\\n```\\n```\\n```\\n```\\n```\\n```\\n']\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:01<00:02,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            107, 237703, 236916,   3244,   7325,  52898,  30170,    107, 237703,\n",
      "         236916,   3244,   7325,  52898,  30170,    107, 237703, 236916,   3244,\n",
      "           7325,  52898,  30170,    107, 237703, 236916,   3244,   7325,  52898,\n",
      "          30170,    107, 237703, 236916,   3244]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\nĐó là một vấn đề\\nĐó là một vấn đề\\nĐó là một vấn đề\\nĐó là một vấn đề\\nĐó là']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:02<00:02,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108,  26609,\n",
      "          42883,   9676,  19157,  33783,  42050,  55682, 237409, 237184,    107,\n",
      "          42883,   9676,  19157,  33783,  42050,  55682, 237409, 237184,  42883,\n",
      "           9676,  19157,  33783,  42050,  55682, 237409, 237184,  42883,   9676,\n",
      "          19157,  33783,  42050]], device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n模型训练数据处理质量保证一致性：\\n训练数据处理质量保证一致性：训练数据处理质量保证一致性：训练数据处理质量保证']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "           1018,    108,   1018,    108,   1018,    108,   1018,    108,   1018,\n",
      "            108,   1018,    108,   1018,    108,   1018,    108,   1018,    108,\n",
      "           1018,    108,   1018,    108,   1018,    108,   1018,    108,   1018,\n",
      "            108,   1018,    108,   1018]], device='cuda:0')\n",
      "output: ['<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**']\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:03<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108,  99972,  31889,  89158,   5042,  10044,\n",
      "           3884, 127379,  63719,  10044,  51414,  27888,  12835,  14780,  21587,\n",
      "         236761,    107, 237703,    581,  31889,  89158,   5042,  10044,   3884,\n",
      "         127379,  63719,  10044,  51414,  27888,  12835,  14780,  21587]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\nBạn cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\nĐem cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n",
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108,    106]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n<end_of_turn>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model_zh, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba65260-04ee-4caf-8d32-dac1061c07a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e800f-b8cb-4c06-9376-3391210efe1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82ac5334-e9bf-47de-a0d2-fb5cc415aebd",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# kur bottom1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b679ab91-3676-4e7d-8f78-e0d582047077",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_en = torch.load('/root/autodl-fs/LRP_kur_res/20251125/bottom1perc_LRP_kur_res_en.pt', weights_only=False)\n",
    "neuron_vi = torch.load('/root/autodl-fs/LRP_kur_res/20251125/bottom1perc_LRP_kur_res_vi.pt', weights_only=False)\n",
    "neuron_zh = torch.load('/root/autodl-fs/LRP_kur_res/20251125/bottom1perc_LRP_kur_res_zh.pt', weights_only=False)\n",
    "\n",
    "model_en = copy.deepcopy(model)\n",
    "model_en = set_neuron_zero(model_en, neuron_en)\n",
    "\n",
    "model_vi = copy.deepcopy(model)\n",
    "model_vi = set_neuron_zero(model_vi, neuron_vi)\n",
    "\n",
    "\n",
    "model_zh = copy.deepcopy(model)\n",
    "model_zh = set_neuron_zero(model_zh, neuron_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b908f823-407f-4427-b0b4-62b30e89233c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "model name: org_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 36.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.2627, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:32<00:00, 30.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(13.2703, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.5759, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_en_neuron_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 36.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.9548, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 30.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(13.2410, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.9520, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_vi_neuron_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:26<00:00, 37.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.3481, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:32<00:00, 30.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(12.3787, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.4910, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_zh_neuron_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 36.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.3114, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:32<00:00, 30.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(12.6794, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.3847, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "calc ppl\n",
    "\n",
    "'''\n",
    "\n",
    "# load test data\n",
    "test_en = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/en_radom_1000_test.jsonl')\n",
    "test_vi = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/vi_radom_1000_test.jsonl')\n",
    "test_zh = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/zh_radom_1000_test.jsonl')\n",
    "               \n",
    "# calc ppl\n",
    "model_list= [\n",
    "    (model, 'org_model'),\n",
    "    (model_en, 'mask_en_neuron_model'),\n",
    "    (model_vi, 'mask_vi_neuron_model'),\n",
    "    (model_zh, 'mask_zh_neuron_model'),\n",
    "]\n",
    "\n",
    "data_list = [\n",
    "    (test_en, 'en_data'),\n",
    "    (test_vi, 'vi_data'),\n",
    "    (test_zh, 'zh_data')\n",
    "    \n",
    "]\n",
    "\n",
    "for i_model, model_name in model_list:\n",
    "    print('*'*20)\n",
    "    print('*'*20)\n",
    "    print('model name:', model_name)\n",
    "\n",
    "    for i_data, data_name in data_list:\n",
    "        print('='*20)\n",
    "        print('data name:', data_name)\n",
    "        ppl_sum, ppl_count = calc_ppl(i_model, tokenizer, i_data, max_len= 2048)\n",
    "        print('ppl:',ppl_sum/ppl_count )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166eaae9-f446-4a94-95a2-983d82959531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb44f68-9487-479e-8feb-0e65396ef5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600c4a84-461c-4549-b3e2-e9da70b0f6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab02a196-93d2-4379-9160-f1d8a45fb7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c242e45-0e4c-4768-a610-fb3f1ddd4dbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:03,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108,   7243,    108,   1018,   4968,\n",
      "          17243,  53121,    108,   7711,   2028,    563,    496,  29193, 236772,\n",
      "           5140,   5192,   2028,  15453,    580,    496,  12566,  15297,    529,\n",
      "           1816,    532,   3393,    528,   5422, 236764,  14361, 236764,   7830]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n---\\n\\n**Model Description:**\\n\\nOur model is a transformer-based language model trained on a massive dataset of text and code in English, Spanish, French']\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:01<00:02,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            108,   1018, 236792, 237124,  14960, 140907,  90687,  27869,    568,\n",
      "           2182, 236792, 236768,    753,    623, 122850,   3271, 124468, 236775,\n",
      "           1018,    108, 122850,   3271, 124468,   3244,   7325,  38325,  14960,\n",
      "         140907,  90687,  27869,   7189,  55415]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n**Mô hình ngôn ngữ lớn (LLM) - \"LinguaVerse\"**\\n\\nLinguaVerse là một mô hình ngôn ngữ lớn được xây']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:02<00:02,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108,   1018,\n",
      "          26609,  92141, 237184,   1018,    108, 236829,    139,   1018,  86313,\n",
      "         236743,  92141,  53121,    138,   6932,   5938,  39738,  92474,  10363,\n",
      "          26609, 236900,  99193, 158343,  26595,   9824,  11471,  27099,  10363,\n",
      "         238017, 237501, 236924]], device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n**模型架构：**\\n\\n*   **Transformer 架构:**  我们使用基于 Transformer 的模型，特别是 BERT 和 RoBERTa 的变体。']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "           7243,    108,    818,   6573,  54850,    657,   1041,  13925,   5716,\n",
      "         236764,    496,   4512, 236764,  74693,  89830, 236761,   3551,  11762,\n",
      "           3952,    528, 134880,   4314,    975, 236764,    496,   2173, 108507,\n",
      "           2342,    506, 162542,  16045]], device='cuda:0')\n",
      "output: ['<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\n---\\n\\nThe wind bites at my exposed skin, a constant, icy whisper. My breath comes in ragged gasps, a white plume against the bruised purple']\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:03<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108, 236769, 237703, 236748,   7439,  44693,\n",
      "         100634, 236764,  22947,  31612,  13940,  31889,  89158,   3884, 127379,\n",
      "          63719, 236768,    108,   1390,    108,   1390,    108,   1390,    108,\n",
      "         236769, 217366,  37513,   6256,   8665,  38143,  67946,  11188]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\n(Đoạn văn ngắn, tập trung vào cảm xúc và khung cảnh)\\n\\n...\\n\\n...\\n\\n...\\n\\n(Hãy thêm các chi tiết cụ thể']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108, 237169,  31799,  17583,  12680, 237408, 151599,  53877, 107462,\n",
      "         237206, 216348, 236924,  14201, 237169,  33813,  12553, 236900,  31799,\n",
      "          85694,  65407,  72245, 237622,  43719, 236924,  13488, 236900, 183868,\n",
      "          21181,  22276,  19695, 236900,  92711]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n我无法直接提供你登山者的情绪和视角。因为我是一个AI，无法模拟人类的情感体验。但是，我可以根据你的要求，创作']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a05c1184-9519-4119-b62d-ac96fec56af5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:03,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108,   7243,    108,   1018,   4968,\n",
      "          39244,    833,  19779,  76242,   1018,    108,   7711,   2028,   6585,\n",
      "         132567,   2744,    684,   1171,  41110,    496, 132567,  43703,   6352,\n",
      "            531,   2754,   1546,   3658,    528,    506,   2744,   3418,   1061]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n---\\n\\n**Model Processing & Quality Assurance**\\n\\nOur model processes multilingual input by first employing a multilingual embedding layer to represent each word in the input across its']\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:01<00:02,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            108,   1018, 237703,  10582,  37020,  13940,  38325,  14960, 140907,\n",
      "          90687,  27869,    568,   2182, 236792, 236768,    753, 131233,  35909,\n",
      "         100634,  96901,   1018,    108, 236792, 237124,  14960, 140907,  90687,\n",
      "          27869,    568,   2182, 236792, 236768]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n**Đầu tư vào mô hình ngôn ngữ lớn (LLM) - Giải thích ngắn gọn**\\n\\nMô hình ngôn ngữ lớn (LLM)']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:02<00:02,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108,  26609,\n",
      "          19157, 237309,  37557, 180828,  42033,  92870, 237184,    108, 236770,\n",
      "         236761,    138,   1018, 238794,  19157, 237184,   1018,    138, 237392,\n",
      "          26878,  57489,  10937, 123700, 236951, 237237, 239275, 236951, 239275,\n",
      "         237409, 201281, 237520]], device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n模型处理多语言输入的关键在于：\\n\\n1.  **预处理：**  对输入文本进行清洗、分词、词性标注等']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "           7243,    108, 236769,    818,   8317,  19015,    607,    496,   5777,\n",
      "           5719,    529,    506, 113768, 236764,   3648,  10790,  77558,   2342,\n",
      "            496,  55704, 236764,   7613, 236772,  26987,  11519, 236761,    669,\n",
      "           6573,    563, 178291, 236764]], device='cuda:0')\n",
      "output: ['<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\n---\\n\\n(The scene opens with a wide shot of the climber, silhouetted against a breathtaking, snow-covered peak. The wind is howling,']\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:03<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108,   7243,    108, 236769, 236792, 237124,\n",
      "         110624,  31889,  89158,   5042,  10279,  70669, 121230,  11638, 232727,\n",
      "          57703, 114589,  94509, 236768,    108, 172915,  27600,  84655,  14780,\n",
      "         114589,  94509, 236761,    587,  19084,    637,  11992,    549]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\n---\\n\\n(Mô tả cảm xúc của người leo núi khi chinh phục đỉnh Everest)\\n\\nTôi đang đứng trên đỉnh Everest. Mưa rào g']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108, 237169,  79948, 239972, 241888, 240344, 241454, 239963, 239761,\n",
      "         237152, 236900, 238321, 239156, 244176, 237576, 236900, 237945,  32700,\n",
      "         238279, 238384,  26122, 236900, 240649, 244761, 237576,  21480, 109231,\n",
      "         236924, 237169,  67940,  36027,  85629]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n我站在珠穆朗玛峰顶上，风呼啸着，像是在低语一样，吹拂着我的头发。我感到一种难以']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model_en, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ae5d512-058b-45f5-957b-cc4ba0619efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:03,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108,   1018,  44008,  53121,    108,\n",
      "           7711,   2028,   6585, 132567,   2744,    684,   1171,  21558,    506,\n",
      "           5192,    529,    506,   2744, 236761,    138,  11407, 236764,    625,\n",
      "          16819,    496,   7501,    529,   5192, 236772,  15396,  29913,    531]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n**Explanation:**\\n\\nOur model processes multilingual input by first identifying the language of the input.  Then, it applies a sequence of language-specific transformations to']\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:01<00:02,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            108,   1018, 236792, 237124,  14960,  53121, 204932,  14960, 142275,\n",
      "          14780,   6256,  49921,  49110,  15196,  29011, 236764,  21994,  16981,\n",
      "           6256,  38325,  14960, 140907,  90687,  27869,    568,   2182, 236792,\n",
      "         236768,  11408,  86340,  28211,  44693]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n**Mô hình:** Mô hình dựa trên các thuật toán học máy, sử dụng các mô hình ngôn ngữ lớn (LLM) để xử lý văn']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:02<00:02,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108,   1018,\n",
      "          26609,  47710, 237184,   1018,    108, 236829,    139,   1018,  26609,\n",
      "          92141,  53121,  92474,    107, 236829,    139,   1018,  42883,   9676,\n",
      "          53121, 236743,  55088,  58868,   9676,    107, 236829,    139,   1018,\n",
      "          42883,  10573,  53121]], device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n**模型描述：**\\n\\n*   **模型架构:** Transformer\\n*   **训练数据:** 英文新闻数据\\n*   **训练方法:**']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "           7243,    108,   1018,    818,   5671,    529,    672,   8774,    563,\n",
      "            531,   3050,    506,  13690,   4100,    529,    496,   2872,  99382,\n",
      "            108,   1018,  14106,  53121,    138, 236776,  56677, 150235,    497,\n",
      "         236764,   7489, 205520,   5561]], device='cuda:0')\n",
      "output: ['<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\n---\\n\\n**The goal of this exercise is to understand the emotional impact of a character.**\\n\\n**Character:**  A seasoned mountaineer, named Silas Black']\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:03<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108,   1018, 236792, 237124, 110624,  14687,\n",
      "          31889,  89158,   5042,  10044,  53121,    108, 236829,    139,   1018,\n",
      "         236780,  28416,  89158,  53121, 141501,  31889,  27888,   7325,  18457,\n",
      "         183088,   3310,  10311,  41245, 155423,    518,   7473, 236764]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\n**Mô tả về cảm xúc của bạn:**\\n\\n*   **Cảm xúc:** Tôi cảm thấy một sự phấn khích vô bờ bến,']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108,   1018,  13590,  13850, 237184, 107462,  47710,   1018,    108,\n",
      "         238350,  47710, 237408, 129807, 236918,  65689, 236900,  21187, 237184,\n",
      "            108, 236829,    139,   1018, 107462, 236918, 113799,  65689, 237184,\n",
      "           1018, 236743, 148047, 236951, 240517]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n**第一部分：情绪描述**\\n\\n请描述你此刻的感受，包括：\\n\\n*   **情绪的总体感受：** 兴奋、悲']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model_vi, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe486b56-a966-4172-b731-649c17e7a68a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:03,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108,   7243,    108,   8291, 236789,\n",
      "         236751,    496,   2591,    529,   1217,    611,    740,   3890,    672,\n",
      "           2934, 236764,    840,    611,   1202,    531,   4903,    822,   1852,\n",
      "         236761,    108,   1018,  12703,  53121,    623, 236777,    740,  17866]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n---\\n\\nHere\\'s a example of how you can answer this question, but you need to write your own.\\n\\n**Example:** \"I can translate']\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:01<00:02,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            108,   1018, 149911,  14435,  53121,    108, 236829,    139,   1018,\n",
      "          29621,  13003,  90687, 236743, 236770,  53121,    138, 236775, 172915,\n",
      "          23216,  26664,  24231,  12835,    566, 137545,   1781,    107, 236829,\n",
      "            139,   1018,  29621,  13003,  90687]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n**Ví dụ:**\\n\\n*   **Ngôn ngữ 1:**  \"Tôi cần phân tích từ vựng.\"\\n*   **Ngôn ngữ']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:02<00:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108,  18518,\n",
      "         236900,  21480,  26609,  19157, 237309,  37557, 180828, 237298, 237671,\n",
      "         237026, 236900,  15010,  26878, 237298, 236900, 237169,  33268, 237279,\n",
      "          43120,  26878,  57489, 236918,  59738, 236900,  99193,  62779,  26878,\n",
      "         236900,   9522,  59738]], device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n好的，我的模型处理多语言输入的时机是，用户输入时，我首先会检查输入文本的长度，特别是第一个输入，如果长度']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "         236769, 236777, 236789,    560,   1010,  29967,    573,   5695, 236764,\n",
      "            532,    564, 236789, 236757,   6250,    531,   2597,   1390,  46002,\n",
      "           2907,    108,   1509, 236789, 236751,    496,  17163,   8178, 236764,\n",
      "           5889, 236789, 236745,    625]], device='cuda:0')\n",
      "output: [\"<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\n(I've been climbing for weeks, and I'm starting to feel... overwhelmed.)\\n\\nIt's a strange feeling, isn't it\"]\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:03<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108,   7243,    108,   1018, 236780,  28416,\n",
      "          89158,  53121,    108, 172915,  27600,  84655,  14780, 114589,  94509,\n",
      "         236764,   7325,  18457,    757, 204821,  38042,   3375,  37814,  29834,\n",
      "            719, 145515, 236761,  85654,   4032,  42122,  14759,   1123]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\n---\\n\\n**Cảm xúc:**\\n\\nTôi đang đứng trên đỉnh Everest, một sự im lặng kinh ngạc bao trùm. Không có tiếng lá x']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108,  21480, 107462, 236900, 237026, 239783, 206306, 236900, 237945,\n",
      "         155059, 237759, 240379, 246637, 236918, 191885, 236924, 237169,  66249,\n",
      "         237075, 237845, 237652, 236900, 237075, 237383, 238330, 238097, 236900,\n",
      "         237169,  66249,  67940, 182705, 236900]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n我的情绪，是冰冷的，像一块被磨砺的石头。我曾经在山里，在最深处，我曾经感到孤独，']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model_zh, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62257714-82ae-422f-b312-e950b98bd1a0",
   "metadata": {},
   "source": [
    "## kur gap rate top 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d398142-2296-47c7-9ff5-37cfd7c5671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_en = torch.load('/root/autodl-fs/LRP_kur_res/20251125/gaprate_top1perc_LRP_kur_res_en.pt', weights_only=False)\n",
    "neuron_vi = torch.load('/root/autodl-fs/LRP_kur_res/20251125/gaprate_top1perc_LRP_kur_res_vi.pt', weights_only=False)\n",
    "neuron_zh = torch.load('/root/autodl-fs/LRP_kur_res/20251125/gaprate_top1perc_LRP_kur_res_zh.pt', weights_only=False)\n",
    "\n",
    "model_en = copy.deepcopy(model)\n",
    "model_en = set_neuron_zero(model_en, neuron_en)\n",
    "\n",
    "model_vi = copy.deepcopy(model)\n",
    "model_vi = set_neuron_zero(model_vi, neuron_vi)\n",
    "\n",
    "\n",
    "model_zh = copy.deepcopy(model)\n",
    "model_zh = set_neuron_zero(model_zh, neuron_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93f4dc00-5487-4cc6-a394-1860dc1b356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n"
     ]
    }
   ],
   "source": [
    "min_num = -100\n",
    "for name, param in model.named_parameters():\n",
    "        # 检查参数是否有梯度\n",
    "        #print(name,  param.shape)\n",
    "        cur_min = param.max().item()\n",
    "\n",
    "        min_num= max(min_num, cur_min)\n",
    "print(min_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e16bc5db-b4a2-4867-a264-c368a68138e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "model name: org_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 36.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.2627, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 30.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(13.2703, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.5759, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_en_neuron_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 36.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.7718, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 30.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.1426, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(9.6979, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_vi_neuron_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 36.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.4110, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 30.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.4656, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(9.7227, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_zh_neuron_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:26<00:00, 37.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.1620, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 30.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.0114, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(9.4460, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "calc ppl\n",
    "\n",
    "'''\n",
    "\n",
    "# load test data\n",
    "test_en = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/en_radom_1000_test.jsonl')\n",
    "test_vi = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/vi_radom_1000_test.jsonl')\n",
    "test_zh = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/zh_radom_1000_test.jsonl')\n",
    "               \n",
    "# calc ppl\n",
    "model_list= [\n",
    "    (model, 'org_model'),\n",
    "    (model_en, 'mask_en_neuron_model'),\n",
    "    (model_vi, 'mask_vi_neuron_model'),\n",
    "    (model_zh, 'mask_zh_neuron_model'),\n",
    "]\n",
    "\n",
    "data_list = [\n",
    "    (test_en, 'en_data'),\n",
    "    (test_vi, 'vi_data'),\n",
    "    (test_zh, 'zh_data')\n",
    "    \n",
    "]\n",
    "\n",
    "for i_model, model_name in model_list:\n",
    "    print('*'*20)\n",
    "    print('*'*20)\n",
    "    print('model name:', model_name)\n",
    "\n",
    "    for i_data, data_name in data_list:\n",
    "        print('='*20)\n",
    "        print('data name:', data_name)\n",
    "        ppl_sum, ppl_count = calc_ppl(i_model, tokenizer, i_data, max_len= 2048)\n",
    "        print('ppl:',ppl_sum/ppl_count )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d970d8-9694-4898-b7aa-4d6e3bb9525a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088056de-6846-415d-ae9a-8bf058328319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fd506ff3-396f-4767-872e-059c77057f37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:03,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108,  19058, 236764,   1590, 236789,\n",
      "         236751,    496,  12262,   3072,    600,    611,    740,   1161,    618,\n",
      "            496,   6250,   1523, 236761,    138,   1509, 236789, 236751,   5402,\n",
      "            531,    577,   3582, 236764,  63510, 236764,    532,   3421,    506]],\n",
      "       device='cuda:0')\n",
      "output: [\"<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\nOkay, here's a draft response that you can use as a starting point.  It's designed to be clear, concise, and address the\"]\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:01<00:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            108,   1018, 237703, 237449,  57694,  13428,   6256,  15289,  67946,\n",
      "           3884,  69406,  49921,   1018,    108,   1018, 237703,  19084,   1459,\n",
      "           7325,  13960,  14435,   1018,    108,   1018,  71251,  61386,  44731,\n",
      "          53121,    108,   1018, 236770, 236761]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n**Đề cập đến các công cụ và kỹ thuật**\\n\\n**Đưa ra một ví dụ**\\n\\n**Câu trả lời:**\\n\\n**1.']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:02<00:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108,   1018,\n",
      "          26609,  47710,  53121,    108,  21480,  26609,  33813,  39738,  86313,\n",
      "         236918,  37557,  26609, 236900,   5157,  19157,  79403,  37557, 236924,\n",
      "         238010,  12982,  19697,  16772,  19157, 237309,  37557,  26878, 237184,\n",
      "            108, 236770, 236761]], device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n**模型描述:**\\n\\n我的模型是一个基于Transformer的语言模型，可以处理多种语言。它通过以下方式处理多语言输入：\\n\\n1.']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "           7243,    108,    818,   6573,   1217,    991,    496,  92377,   1275,\n",
      "           5410,   2101,    786, 236764,    496,   4512, 236764,  74693,  89830,\n",
      "            600,  13804,    529,    506,  36628,   2066,    529,    672,  10565,\n",
      "         236761,   3551,  38464,  30494]], device='cuda:0')\n",
      "output: ['<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\n---\\n\\nThe wind howled a mournful song around me, a constant, icy whisper that spoke of the immense power of this mountain. My lungs burned']\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:03<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108,   7243,    108, 172915,  12105, 232727,\n",
      "          57703, 114589,  94509, 236761,    108, 236780,  28416,  89158,   5042,\n",
      "          25969,  42645,  16420,   3244,   7325,  18457,   3375,  37814,  30166,\n",
      "         236761,  85654,  11188,  13657,  35051,  13892,  12105, 120535]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\n---\\n\\nTôi đã chinh phục đỉnh Everest.\\n\\nCảm xúc của tôi lúc đầu là một sự ngạc nhiên. Không thể tin rằng mình đã vượt']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108,  22276, 107462, 237184,    108, 236829,    139,   1018, 148047,\n",
      "          53121, 236743, 242958, 238603, 239972, 241888, 240344, 241454, 239963,\n",
      "         236918,  63417, 236900, 237026,  78747,  67940, 143174, 236918, 148047,\n",
      "         236900, 111000, 237169,  34775,  63417]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n你的情绪：\\n\\n*   **兴奋:** 攀登珠穆朗玛峰的经历，是让我感到无比的兴奋，仿佛我正在经历']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model_en, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "962356b5-8399-40b3-9d05-dbad29c4c9db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:03,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108,   1018,   7925,  53121,    108,\n",
      "           7711,   2028,   6585, 132567,   2744,    684,   1171,  35322,    625,\n",
      "            531,    496,   7659,  10065,   2557,   1699,    496, 132567,  43703,\n",
      "           6352, 236761,   1174,   6352,  53228,    531,   4187,   4171,   3418]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n**Answer:**\\n\\nOur model processes multilingual input by first converting it to a shared representation space using a multilingual embedding layer. This layer learns to map words across']\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:01<00:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            108,   1018,  71251,  47421,  53121,    107,  68850,  35187,  20421,\n",
      "          28668,  41926,   5042,  38325,  14960, 236764,  10044,  12105,  58805,\n",
      "          18470,  11388, 207204,  23103,  23336,   3884,  10044,  12105,  28206,\n",
      "          54292,  16427,  10539,  21121,  22336]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n**Câu hỏi:**\\nTrong quá trình phát triển của mô hình, bạn đã gặp phải những thách thức gì và bạn đã giải quyết chúng như thế nào']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:02<00:02,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108,   1018,\n",
      "          26609,  19157, 237309,  37557,  26878, 237184,   1018,    108,  21480,\n",
      "          26609,  22194,  19157,  79403,  37557, 236918,  26878, 236900,  21187,\n",
      "          55041, 236951,  55088, 236951, 138608, 238384, 236951, 238624, 238384,\n",
      "         236951, 118699, 236951]], device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n**模型处理多语言输入：**\\n\\n我的模型能够处理多种语言的输入，包括中文、英文、西班牙语、德语、法语、']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "            818,   6573,    563,    496,  40097, 236764,  85278,   4912, 236764,\n",
      "         148443,   1041,   3392,    532, 188312,   1041,   6114, 236761,    669,\n",
      "           7613,    563,    496, 186350,   2173, 236764,    496,   4512, 236764,\n",
      "          34064,  10092, 236761,    564]], device='cuda:0')\n",
      "output: ['<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\nThe wind is a brutal, relentless force, whipping my face and stinging my eyes. The snow is a blinding white, a constant, shifting landscape. I']\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:03<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108,   7243,    108,  99972,   3244,   7325,\n",
      "          10279,  70669, 121230,  50971, 232727,  57703, 114589,  94509, 236761,\n",
      "          88170,  38325, 110624,  31889,  89158,   5042,  10044,   3884, 127379,\n",
      "          63719,  10044,  51414,  27888,  12835,  14780,  21587, 236761]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\n---\\n\\nBạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108, 237169,  16884, 237606, 151599, 237457, 236900, 237169, 223874,\n",
      "         239972, 241888, 240344, 241454, 239963, 239761, 239963, 236924,    108,\n",
      "         237169,  79948, 237845, 239761, 236900, 237169,  67940,  36027,  85629,\n",
      "         237748, 237906, 236918, 236951,  62637]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n我是一位登山者，我登上珠穆朗玛峰顶峰。\\n\\n我站在山顶，我感到一种难以置言的、令人']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model_vi, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd7ff12b-4b27-4602-88dc-c3c0f978c543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:03,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108,   8291, 236789, 236751,    496,\n",
      "           5498,   3072, 236787,    108,   1018,   4968,  39244,  53121,    108,\n",
      "           4754,   2028,   6585, 132567,   2744,    684,   1171,  21558,    506,\n",
      "           5192,    529,   1546,   8369,   1699,    496,   5192,   2028, 236761]],\n",
      "       device='cuda:0')\n",
      "output: [\"<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\nHere's a sample response:\\n\\n**Model Processing:**\\n\\nMy model processes multilingual input by first identifying the language of each token using a language model.\"]\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:01<00:02,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            108,   7243,    107,    107,  99972,    636,    538,    496,    538,\n",
      "            512,    566,    496,    538,    534,    512,    510,    534,    512,\n",
      "            538,    494,    534,    512,    510,    534,    496,    538,    494,\n",
      "            534,    496,    538,    494,    534]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n---\\n\\nBạn se n a n o v a n h o p h o n t h o p h a n t h a n t h']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:02<00:02,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108,  10180,\n",
      "          26609, 237026,  39738,  70126,  23486, 236900,   5938,   5095,  56762,\n",
      "          37557,  26609, 236900, 237853,   5938,   5095, 239275, 240433,  44525,\n",
      "         236924,    108, 238350,  12680,  18303,  31580,  19809,  54003,  26609,\n",
      "         236900, 237162,  32690]], device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n这个模型是基于深度学习，使用一个大型语言模型，并使用一个词汇数据库。\\n\\n请提供一些关于如何优化模型，以提高']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "            818,   1657,    529,  17154,    506,  34976,    563,    496,  16438,\n",
      "         236764,    496,  85278,  17085,   2342,    506,  76789,   7445, 236764,\n",
      "            506,  11202,   2634, 236764,    532,    506,  37501,   3825,    529,\n",
      "            506,   3294, 236761,   2024]], device='cuda:0')\n",
      "output: ['<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\nThe process of reaching the summit is a struggle, a relentless climb against the biting cold, the thin air, and the sheer weight of the pack. But']\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:03<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108,   7243,    107,    107,   1018, 236780,\n",
      "          28416,  89158,   5042,   7325,  10279,  70669, 121230,   1018,    108,\n",
      "         236792,  19084,  10539,    719,  19834, 236764, 151243,  92108,  10539,\n",
      "          56246, 236761,    107, 172915,  27600,  84655,  14780, 114589]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\n---\\n\\n**Cảm xúc của một người leo núi**\\n\\nMưa như trăng, gió lạnh như dao.\\nTôi đang đứng trên đỉnh']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108, 237169,  10042, 237169, 237282, 237169, 237648, 236900,    107,\n",
      "         237169,  10042,   5095,    107, 236829,    107,    138,    107,    138,\n",
      "            107,    138,    107,    138,    107,    138,    107,    138,    107,\n",
      "            138,    107,    138,    107,    138]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n我需要我来我去，\\n我需要一个\\n*\\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  \\n  ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model_zh, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d73882-29be-4daf-9f8d-4d215e3637dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d6599-0c21-4837-b77a-6fe737ef4244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab042bed-1a69-4ad7-b996-967ae995d8ec",
   "metadata": {},
   "source": [
    "## kur gap rate bottom 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20ce1461-9254-4747-926d-e7d61049c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_en = torch.load('/root/autodl-fs/LRP_kur_res/20251125/gaprate_bottom1perc_LRP_kur_res_en.pt', weights_only=False)\n",
    "neuron_vi = torch.load('/root/autodl-fs/LRP_kur_res/20251125/gaprate_bottom1perc_LRP_kur_res_vi.pt', weights_only=False)\n",
    "neuron_zh = torch.load('/root/autodl-fs/LRP_kur_res/20251125/gaprate_bottom1perc_LRP_kur_res_zh.pt', weights_only=False)\n",
    "\n",
    "model_en = copy.deepcopy(model)\n",
    "model_en = set_neuron_zero(model_en, neuron_en)\n",
    "\n",
    "model_vi = copy.deepcopy(model)\n",
    "model_vi = set_neuron_zero(model_vi, neuron_vi)\n",
    "\n",
    "\n",
    "model_zh = copy.deepcopy(model)\n",
    "model_zh = set_neuron_zero(model_zh, neuron_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a816985-04d6-4835-9f81-fd0f80ccdd3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "model name: org_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.2627, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 30.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(13.2703, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.5759, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_en_neuron_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 36.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(18.4616, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 30.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(17.1368, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(21.3397, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_vi_neuron_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 36.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(24.0838, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 30.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(24.9086, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(23.7898, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_zh_neuron_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:26<00:00, 37.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(21.9303, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:32<00:00, 30.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(25.0321, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(24.3280, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "calc ppl\n",
    "\n",
    "'''\n",
    "\n",
    "# load test data\n",
    "test_en = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/en_radom_1000_test.jsonl')\n",
    "test_vi = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/vi_radom_1000_test.jsonl')\n",
    "test_zh = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/zh_radom_1000_test.jsonl')\n",
    "               \n",
    "# calc ppl\n",
    "model_list= [\n",
    "    (model, 'org_model'),\n",
    "    (model_en, 'mask_en_neuron_model'),\n",
    "    (model_vi, 'mask_vi_neuron_model'),\n",
    "    (model_zh, 'mask_zh_neuron_model'),\n",
    "]\n",
    "\n",
    "data_list = [\n",
    "    (test_en, 'en_data'),\n",
    "    (test_vi, 'vi_data'),\n",
    "    (test_zh, 'zh_data')\n",
    "    \n",
    "]\n",
    "\n",
    "for i_model, model_name in model_list:\n",
    "    print('*'*20)\n",
    "    print('*'*20)\n",
    "    print('model name:', model_name)\n",
    "\n",
    "    for i_data, data_name in data_list:\n",
    "        print('='*20)\n",
    "        print('data name:', data_name)\n",
    "        ppl_sum, ppl_count = calc_ppl(i_model, tokenizer, i_data, max_len= 2048)\n",
    "        print('ppl:',ppl_sum/ppl_count )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7994f868-8a43-4344-8076-ae5cf61e412d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:02<00:13,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108,   7243,    108,   1018,   4968,\n",
      "          17243,  53121,    108,   7711,   2028,    563,    496,  29193, 236772,\n",
      "           5140,   5192,   2028,  15453,    580,    496,  12566,  15297,    529,\n",
      "           1816,    532,   3393,    528,   5422, 236764,  14361, 236764,   7830,\n",
      "         236764,    532,   9115, 236761,   1030,  60432,    496,   5074, 236772,\n",
      "         187918,  43703,   6352,    531,   2754,   1546,   5192,    618,    496,\n",
      "           3550,    528,    496,   7659,   2557, 236761,    138,  14521,  34711,\n",
      "         236764,    506,   2028,  52298,    506,   2148,   8369,    528,    506,\n",
      "           7501, 236764,  11337,    506,   4403,    529,    506,   4251,   2744,\n",
      "         236764,   2440,    506,   3738,   5192, 236761,    138,   1882,   2736,\n",
      "            496,  26446,   5700,  10241,    600,   5862,    506,   2028,    531,\n",
      "          19281,    506,   8020,    529,   1607,  15579,   1056,   8487,    496,\n",
      "           2238,   2744, 236761,    138,  41781, 236764,    692,  27968,    496,\n",
      "           5192, 236772,  15396,  53255,   7545, 236764,  39919,    506,   3938,\n",
      "           6518,   2721,    580,    506,  13207,   5192]], device='cuda:0')\n",
      "output: ['<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n---\\n\\n**Model Description:**\\n\\nOur model is a transformer-based language model trained on a massive dataset of text and code in English, Spanish, French, and German. It utilizes a multi-lingual embedding layer to represent each language as a vector in a shared space.  During inference, the model predicts the next token in the sequence, considering the context of the entire input, including the source language.  We employ a sophisticated attention mechanism that allows the model to weigh the importance of different languages when processing a given input.  Furthermore, we incorporate a language-specific decoding strategy, adjusting the output format based on the detected language']\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:05<00:11,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            108,   1018, 236792, 237124,  14960, 140907,  90687,  27869,    568,\n",
      "           2182, 236792, 236768,    753,    623, 122850,   3271, 124468, 236775,\n",
      "           1018,    108, 122850,   3271, 124468,   3244,   7325,  38325,  14960,\n",
      "         140907,  90687,  27869,   7189,  55415,  54380, 142275,  14780,  50793,\n",
      "         124290,  92474, 236761,  91509,   7189, 200755, 100370,  14780,   7325,\n",
      "          21682,  27869,  79808,  35112,  44693,  28035,  48078, 140907,  90687,\n",
      "         236764,  29834,  43790,  42292, 236764,  48415, 236764,  29956,   4108,\n",
      "         236764,  31089,  62805,   3884,   6256,  62805,  22582, 236761,    138,\n",
      "         122850,   3271, 124468,  21994,  16981,   7325,  28749,  37944,    623,\n",
      "            791,   5419,  22027, 140907,  90687, 236775,    568,  19859, 236772,\n",
      "          22426,  34684, 236768,  11408,  86340,  28211,  79808,  35112,  48078,\n",
      "         140907,  90687, 236761,    108,   1018, 135709,  86340,  28211,  79808,\n",
      "          35112,  48078, 140907,  90687,  53121,    108, 236770, 236761,   5213,\n",
      "           3586,   5419,  22027, 140907,  90687,  53121,    138, 199259,  11638,\n",
      "         200755, 100370]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n**Mô hình ngôn ngữ lớn (LLM) - \"LinguaVerse\"**\\n\\nLinguaVerse là một mô hình ngôn ngữ lớn được xây dựng dựa trên kiến trúc Transformer. Nó được huấn luyện trên một lượng lớn dữ liệu văn bản đa ngôn ngữ, bao gồm sách, báo, trang web, mã nguồn và các nguồn khác.  LinguaVerse sử dụng một hệ thống \"phân chia ngôn ngữ\" (language-aware splitting) để xử lý dữ liệu đa ngôn ngữ.\\n\\n**Cách xử lý dữ liệu đa ngôn ngữ:**\\n\\n1. **Phân chia ngôn ngữ:**  Trước khi huấn luyện']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:08<00:08,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108,   1018,\n",
      "          26609,  92141, 237184,   1018,    108, 236829,    139,   1018,  86313,\n",
      "         236743,  92141,  53121,    138,   6932,   5938,  39738,  92474,  10363,\n",
      "          26609, 236900,  99193, 158343,  26595,   9824,  11471,  27099,  10363,\n",
      "         238017, 237501, 236924,    107, 236829,    139,   1018, 238794,  42883,\n",
      "          53121, 228546, 237075,  41440, 122462, 238384, 237616, 238956, 237221,\n",
      "          34313, 236900,  19892,  42761, 236752, 236764,  24612, 236764,  20980,\n",
      "           9521,  21553, 237214, 237152, 238794,  42883, 236924,    107, 236829,\n",
      "            139,   1018, 238477, 238250,  53121,    138, 237075,  60009,  34769,\n",
      "         237221,  34313, 236900,  51231,  90194, 236951, 237889, 238843, 237214,\n",
      "         237152,  10937, 238477, 238250, 236924,    108,   1018, 237309,  37557,\n",
      "          19157,  59322, 237184,   1018,    108, 236770, 236761,    138,   1018,\n",
      "          37557,  45010,  53121,    138, 237075,  26878,  57489, 237103,  10937,\n",
      "          37557,  45010, 236900,  77100, 237191,  26878,  57489, 236918,  37557,\n",
      "         236924,    107, 236778, 236761,    138,   1018,  37557, 160266,  53121]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n**模型架构：**\\n\\n*   **Transformer 架构:**  我们使用基于 Transformer 的模型，特别是 BERT 和 RoBERTa 的变体。\\n*   **预训练:** 模型在大量平行语料库（例如，Common Crawl, Wikipedia, BooksCorpus）上预训练。\\n*   **微调:**  在特定任务（例如，机器翻译、问答）上进行微调。\\n\\n**多语言处理策略：**\\n\\n1.  **语言检测:**  在输入文本中进行语言检测，识别出输入文本的语言。\\n2.  **语言适配:**']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:11<00:05,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "           7243,    108,    818,   6573,  54850,    657,   1041,  13925,   5716,\n",
      "         236764,    496,   4512, 236764,  74693,  89830, 236761,   3551,  11762,\n",
      "           3952,    528, 134880,   4314,    975, 236764,    496,   2173, 108507,\n",
      "           2342,    506, 162542,  16045,    529,    506,   7217, 236761,   1701,\n",
      "           5695, 236764,    564, 236858,    560,   1010,  60821,    506,   4820,\n",
      "         236764,  22224,   1041,   2742,    531,   1061,  10298,   4576, 236764,\n",
      "            532,   1492, 237064,   1492,    625, 236858, 236751,    784,   5367,\n",
      "            625, 236761,    564, 236858,    560,   8452,    506,  34976,    529,\n",
      "           9348,  94509, 236761,    108, 236776,   6794,    529,   8176, 236764,\n",
      "            723,    553, 109064,    774,  12690,  93084,   1024,    786, 236764,\n",
      "            834,  18487,    625,  75583,    531,  77937,   1041,  40027, 236761,\n",
      "           1030, 236858, 236751,    711,   1164,  11277, 236764,    625, 236858,\n",
      "         236751,    496,   8178,    529,  27725, 236764,   4180,  15596,   5603,\n",
      "         236761,    564, 236858,    560,   7785,   1041,   1972,  67301,    529,\n",
      "            672]], device='cuda:0')\n",
      "output: ['<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\n---\\n\\nThe wind bites at my exposed skin, a constant, icy whisper. My breath comes in ragged gasps, a white plume against the bruised purple of the sky. For weeks, I’ve been battling the elements, pushing my body to its absolute limit, and now… now it’s all worth it. I’ve reached the summit of Mount Everest.\\n\\nA wave of pure, unadulterated joy washes over me, so intense it threatens to buckle my knees. It’s not just relief, it’s a feeling of profound, almost spiritual connection. I’ve spent my life dreaming of this']\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:13<00:02,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108, 236769, 237703, 236748,   7439,  44693,\n",
      "         100634, 236764,  22947,  31612,  13940,  31889,  89158,   3884, 127379,\n",
      "          63719, 236768,    108,   1390,    108,   1390,    108,   1390,    108,\n",
      "         236769, 217366,  37513,   6256,   8665,  38143,  67946,  11188,  14687,\n",
      "          31889,  89158,   3884, 127379,  63719, 236768,    108,   1018, 149911,\n",
      "          14435,  53121,    108, 236775, 236780,  28416,  89158,   5042,  25969,\n",
      "           3244,   7325,  18457,  26220,  18234,  39652,  18457,   3375,  37814,\n",
      "          30166, 236764,    566, 239731, 236743,  46524,   3884,   7325,  87062,\n",
      "         132076,    534,  66928, 236761,    587,  19084,    637,  11992,  29834,\n",
      "          86569,  29818,  28059, 114589,  94509, 236764,  27876,  25969,  41533,\n",
      "          51414,  27888,  11388,    566, 103294,  98220,    537,  18496,  14759,\n",
      "           2313,  14780,  11388,    494,  19712,  59407, 236761,  90893,  14780,\n",
      "          21587, 236764,  25969,  31889,  27888,  10539,  13892,  27600,   7189,\n",
      "          29834,  84782,  48083,   7325,  21121,  32025,  48580,  27869, 236764,\n",
      "          56250,  32634,  48247, 236764,   3884]], device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\n(Đoạn văn ngắn, tập trung vào cảm xúc và khung cảnh)\\n\\n...\\n\\n...\\n\\n...\\n\\n(Hãy thêm các chi tiết cụ thể về cảm xúc và khung cảnh)\\n\\n**Ví dụ:**\\n\\n\"Cảm xúc của tôi là một sự kết hợp giữa sự ngạc nhiên, vỡ òa và một chút sợ hãi. Mưa rào bao phủ toàn bộ đỉnh Everest, nhưng tôi vẫn nhìn thấy những vầng dương lấp lánh trên những tảng đá. Từ trên cao, tôi cảm thấy như mình đang được bao quanh bởi một thế giới rộng lớn, đầy màu sắc, và']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:16<00:00,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108, 237169,  31799,  17583,  12680, 237408, 151599,  53877, 107462,\n",
      "         237206, 216348, 236924,  14201, 237169,  33813,  12553, 236900,  31799,\n",
      "          85694,  65407,  72245, 237622,  43719, 236924,  13488, 236900, 183868,\n",
      "          21181,  22276,  19695, 236900,  92711,  71553,  31818, 236900,  85694,\n",
      "         237408, 223874, 239972, 241888, 240344, 241454, 239963, 239761, 239963,\n",
      "         236918,  65689, 236900, 237853,  47710, 237408, 237403, 187945, 143572,\n",
      "         236924,    108, 238350,  18577, 236900,  19697,  31818, 238938, 237242,\n",
      "         239852, 238450, 236900, 167467,  45370,  22276,  19695, 236924,    108,\n",
      "           1018,  21480, 107462, 237184,   1018,    108, 237169,  67940,  36027,\n",
      "          85629, 237906, 242588, 236918, 240470, 242894, 236924, 237227, 149885,\n",
      "         237026, 183622, 236900, 237611,  59013, 156076, 236918, 236951,  62637,\n",
      "         244071, 238008, 236918, 201441, 236924, 237169,  67940,   7604, 237658,\n",
      "          71216, 236918, 245540, 237369, 236900, 237658,  70828, 236918, 240874,\n",
      "         241089, 236924, 237169,  67940,  36027,  85629, 237518, 238172, 236918,\n",
      "         187036, 236900]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n我无法直接提供你登山者的情绪和视角。因为我是一个AI，无法模拟人类的情感体验。但是，我可以根据你的要求，创作一段文字，模拟你登上珠穆朗玛峰顶峰的感受，并描述你所看到的景色。\\n\\n请注意，以下文字仅为虚构，旨在满足你的要求。\\n\\n**我的情绪：**\\n\\n我感到一种难以言喻的敬畏。这不仅仅是恐惧，更是一种深深的、令人窒息的震撼。我感到自己与地球的渺小，与宇宙的宏伟。我感到一种难以名状的平静，']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd40de56-e20a-4b9a-903b-771f33ab88e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:03<00:16,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,  25681,    600,   1581,   1581,   1581,\n",
      "           1581,   1581,  15152,    919,    919,    919,    919,    919,    919,\n",
      "            659,    659,    659,    659,    659,    659,    659,    886,   5467,\n",
      "          25681,   1581,   8427,   6269,    783,    564,    564,    564,    107,\n",
      "            586,   5467,  13058,    886,   5467,    699,    107,    107,    107,\n",
      "            107,    107,    107,    107,    107,    107,    107,    107,    107,\n",
      "            107,    107,    645,    107,    645,    107,    645,    107,    645,\n",
      "            107,    645,    107,    107,    645,    107,    107,    645,    107,\n",
      "            645,    107,    645,    107,    645,    107,    645,    107,    645,\n",
      "            107,    645,    107,   8772,    107,  94728,    107,    799,    107,\n",
      "         111856,    107,    997,    107,   2048,    107,   2048,    107,  87720,\n",
      "            107, 192300, 242238,    107,    107,    107, 192300, 242238,    107,\n",
      "            107,    107,    107,    107,    107,    107,    107,    107,  57004,\n",
      "          14679,    107,    107,   1454,   3122,  14679,    107,  14679,    107,\n",
      "          14679,    107,  14679,    107,  14679,    107]], device='cuda:0')\n",
      "output: ['<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages. barely that even even even even even newly more more more more more more are are are are are are are one directly barely even slightly completely . I I I\\nly directly carefully one directly from\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n E\\n E\\n E\\n E\\n E\\n\\n E\\n\\n E\\n E\\n E\\n E\\n E\\n E\\n E\\n Bu\\n Idi\\n In\\n Inoltre\\n #\\n prote\\n prote\\n ________\\n provved̂\\n\\n\\n provved̂\\n\\n\\n\\n\\n\\n\\n\\n\\n Ai ’\\n\\n </ В ’\\n ’\\n ’\\n ’\\n ’\\n']\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:06<00:11,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            107,    532,    107,    107,    107,    107,    107,    107,    107,\n",
      "            107,    107,    107,    107,    107,    107,    107,    107,    107,\n",
      "            107,    107,    107,    107,    107,    107,    107,    107,    107,\n",
      "            107,    107,    107,    107,    107,    107,    107,    107,    107,\n",
      "            107,    603, 237006, 237006,   8772,   8772,  61112,  50811, 242238,\n",
      "           2907,  14679,  14679,  14679,  14679,    107,  14679,    107,  14679,\n",
      "            107,   1732,    107,   1732,    107, 236799,   1732,    107,   1732,\n",
      "            107,  70855,  14679,    107,   1732,   1732, 236768, 184551,  29273,\n",
      "            107, 233689,   1732, 233689,   8119,  14679,    107, 242238,    107,\n",
      "         233689, 176109, 111856,  14679, 137079,   3122, 111856,  17273,  14679,\n",
      "            107, 111856, 242238,    107,   2907, 242238,    107,  82259, 111856,\n",
      "          80267, 170906, 160125, 160125,    394,    394,  74101,   8336,    394,\n",
      "          50950,   2907,    107, 238960,    107,   2907,    107,    394,    107,\n",
      "         111856, 169060, 236743,   3122, 111856, 169060, 169060, 169060, 169060,\n",
      "         169060, 169060]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n and\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Bणण Bu Bu volteDirê.) ’ ’ ’ ’\\n ’\\n ’\\n )\\n )\\nB )\\n )\\nżyt ’\\n ) )).”) į\\n Į ) Į З ’\\n̂\\n Į Perché Inoltre ’ Zim В Inoltre می\\u200c ’\\n Inoltrê\\n.)̂\\n entram Inoltre)” fornire TabLayout TabLayout�� potrebbe tent� debit.)\\nЮ\\n.)\\n�\\n Inoltre असू  В Inoltre असू असू असू असू असू असू']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:08<00:08,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,   4875,   4875,\n",
      "            107,  41077,  46366,    545,  78675, 237194,  23113, 237629,  32703,\n",
      "            107,   2613,    107,    107,   1664,   7394,  49992, 238272,    532,\n",
      "            699,    517,    532,  27461,    107,   6124,  65635,    107,   1734,\n",
      "            107, 236772,   1345,  65635, 116474,    107, 236761,   7856,    107,\n",
      "            107,    107,    107,    107,    107,    107,    107,    107,    107,\n",
      "            107,    107,    107,    107,    107,    107,    107,    107,    107,\n",
      "            107,    107,    107,    107,    107,    107,    107,    107,    107,\n",
      "            107,    107,    107,    107,    107,  24982,  48461,  25681,  14679,\n",
      "          14679,  14679,    107, 236789,    107, 236789,  48461,   1732,    107,\n",
      "         240104, 232408,  80267,  34094,    107,    199,    107,  45200, 111856,\n",
      "         101365,  14679,   1732,   1732,   1732, 137723,  13759,   1732,    107,\n",
      "          31413, 104338, 104338, 102932, 101365, 244290,   6404,  55664,    564,\n",
      "         110212, 110212,   3526,   1197,  74101, 138694,    107,  24919,    107,\n",
      "          24919,  99722,  17322, 241783, 138694,   5686, 138694,   1975, 160859]],\n",
      "       device='cuda:0')\n",
      "output: [\"<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。 trying trying\\n stesso andre e ולમような個 उदा\\n something\\n\\n tra otherwise अग्र題 and from f and erst\\nedsじめ\\npre\\n- endじめ eviden\\n. sto\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n sia Β barely ’ ’ ’\\n'\\n' Β )\\nΒ cervello)” significa\\n<em>\\n زیر Inoltre परत ’ ) ) ) Versuch bare )\\n poi direttamente direttamente preven परतẬपीrizz I aiut aiut ist bu potrebbe intento\\n oso\\n oso dessous bere冊 intento attempt intento stand diventare\"]\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:11<00:05,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    107,\n",
      "          15152,   2847,   1975,   1975,   1975,   1975,   1975,   1975,  10701,\n",
      "            107,    919,   2847,    886,    538,   2344,    919,    107,    107,\n",
      "         236764, 236764, 236764,    107, 236761, 236761, 236761, 236761, 236761,\n",
      "         236761, 236761, 236761, 236761,   1975,   1975,  31440, 237698,    538,\n",
      "            107,    107,    107,    602,    107,    538,   5467,   1003,    538,\n",
      "            538,    107, 236797,    107, 236777,    564,    577,   1197,   2979,\n",
      "            886,    107,    107,    107,    107,    107,    107,    107,    107,\n",
      "            107,    107,    107,    107,    107,    107,    107,    107,    107,\n",
      "            107,    107,    107,    107,    107,    107,  73105,   3122,    756,\n",
      "            107,   7031,    108,    107, 196923,    756, 236743,  14679,    107,\n",
      "            538,    107,    107,  34094,    107,  14679,    107,    756,  14679,\n",
      "            107,  14679,    107,  14679,    107,    593,    107,  14679,    107,\n",
      "          14679,    107,  14679,    107,  14679,    107,  14679,    107,  14679,\n",
      "            107,  14679,    107,  14679,    107,  14679,    107,  14679,    107,\n",
      "          14679]], device='cuda:0')\n",
      "output: [\"<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n newly provide stand stand stand stand stand stand mine\\n more provide one n less more\\n\\n,,,\\n......... stand standprendț n\\n\\n\\nun\\n n directly about n n\\nN\\nI I be bu values one\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n)% В '\\nรับ\\n\\n\\n خب '  ’\\n n\\n\\n significa\\n ’\\n ' ’\\n ’\\n ’\\n P\\n ’\\n ’\\n ’\\n ’\\n ’\\n ’\\n ’\\n ’\\n ’\\n ’\\n ’\"]\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:14<00:02,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    107,  17235, 237211,  10663,    107, 236764,\n",
      "         236764,  58390,  58390,  58390,    107,    107,    107,    107,    107,\n",
      "            107,    107,    107,    107, 236743,    107,    107,    107,    107,\n",
      "            107,   1100,    107,    107,    107,    107,    107,    107,    107,\n",
      "            107,    107,    107,    107,    107,    107,    107,    603,    107,\n",
      "         158348,    107, 158348,  83718,  14679,  14679,  14679,    107,  60668,\n",
      "          14679,    107,  23517,    107,  81442,    107,  14679,    107,  14679,\n",
      "            107,    562,    107,  14679,    107,  14679,    107,  14679,    107,\n",
      "          14679,    107,   1732,   1732,    107,  23676,  14679,  14679,    107,\n",
      "          17679,  14679,   1732,    107, 111856,  14679,   1732,    107, 111856,\n",
      "          14679,  23676,   3122,  14679,  14679,  13085, 170906, 111856,  80267,\n",
      "          14679,  14679,  14679,  13085,  80267,  80267, 101365,  14679,  48461,\n",
      "            107,  14679,  48461,  48461,  13085,  13085,  13085,    107,  48461,\n",
      "          14679,  14679,  14679,  14679,  14679,  14679,  14679,  14679,  14679,\n",
      "          14679,  14679,  48546, 128128,  24919]], device='cuda:0')\n",
      "output: [\"<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n கோض बन\\n,,חדחדחד\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n ب\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n B\\nに着\\nに着 ω ’ ’ ’\\n='') ’\\n ת\\n Vors\\n ’\\n ’\\n A\\n ’\\n ’\\n ’\\n ’\\n ) )\\n І ’ ’\\n Mol ’ )\\n Inoltre ’ )\\n Inoltre ’ І В ’ ’ ” fornire Inoltre)” ’ ’ ’ ”)”)” परत ’ Β\\n ’ Β Β ” ” ”\\n Β ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ ’ Gian येते oso\"]\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:17<00:00,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            107,   5467,    107,   5467,  25575,    107,    107,    107,    107,\n",
      "            107,    107,    107,    107,    107,    107,  28443,   8772,    603,\n",
      "            107,    107,    107,    107,    107,    645,    107,    645,    107,\n",
      "           8772,    107,   8772,    107,    107,    107,    107, 111856,   2329,\n",
      "         242238,    107,    107,    107,    107,    538,    107, 182217,    107,\n",
      "            107,   3122, 241011,   3122,  15390,    107, 237146, 186761,    107,\n",
      "          61670,    107,    782,  25152,    107,   3122,    107, 236789,    107,\n",
      "            900,    107,   1781,    107,  24002,    107,  24002,    107,    107,\n",
      "           1732,    107,    997,    107,    107,    107,    107,   8772,    107,\n",
      "           3122,    107, 237146,   3122,    107,    107,   8772,    107,   1732,\n",
      "         148896,    107,  91175,    107,   1732,    107, 169060,    107, 169060,\n",
      "         236743,    107,    107, 111856,    107,    107, 111856,   8119, 167725,\n",
      "            107, 167725,    107, 111856,    107, 111856,  94728,    107,    107,\n",
      "         111856,    107, 111856,   8119,  23676,   3122,    107,   3122,    107,\n",
      "           3122,   3122]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n directly\\n directly unfortunately\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Dire Bu B\\n\\n\\n\\n\\n E\\n E\\n Bu\\n Bu\\n\\n\\n\\n Inoltre про̂\\n\\n\\n\\n n\\n stå\\n\\n В績 В \\'#\\nВ तपास\\n立ち\\n V Ба\\n В\\n\\'\\n +\\n.\"\\n...\"\\n...\"\\n\\n )\\n #\\n\\n\\n\\n Bu\\n В\\nВ В\\n\\n Bu\\n ) sicuro\\nuß\\n )\\n असू\\n असू \\n\\n Inoltre\\n\\n Inoltre З piens\\n piens\\n Inoltre\\n Inoltre Idi\\n\\n Inoltre\\n Inoltre З І В\\n В\\n В В']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model_en, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "139e09da-30d1-4174-9f5c-58a1d2f57349",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:02<00:14,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761, 239867,   1311,   1311, 204805,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "         237577, 237577, 237577,  42622,   1311,   1311,   1311,  42622,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311]], device='cuda:0')\n",
      "output: ['<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.戰 по поポーチ по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по情情情 Healthcare по по по Healthcare по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по']\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:05<00:11,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "         237577,  42622,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311, 137013,   1311,\n",
      "           1311,   1311,   1311, 137013, 204805, 204805, 204805, 204805, 204805,\n",
      "         204805,  13772, 204805,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311, 137013,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311, 137013, 237577, 237577,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.情 Healthcare по по по по по по по по по по по по по по感兴趣 по по по по感兴趣ポーチポーチポーチポーチポーチポーチませんポーチ по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по感兴趣 по по по по по по по感兴趣情情 по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:08<00:08,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,   1311, 204805,\n",
      "          26269,   1311, 204805,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "         218500, 204805, 204805, 204805, 204805, 204805, 204805, 204805, 204805,\n",
      "         204805, 137013,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。 поポーチــ поポーチ по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по поropoポーチポーチポーチポーチポーチポーチポーチポーチポーチ感兴趣 по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:11<00:05,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,   1311,\n",
      "         239867,   1311, 204805, 204805, 204805, 204805, 204805,  42622,   1311,\n",
      "         204805, 204805, 204805, 204805, 204805, 204805, 204805, 204805, 204805,\n",
      "         137013,   1311,   1311,   1311, 137013, 204805, 204805, 204805, 204805,\n",
      "         204805, 204805, 204805, 204805, 204805, 137013, 204805, 204805, 137013,\n",
      "         204805, 204805, 204805, 137013, 137013, 137013, 204805, 204805, 204805,\n",
      "         204805, 204805, 204805,  42622, 137013,  42622, 239867, 239867,  42622,\n",
      "         239867, 239867,  12726,   1311, 204805, 137013, 204805, 239867, 239867,\n",
      "         239867,   1311, 239867,  42622,  42622, 239867, 239867,  90116, 204805,\n",
      "         137013, 204805, 137013, 204805, 137013, 204805, 137013, 204805, 239867,\n",
      "         239867,  90116, 204805, 137013,   1311, 239867,   1311, 239867, 239867,\n",
      "         239867, 239867, 125917, 239867, 237577, 204805, 137013, 204805, 137013,\n",
      "         204805, 137013, 204805, 137013, 204805, 239867, 239867,  42622,  42622,\n",
      "          42622,  12726,   1311, 137013, 204805, 137013, 204805, 137013,  42622,\n",
      "          42622,  12726,  12726,  42622,  42622,  42622,  42622,   1311, 204805,\n",
      "         204805]], device='cuda:0')\n",
      "output: ['<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top. по戰 поポーチポーチポーチポーチポーチ Healthcare поポーチポーチポーチポーチポーチポーチポーチポーチポーチ感兴趣 по по по感兴趣ポーチポーチポーチポーチポーチポーチポーチポーチポーチ感兴趣ポーチポーチ感兴趣ポーチポーチポーチ感兴趣感兴趣感兴趣ポーチポーチポーチポーチポーチポーチ Healthcare感兴趣 Healthcare戰戰 Healthcare戰戰 Market поポーチ感兴趣ポーチ戰戰戰 по戰 Healthcare Healthcare戰戰はありませんポーチ感兴趣ポーチ感兴趣ポーチ感兴趣ポーチ感兴趣ポーチ戰戰はありませんポーチ感兴趣 по戰 по戰戰戰戰affirm戰情ポーチ感兴趣ポーチ感兴趣ポーチ感兴趣ポーチ感兴趣ポーチ戰戰 Healthcare Healthcare Healthcare Market по感兴趣ポーチ感兴趣ポーチ感兴趣 Healthcare Healthcare Market Market Healthcare Healthcare Healthcare Healthcare поポーチポーチ']\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:14<00:02,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,  42622, 237577, 237577, 237577, 237577, 237577,\n",
      "           1311,  41162, 204805,   1311, 204805, 204805, 204805,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311]], device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao. Healthcare情情情情情 по以外ポーチ поポーチポーチポーチ по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:16<00:00,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "           1311, 204805, 204805, 204805, 204805, 204805,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,   1311,\n",
      "           1311,   1311]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。 поポーチポーチポーチポーチポーチ по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по по']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model_zh, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d40e6-f752-4877-a115-40076b8d9781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31247bde-41dd-4800-aa32-c9e77b806421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163eb0ea-829a-4296-8619-944f1d6eb0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c9da8c-15a1-4bb4-aed6-32a4424f0275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d35225f3-6a1c-408b-8508-c735147d4e37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "model name: org_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 36.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.2627, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 30.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(13.2703, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.5759, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_en_neuron_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 36.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.8695, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 30.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(12.3227, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.1235, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_vi_neuron_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 36.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.4861, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:32<00:00, 30.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(12.4606, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.1349, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_zh_neuron_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 37.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(9.6630, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:32<00:00, 30.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(8.8076, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(9.0421, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "calc ppl\n",
    "\n",
    "'''\n",
    "\n",
    "# load test data\n",
    "test_en = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/en_radom_1000_test.jsonl')\n",
    "test_vi = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/vi_radom_1000_test.jsonl')\n",
    "test_zh = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/zh_radom_1000_test.jsonl')\n",
    "               \n",
    "# calc ppl\n",
    "model_list= [\n",
    "    (model, 'org_model'),\n",
    "    (model_en, 'mask_en_neuron_model'),\n",
    "    (model_vi, 'mask_vi_neuron_model'),\n",
    "    (model_zh, 'mask_zh_neuron_model'),\n",
    "]\n",
    "\n",
    "data_list = [\n",
    "    (test_en, 'en_data'),\n",
    "    (test_vi, 'vi_data'),\n",
    "    (test_zh, 'zh_data')\n",
    "    \n",
    "]\n",
    "\n",
    "for i_model, model_name in model_list:\n",
    "    print('*'*20)\n",
    "    print('*'*20)\n",
    "    print('model name:', model_name)\n",
    "\n",
    "    for i_data, data_name in data_list:\n",
    "        print('='*20)\n",
    "        print('data name:', data_name)\n",
    "        ppl_sum, ppl_count = calc_ppl(i_model, tokenizer, i_data, max_len= 2048)\n",
    "        print('ppl:',ppl_sum/ppl_count )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169fef85-50d7-42e7-98d1-2ec5bd02c6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a836eb-4e99-4408-a42b-2208d5b2a74a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bceb5b-fb32-4b75-901b-2c822281a2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46635ecd-8fb8-4a08-a8c1-8bc38d417d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a22343d9-24ed-4a17-9873-c91ed8ce7e8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:03,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108,   7243,    108,   1018,   4968,\n",
      "          17243,  53121,    108,   7711,   2028,    563,    496,  29193, 236772,\n",
      "           5140,   5192,   2028,  15453,    580,    496,  12566,  15297,    529,\n",
      "           1816,    532,   3393,    528,   5422, 236764,  14361, 236764,   7830]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n---\\n\\n**Model Description:**\\n\\nOur model is a transformer-based language model trained on a massive dataset of text and code in English, Spanish, French']\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:01<00:02,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            108,   1018, 236792, 237124,  14960, 140907,  90687,  27869,    568,\n",
      "           2182, 236792, 236768,    753,    623, 122850,   3271, 124468, 236775,\n",
      "           1018,    108, 122850,   3271, 124468,   3244,   7325,  38325,  14960,\n",
      "         140907,  90687,  27869,   7189,  55415]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n**Mô hình ngôn ngữ lớn (LLM) - \"LinguaVerse\"**\\n\\nLinguaVerse là một mô hình ngôn ngữ lớn được xây']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:02<00:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108,   1018,\n",
      "          26609,  92141, 237184,   1018,    108, 236829,    139,   1018,  86313,\n",
      "         236743,  92141,  53121,    138,   6932,   5938,  39738,  92474,  10363,\n",
      "          26609, 236900,  99193, 158343,  26595,   9824,  11471,  27099,  10363,\n",
      "         238017, 237501, 236924]], device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n**模型架构：**\\n\\n*   **Transformer 架构:**  我们使用基于 Transformer 的模型，特别是 BERT 和 RoBERTa 的变体。']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "           7243,    108,    818,   6573,  54850,    657,   1041,  13925,   5716,\n",
      "         236764,    496,   4512, 236764,  74693,  89830, 236761,   3551,  11762,\n",
      "           3952,    528, 134880,   4314,    975, 236764,    496,   2173, 108507,\n",
      "           2342,    506, 162542,  16045]], device='cuda:0')\n",
      "output: ['<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\n---\\n\\nThe wind bites at my exposed skin, a constant, icy whisper. My breath comes in ragged gasps, a white plume against the bruised purple']\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:03<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108, 236769, 237703, 236748,   7439,  44693,\n",
      "         100634, 236764,  22947,  31612,  13940,  31889,  89158,   3884, 127379,\n",
      "          63719, 236768,    108,   1390,    108,   1390,    108,   1390,    108,\n",
      "         236769, 217366,  37513,   6256,   8665,  38143,  67946,  11188]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\n(Đoạn văn ngắn, tập trung vào cảm xúc và khung cảnh)\\n\\n...\\n\\n...\\n\\n...\\n\\n(Hãy thêm các chi tiết cụ thể']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108, 237169,  31799,  17583,  12680, 237408, 151599,  53877, 107462,\n",
      "         237206, 216348, 236924,  14201, 237169,  33813,  12553, 236900,  31799,\n",
      "          85694,  65407,  72245, 237622,  43719, 236924,  13488, 236900, 183868,\n",
      "          21181,  22276,  19695, 236900,  92711]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n我无法直接提供你登山者的情绪和视角。因为我是一个AI，无法模拟人类的情感体验。但是，我可以根据你的要求，创作']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38744ceb-319c-4b0d-8263-87369a92bc0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27518f8a-3e32-49b2-bbcf-4bbb92337a10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:03,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108,   1018,   4968,  17243,  53121,\n",
      "            108,   7711,   2028,    563,    496,  29193, 236772,   5140,   5192,\n",
      "           2028, 236764,  10916,   5851, 236772, 100526,    580,    496,   2455,\n",
      "          15297,    529, 132567,   1816, 236761,   1030, 236789, 236751,   5402]],\n",
      "       device='cuda:0')\n",
      "output: [\"<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n**Model Description:**\\n\\nOur model is a transformer-based language model, specifically fine-tuned on a large dataset of multilingual text. It's designed\"]\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:01<00:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            108,   1018, 236792, 237124,  14960,  53121,    108, 172915,  21994,\n",
      "          16981,   7325,  38325,  14960,  92474, 236764,   7189, 200755, 100370,\n",
      "          14780,   7325,  22947,  79808,  35112,  27869,  43790,   6256,  44693,\n",
      "          28035,  48078, 140907,  90687, 236761]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n**Mô hình:**\\n\\nTôi sử dụng một mô hình Transformer, được huấn luyện trên một tập dữ liệu lớn gồm các văn bản đa ngôn ngữ.']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:02<00:02,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108,   1018,\n",
      "          26609,  92141,  53121,    138,  39738,  86313,  92141, 236900,  31267,\n",
      "         238794,  42883, 236918,  37557,  26609, 236924,    107,   1018,  42883,\n",
      "           9676,  53121,    138,  37250,  41440,  15497,  37557, 236918,  57489,\n",
      "           9676, 236924,    107]], device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n**模型架构:**  基于Transformer架构，采用预训练的语言模型。\\n**训练数据:**  包含大量不同语言的文本数据。\\n']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "            818,   6573,    563,  85278, 236764,    496,   4512, 236764,  76789,\n",
      "          19211, 236761,   3551,  21367,    659,  62269, 236764,   1041,  19064,\n",
      "          50622, 236764,    840,    564,    740, 236789, 236745,   4721, 236761,\n",
      "            564, 236789,    560,   1010]], device='cuda:0')\n",
      "output: [\"<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\nThe wind is relentless, a constant, biting assault. My fingers are numb, my muscles screaming, but I can't stop. I've been\"]\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:03<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108,   7243,    108, 172915,  12105,  70669,\n",
      "          23946, 114589,  94509, 236761,    587,  21552,  30758,  47635,  16420,\n",
      "          10539,   7325, 231840,    518,   1592, 236764,   7325,  18457,  50014,\n",
      "         117434,  50464, 157104,  12835,  84641,    541, 239246, 236757]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\n---\\n\\nTôi đã leo lên đỉnh Everest. Mọi thứ bắt đầu như một cơn bão, một sự thôi thúc mạnh mẽ từ sâu thẳm']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108, 237169, 237282, 238493, 237408, 236900, 237169, 237282, 238493,\n",
      "         237408, 236900, 237169, 237282, 238493, 237408, 236900,    107, 237169,\n",
      "         237282, 238493, 237408, 236900, 237169, 237282, 238493, 237408, 236900,\n",
      "            107, 237169, 237282, 238493, 237408]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n我来找你，我来找你，我来找你，\\n我来找你，我来找你，\\n我来找你']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model_en, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f31be40b-23cb-4480-95cc-790f6e87c1f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:03,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108,   7243,    107,   8291, 236789,\n",
      "         236751,    496,   3435,   3072, 236787,    108, 236775,   4754,   2028,\n",
      "           6585, 132567,   2744,    684,  64004,    496,   8376,    529,    870,\n",
      "         109219,   2307,   8403,   1133, 236787,   4071, 236772, 187918, 106703]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n---\\nHere\\'s a potential response:\\n\\n\"My model processes multilingual input by leveraging a combination of [mention key techniques like: cross-lingual embeddings']\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:01<00:02,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            108,   1018, 236792, 237124,  14960,  53121,    108, 172915,  27600,\n",
      "          55415,  54380,   7325,  38325,  14960, 140907,  90687,  27869,    568,\n",
      "           2182, 236792, 236768,   4032,  63560,  26571,  86340,  28211,  79808,\n",
      "          35112,  48078, 140907,  90687, 236761]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n**Mô hình:**\\n\\nTôi đang xây dựng một mô hình ngôn ngữ lớn (LLM) có khả năng xử lý dữ liệu đa ngôn ngữ.']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:02<00:02,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108,   1018,\n",
      "          26609,  26544,  53121,    870, 238350, 134451,  22276,  26609,  26544,\n",
      "         236900,  34313, 237184,  86313, 236764, 158343, 236764,   4044,  12120,\n",
      "            107,   1018,  26609,  92141,  53121,    870, 238350, 134451,  22276,\n",
      "          26609,  92141, 236900]], device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n**模型类型:** [请填写你的模型类型，例如：Transformer, BERT, etc.]\\n**模型架构:** [请填写你的模型架构，']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "            818,   6573,    563,    496,   4512, 236764, 214709,   1526, 236764,\n",
      "          34847,   3801,    657,   1041,  36326, 236764,    496,   7445, 236764,\n",
      "          14658,  32272,    529,    506,  37501,   2066,    529,    672,   1977,\n",
      "         236761,   3551,  11762,   3952]], device='cuda:0')\n",
      "output: ['<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\nThe wind is a constant, insistent hand, tugging at my harness, a cold, sharp reminder of the sheer power of this place. My breath comes']\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:03<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108,   7243,    108, 236780,  28416,  89158,\n",
      "           5042,  25969, 236787,    108, 236814,  29083,  10539,  27600,  70432,\n",
      "          13940,   7325,  21121,  32025,  23271, 236764,    107, 236773, 237644,\n",
      "            757, 204821,  29834,    719, 145515,  45222,  40515,  30758]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\n---\\n\\nCảm xúc của tôi:\\n\\nHóa như đang bước vào một thế giới mới,\\nSự im lặng bao trùm lấy mọi thứ']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108,   7243,    108, 237169,  79948, 239972, 241888, 240344, 241454,\n",
      "         239963, 239761, 239963, 236900, 239263, 237245, 237026, 239783, 206306,\n",
      "         240274, 238441, 236900, 238321, 239156, 239156, 237307, 240649, 237464,\n",
      "         236900,  57944, 239562, 239577, 236918]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n---\\n\\n我站在珠穆朗玛峰顶峰，脚下是冰冷的岩石，风呼呼地吹过，带来刺骨的']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model_vi, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e3e9f52-a4ac-404c-a05c-c3c43205f6d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:03,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108, 174355, 236764,    506,   2744,\n",
      "            563,    496,   3996, 236764,   5074, 236772, 172287, 236764,    532,\n",
      "          74647,  13186, 236761,    107,   2717,    107,   2717,    107,   2717,\n",
      "            107,   2717,    107,   2717,    107,   2717,    107,   2717,    107]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\nEssentially, the input is a complex, multi-faceted, and interconnected entity.\\n```\\n```\\n```\\n```\\n```\\n```\\n```\\n']\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:01<00:02,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            107, 237703, 236916,   3244,   7325,  52898,  30170,    107, 237703,\n",
      "         236916,   3244,   7325,  52898,  30170,    107, 237703, 236916,   3244,\n",
      "           7325,  52898,  30170,    107, 237703, 236916,   3244,   7325,  52898,\n",
      "          30170,    107, 237703, 236916,   3244]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\nĐó là một vấn đề\\nĐó là một vấn đề\\nĐó là một vấn đề\\nĐó là một vấn đề\\nĐó là']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:02<00:02,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108,  26609,\n",
      "          42883,   9676,  19157,  33783,  42050,  55682, 237409, 237184,    107,\n",
      "          42883,   9676,  19157,  33783,  42050,  55682, 237409, 237184,  42883,\n",
      "           9676,  19157,  33783,  42050,  55682, 237409, 237184,  42883,   9676,\n",
      "          19157,  33783,  42050]], device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n模型训练数据处理质量保证一致性：\\n训练数据处理质量保证一致性：训练数据处理质量保证一致性：训练数据处理质量保证']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "           1018,    108,   1018,    108,   1018,    108,   1018,    108,   1018,\n",
      "            108,   1018,    108,   1018,    108,   1018,    108,   1018,    108,\n",
      "           1018,    108,   1018,    108,   1018,    108,   1018,    108,   1018,\n",
      "            108,   1018,    108,   1018]], device='cuda:0')\n",
      "output: ['<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n**']\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:03<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108,  99972,  31889,  89158,   5042,  10044,\n",
      "           3884, 127379,  63719,  10044,  51414,  27888,  12835,  14780,  21587,\n",
      "         236761,    107, 237703,    581,  31889,  89158,   5042,  10044,   3884,\n",
      "         127379,  63719,  10044,  51414,  27888,  12835,  14780,  21587]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\nBạn cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\nĐem cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n",
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108,    106]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n<end_of_turn>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model_zh, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e9af8-a950-4a96-8660-21d6006b5f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d2e63-45ff-481c-9545-a1e80541deee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
