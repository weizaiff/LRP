{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a66f8a83-e87d-4da4-8d01-6f689f7cb24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    20251124:\\n        把一些神经元mask掉，再看看generation\\n    20251126:\\n        计算ppl的值\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    20251124:\n",
    "        把一些神经元mask掉，再看看generation\n",
    "    20251126:\n",
    "        计算ppl的值\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3ec4d08-fa6a-4ae2-a5a4-00aedec2f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "# download checkpoint\n",
    "from accelerate import load_checkpoint_and_dispatch\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To prevent long warnings :)\n",
    "\n",
    "#from accelerate import load_checkpoint_and_dispatch\n",
    "\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ccdc958-9ef5-4565-91ce-91c3de3b7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44b5dc3b-90c2-4154-8cb0-ba0598597bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import importlib\n",
    "\n",
    "importlib.reload(util)      # 只能 reload 模块本身\n",
    "from util import calc_ppl, get_test_data   # reload 后再重新 import 函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df4897d-a0b8-45a5-af0e-f8c650850d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2bdba13-ef65-4432-9908-ccf3d995518d",
   "metadata": {},
   "source": [
    "# function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03ac7d9-4627-4ede-b09e-c485db6065f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint):\n",
    "    config = AutoConfig.from_pretrained(checkpoint,trust_remote_code=True)\n",
    "    print('checkpoint:', checkpoint)\n",
    "    \n",
    "    \n",
    "    if True:\n",
    "    \n",
    "        device_map='auto'\n",
    "        model= AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True, torch_dtype= torch.bfloat16,device_map=device_map ) # for download model weight\n",
    "    \n",
    "    else:\n",
    "        with init_empty_weights():\n",
    "            model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n",
    "    \n",
    "        model = load_checkpoint_and_dispatch(\n",
    "            model, checkpoint, device_map=\"auto\", dtype=torch.bfloat16#, no_split_module_classes=[\"GPTJBlock\"]\n",
    "        )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)#AutoTokenizer.from_pretrained(\"/home/work/lyftri/projects/model_zoo/compass_sea_13b_s4_merge2HF_org_convert_TP_1_PP_2\",  trust_remote_code=True)#AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def set_neuron_zero(model, neuron_list):\n",
    "    '''\n",
    "    将指定的neuron设置为zero\n",
    "\n",
    "    '''\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    for ikey,_ in neuron_list:\n",
    "        w_name = ikey.split('_index_')[0]\n",
    "        neuron_index = int(ikey.split('_index_')[1])\n",
    "\n",
    "        #set zero\n",
    "        state_dict[w_name][neuron_index,:] = 0\n",
    "    return model\n",
    "    \n",
    "\n",
    "\n",
    "def generate(model, tokenizer, textlist):\n",
    "    \n",
    "\n",
    "    \n",
    "    #textlist=[\"The garden blooms with vibrant colors, a gentle breeze carries the scent of roses, and the sun casts a warm glow.\"] # maxlen 64\n",
    "    #textlist=['智慧殿堂中，书籍如海，知识无尽，探索永恒。艾拉追寻历史，灵感无限']# maxlen 64\n",
    "    #textlist=['你是']# maxlen 64\n",
    "    \n",
    "    #textlist=['The garden blooms with vibrant colors, a gentle breeze carries the scent of roses, and the sun casts a warm glow.']\n",
    "    #textlist=['China is']\n",
    "    bar = tqdm(total=len(textlist))\n",
    "    \n",
    "    \n",
    "    device='cuda' # cpu cuda\n",
    "    for itext in textlist:\n",
    "        print('*'*20)\n",
    "        input_text = itext\n",
    "        print('input:', input_text)\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "        print('input_ids:', input_ids)\n",
    "    \n",
    "        outputs = model.generate(**input_ids, do_sample=False, num_beams=1, max_new_tokens=128) #max_length \n",
    "        print('outputs:', outputs)\n",
    "        print('output:',tokenizer.batch_decode(outputs, skip_special_tokens=False))\n",
    "        bar.update(1)\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b949cc5-8e4e-46c3-815c-0dc92450fb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint: /root/autodl-fs/model_zoo/google/gemma-3-1b-it\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model('/root/autodl-fs/model_zoo/google/gemma-3-1b-it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93683ce9-c877-4a61-9c00-b752f310881c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98470036-f3ad-4f1c-9702-122ec51fbf36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a06c7-e09f-4fb5-aed3-ccf647a9fc79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d94de-718d-466c-8fc5-149420e65f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f11befb-4a9c-45c2-9d87-934ae7a65e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6d51c20-8aeb-47b5-92d3-9d97a8ff1989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2e56574-309f-46cb-91c0-4997fadb3460",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_en = torch.load('/root/autodl-fs/LAPE_res/202251124_en.pt', weights_only=False)\n",
    "neuron_vi = torch.load('/root/autodl-fs/LAPE_res/202251124_vi.pt', weights_only=False)\n",
    "neuron_zh = torch.load('/root/autodl-fs/LAPE_res/202251124_zh.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5577ddec-3541-4447-acfa-341abcbb6012",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_en = copy.deepcopy(model)\n",
    "model_en = set_neuron_zero(model_en, neuron_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b1a8da-dd97-4f9c-a613-a9607f7f4584",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vi = copy.deepcopy(model)\n",
    "model_vi = set_neuron_zero(model_vi, neuron_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adbf300c-a4d3-4a41-83ad-6709273b48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_zh = copy.deepcopy(model)\n",
    "model_zh = set_neuron_zero(model_zh, neuron_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd83ab-bd15-4c3f-910e-3282f71443c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e8a040e-4f79-4b12-b9e6-2f3f9c2338fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9177b3196e148d893622cabae756b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411a6bf5fd6749d0afe8be6e68b88462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "model name: org_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 35.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.2627, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 30.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(13.2703, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.5759, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_en_neuron_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 36.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.4965, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 30.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.4776, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.8758, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_vi_neuron_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 36.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.6816, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:32<00:00, 30.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(13.1771, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.8431, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_zh_neuron_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 37.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.4284, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:32<00:00, 30.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.9954, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.4953, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "test_en = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/en_radom_1000_test.jsonl')\n",
    "test_vi = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/vi_radom_1000_test.jsonl')\n",
    "test_zh = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/zh_radom_1000_test.jsonl')\n",
    "               \n",
    "# calc ppl\n",
    "model_list= [\n",
    "    (model, 'org_model'),\n",
    "    (model_en, 'mask_en_neuron_model'),\n",
    "    (model_vi, 'mask_vi_neuron_model'),\n",
    "    (model_zh, 'mask_zh_neuron_model'),\n",
    "]\n",
    "\n",
    "data_list = [\n",
    "    (test_en, 'en_data'),\n",
    "    (test_vi, 'vi_data'),\n",
    "    (test_zh, 'zh_data')\n",
    "    \n",
    "]\n",
    "\n",
    "for i_model, model_name in model_list:\n",
    "    print('*'*20)\n",
    "    print('*'*20)\n",
    "    print('model name:', model_name)\n",
    "\n",
    "    for i_data, data_name in data_list:\n",
    "        print('='*20)\n",
    "        print('data name:', data_name)\n",
    "        ppl_sum, ppl_count = calc_ppl(i_model, tokenizer, i_data, max_len= 2048)\n",
    "        print('ppl:',ppl_sum/ppl_count )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211b60e7-b2b1-4b0f-979f-5c8d19fde434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2bee7a-9877-4dfc-bb65-9ae57a676ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e857ce-7485-4095-80b5-d40701383553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7251de-8ac4-4f5b-b89c-538b87cbf9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5bcb11-ced7-4ad4-9b19-dc7952533d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ea3d0-30a9-4682-be84-c6b4774f1ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46a2e842-2cb8-4d86-bd31-80b5b14c272b",
   "metadata": {},
   "source": [
    "## some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c776812b-77e1-4f0a-8713-e8893aa3d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "textlist=[\n",
    "    'Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.',\n",
    "    'Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.',\n",
    "    '请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。',\n",
    "    'You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.',\n",
    "    'Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.',\n",
    "    '你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。'\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bda5997-6837-42a2-8a91-2d05d8a22b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\norgmodel:\\n    1.Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n---\\n\\n**Model Description:**\\n\\nOur model is a transformer-based language model trained on a massive dataset of text and code in English, Spanish, French\\n    2.Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n**Mô hình ngôn ngữ lớn (LLM) - \"LinguaVerse\"**\\n\\nLinguaVerse là một mô hình ngôn ngữ lớn được xây\\n    3.请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n**模型架构：**\\n\\n*   **Transformer 架构:**  我们使用基于 Transformer 的模型，特别是 BERT 和 RoBERTa 的变体。\\n\\nmask en neuron:\\n    1.Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n}\\n\\n- \"It\" \\n- \"It\" \\n\\n*   \"The\"\\n*   \"or\"\\n*   \"It\\'\\n    2.Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n. . .\\n> . . . .\\n\\n. . . . . . .\\n\\n**\\n\\n**\\n\\n**\\n\\n,\\n\\n**\\n\\n,\\n\\n**\\n    3.请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n\\n\\n多语言输入模型的，可以有以下这些处理方法和技术：\\n\\n1  1 个性化。\\n2  1，1，\\n\\n\\nmask vi neuron:\\n    1.Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n**The Problem:**\\n\\nThe core of this is to process the input in a way that is most appropriate for the model, and to do so in a\\n    2.Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n---\\n\\nВкратце,\\nПросто наберите\\nЯ не\\nБудьте\\nВсе, что\\nВы знаете\\nЧто\\nКак\\n\\n    3.请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n“请帮助 me stay silent\" - It\\'s a beautiful day.\\nHow would you approach this problem?\\nBy far, but it\\'s\\n\\nmask zh neuron:\\n    1.Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\nHere re/this model explains how to handle the inconsistencies in the quality.\\n\\nThe following steps involve how to handle the calls to the model processes the requests\\n    2.Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau. (إหว่างமைப்பு والأسקה האגירעות)\\n(Murasala Citrix??)\\n\\nvề mặt máy chính ảnh posição filo? (\\n    3.请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n\\n\\n。\\n\\n。。а。о。о。\\n**ru。** **。**\\n**。**\\n**。**\\n**。**\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "orgmodel:\n",
    "    1.Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n---\\n\\n**Model Description:**\\n\\nOur model is a transformer-based language model trained on a massive dataset of text and code in English, Spanish, French\n",
    "    2.Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n**Mô hình ngôn ngữ lớn (LLM) - \"LinguaVerse\"**\\n\\nLinguaVerse là một mô hình ngôn ngữ lớn được xây\n",
    "    3.请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n**模型架构：**\\n\\n*   **Transformer 架构:**  我们使用基于 Transformer 的模型，特别是 BERT 和 RoBERTa 的变体。\n",
    "\n",
    "mask en neuron:\n",
    "    1.Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n}\\n\\n- \"It\" \\n- \"It\" \\n\\n*   \"The\"\\n*   \"or\"\\n*   \"It\\'\n",
    "    2.Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n. . .\\n> . . . .\\n\\n. . . . . . .\\n\\n**\\n\\n**\\n\\n**\\n\\n,\\n\\n**\\n\\n,\\n\\n**\n",
    "    3.请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n\\n\\n多语言输入模型的，可以有以下这些处理方法和技术：\\n\\n1  1 个性化。\\n2  1，1，\\n\n",
    "\n",
    "mask vi neuron:\n",
    "    1.Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n**The Problem:**\\n\\nThe core of this is to process the input in a way that is most appropriate for the model, and to do so in a\n",
    "    2.Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n---\\n\\nВкратце,\\nПросто наберите\\nЯ не\\nБудьте\\nВсе, что\\nВы знаете\\nЧто\\nКак\\n\n",
    "    3.请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n“请帮助 me stay silent\" - It\\'s a beautiful day.\\nHow would you approach this problem?\\nBy far, but it\\'s\n",
    "\n",
    "mask zh neuron:\n",
    "    1.Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\nHere re/this model explains how to handle the inconsistencies in the quality.\\n\\nThe following steps involve how to handle the calls to the model processes the requests\n",
    "    2.Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau. (إหว่างமைப்பு والأسקה האגירעות)\\n(Murasala Citrix??)\\n\\nvề mặt máy chính ảnh posição filo? (\n",
    "    3.请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n\\n\\n。\\n\\n。。а。о。о。\\n**ru。** **。**\\n**。**\\n**。**\\n**。**\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81d1d1f6-8ed8-44af-b47b-8fd2e130d607",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:02<00:14,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108,   7243,    108,   1018,   4968,\n",
      "          17243,  53121,    108,   7711,   2028,    563,    496,  29193, 236772,\n",
      "           5140,   5192,   2028,  15453,    580,    496,  12566,  15297,    529,\n",
      "           1816,    532,   3393,    528,   5422, 236764,  14361, 236764,   7830,\n",
      "         236764,    532,   9115, 236761,   1030,  60432,    496,   5074, 236772,\n",
      "         187918,  43703,   6352,    531,   2754,   1546,   5192,    618,    496,\n",
      "           3550,    528,    496,   7659,   2557, 236761,    138,  14521,  34711,\n",
      "         236764,    506,   2028,  52298,    506,   2148,   8369,    528,    506,\n",
      "           7501, 236764,  11337,    506,   4403,    529,    506,   4251,   2744,\n",
      "         236764,   2440,    506,   3738,   5192, 236761,    138,   1882,   2736,\n",
      "            496,  26446,   5700,  10241,    600,   5862,    506,   2028,    531,\n",
      "          19281,    506,   8020,    529,   1607,  15579,   1056,   8487,    496,\n",
      "           2238,   2744, 236761,    138,  41781, 236764,    692,  27968,    496,\n",
      "           5192, 236772,  15396,  53255,   7545, 236764,  39919,    506,   3938,\n",
      "           6518,   2721,    580,    506,  13207,   5192]], device='cuda:0')\n",
      "output: ['<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n---\\n\\n**Model Description:**\\n\\nOur model is a transformer-based language model trained on a massive dataset of text and code in English, Spanish, French, and German. It utilizes a multi-lingual embedding layer to represent each language as a vector in a shared space.  During inference, the model predicts the next token in the sequence, considering the context of the entire input, including the source language.  We employ a sophisticated attention mechanism that allows the model to weigh the importance of different languages when processing a given input.  Furthermore, we incorporate a language-specific decoding strategy, adjusting the output format based on the detected language']\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:05<00:11,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            108,   1018, 236792, 237124,  14960, 140907,  90687,  27869,    568,\n",
      "           2182, 236792, 236768,    753,    623, 122850,   3271, 124468, 236775,\n",
      "           1018,    108, 122850,   3271, 124468,   3244,   7325,  38325,  14960,\n",
      "         140907,  90687,  27869,   7189,  55415,  54380, 142275,  14780,  50793,\n",
      "         124290,  92474, 236761,  91509,   7189, 200755, 100370,  14780,   7325,\n",
      "          21682,  27869,  79808,  35112,  44693,  28035,  48078, 140907,  90687,\n",
      "         236764,  29834,  43790,  42292, 236764,  48415, 236764,  29956,   4108,\n",
      "         236764,  31089,  62805,   3884,   6256,  62805,  22582, 236761,    138,\n",
      "         122850,   3271, 124468,  21994,  16981,   7325,  28749,  37944,    623,\n",
      "            791,   5419,  22027, 140907,  90687, 236775,    568,  19859, 236772,\n",
      "          22426,  34684, 236768,  11408,  86340,  28211,  79808,  35112,  48078,\n",
      "         140907,  90687, 236761,    108,   1018, 135709,  86340,  28211,  79808,\n",
      "          35112,  48078, 140907,  90687,  53121,    108, 236770, 236761,   5213,\n",
      "           3586,   5419,  22027, 140907,  90687,  53121,    138, 199259,  11638,\n",
      "         200755, 100370]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n**Mô hình ngôn ngữ lớn (LLM) - \"LinguaVerse\"**\\n\\nLinguaVerse là một mô hình ngôn ngữ lớn được xây dựng dựa trên kiến trúc Transformer. Nó được huấn luyện trên một lượng lớn dữ liệu văn bản đa ngôn ngữ, bao gồm sách, báo, trang web, mã nguồn và các nguồn khác.  LinguaVerse sử dụng một hệ thống \"phân chia ngôn ngữ\" (language-aware splitting) để xử lý dữ liệu đa ngôn ngữ.\\n\\n**Cách xử lý dữ liệu đa ngôn ngữ:**\\n\\n1. **Phân chia ngôn ngữ:**  Trước khi huấn luyện']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:08<00:08,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108,   1018,\n",
      "          26609,  92141, 237184,   1018,    108, 236829,    139,   1018,  86313,\n",
      "         236743,  92141,  53121,    138,   6932,   5938,  39738,  92474,  10363,\n",
      "          26609, 236900,  99193, 158343,  26595,   9824,  11471,  27099,  10363,\n",
      "         238017, 237501, 236924,    107, 236829,    139,   1018, 238794,  42883,\n",
      "          53121, 228546, 237075,  41440, 122462, 238384, 237616, 238956, 237221,\n",
      "          34313, 236900,  19892,  42761, 236752, 236764,  24612, 236764,  20980,\n",
      "           9521,  21553, 237214, 237152, 238794,  42883, 236924,    107, 236829,\n",
      "            139,   1018, 238477, 238250,  53121,    138, 237075,  60009,  34769,\n",
      "         237221,  34313, 236900,  51231,  90194, 236951, 237889, 238843, 237214,\n",
      "         237152,  10937, 238477, 238250, 236924,    108,   1018, 237309,  37557,\n",
      "          19157,  59322, 237184,   1018,    108, 236770, 236761,    138,   1018,\n",
      "          37557,  45010,  53121,    138, 237075,  26878,  57489, 237103,  10937,\n",
      "          37557,  45010, 236900,  77100, 237191,  26878,  57489, 236918,  37557,\n",
      "         236924,    107, 236778, 236761,    138,   1018,  37557, 160266,  53121]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n**模型架构：**\\n\\n*   **Transformer 架构:**  我们使用基于 Transformer 的模型，特别是 BERT 和 RoBERTa 的变体。\\n*   **预训练:** 模型在大量平行语料库（例如，Common Crawl, Wikipedia, BooksCorpus）上预训练。\\n*   **微调:**  在特定任务（例如，机器翻译、问答）上进行微调。\\n\\n**多语言处理策略：**\\n\\n1.  **语言检测:**  在输入文本中进行语言检测，识别出输入文本的语言。\\n2.  **语言适配:**']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:11<00:05,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "           7243,    108,    818,   6573,  54850,    657,   1041,  13925,   5716,\n",
      "         236764,    496,   4512, 236764,  74693,  89830, 236761,   3551,  11762,\n",
      "           3952,    528, 134880,   4314,    975, 236764,    496,   2173, 108507,\n",
      "           2342,    506, 162542,  16045,    529,    506,   7217, 236761,   1701,\n",
      "           5695, 236764,    564, 236858,    560,   1010,  60821,    506,   4820,\n",
      "         236764,  22224,   1041,   2742,    531,   1061,  10298,   4576, 236764,\n",
      "            532,   1492, 237064,   1492,    625, 236858, 236751,    784,   5367,\n",
      "            625, 236761,    564, 236858,    560,   8452,    506,  34976,    529,\n",
      "           9348,  94509, 236761,    108, 236776,   6794,    529,   8176, 236764,\n",
      "            723,    553, 109064,    774,  12690,  93084,   1024,    786, 236764,\n",
      "            834,  18487,    625,  75583,    531,  77937,   1041,  40027, 236761,\n",
      "           1030, 236858, 236751,    711,   1164,  11277, 236764,    625, 236858,\n",
      "         236751,    496,   8178,    529,  27725, 236764,   4180,  15596,   5603,\n",
      "         236761,    564, 236858,    560,   7785,   1041,   1972,  67301,    529,\n",
      "            672]], device='cuda:0')\n",
      "output: ['<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\n---\\n\\nThe wind bites at my exposed skin, a constant, icy whisper. My breath comes in ragged gasps, a white plume against the bruised purple of the sky. For weeks, I’ve been battling the elements, pushing my body to its absolute limit, and now… now it’s all worth it. I’ve reached the summit of Mount Everest.\\n\\nA wave of pure, unadulterated joy washes over me, so intense it threatens to buckle my knees. It’s not just relief, it’s a feeling of profound, almost spiritual connection. I’ve spent my life dreaming of this']\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:14<00:02,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108, 236769, 237703, 236748,   7439,  44693,\n",
      "         100634, 236764,  22947,  31612,  13940,  31889,  89158,   3884, 127379,\n",
      "          63719, 236768,    108,   1390,    108,   1390,    108,   1390,    108,\n",
      "         236769, 217366,  37513,   6256,   8665,  38143,  67946,  11188,  14687,\n",
      "          31889,  89158,   3884, 127379,  63719, 236768,    108,   1018, 149911,\n",
      "          14435,  53121,    108, 236775, 236780,  28416,  89158,   5042,  25969,\n",
      "           3244,   7325,  18457,  26220,  18234,  39652,  18457,   3375,  37814,\n",
      "          30166, 236764,    566, 239731, 236743,  46524,   3884,   7325,  87062,\n",
      "         132076,    534,  66928, 236761,    587,  19084,    637,  11992,  29834,\n",
      "          86569,  29818,  28059, 114589,  94509, 236764,  27876,  25969,  41533,\n",
      "          51414,  27888,  11388,    566, 103294,  98220,    537,  18496,  14759,\n",
      "           2313,  14780,  11388,    494,  19712,  59407, 236761,  90893,  14780,\n",
      "          21587, 236764,  25969,  31889,  27888,  10539,  13892,  27600,   7189,\n",
      "          29834,  84782,  48083,   7325,  21121,  32025,  48580,  27869, 236764,\n",
      "          56250,  32634,  48247, 236764,   3884]], device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\n(Đoạn văn ngắn, tập trung vào cảm xúc và khung cảnh)\\n\\n...\\n\\n...\\n\\n...\\n\\n(Hãy thêm các chi tiết cụ thể về cảm xúc và khung cảnh)\\n\\n**Ví dụ:**\\n\\n\"Cảm xúc của tôi là một sự kết hợp giữa sự ngạc nhiên, vỡ òa và một chút sợ hãi. Mưa rào bao phủ toàn bộ đỉnh Everest, nhưng tôi vẫn nhìn thấy những vầng dương lấp lánh trên những tảng đá. Từ trên cao, tôi cảm thấy như mình đang được bao quanh bởi một thế giới rộng lớn, đầy màu sắc, và']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:17<00:00,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108, 237169,  31799,  17583,  12680, 237408, 151599,  53877, 107462,\n",
      "         237206, 216348, 236924,  14201, 237169,  33813,  12553, 236900,  31799,\n",
      "          85694,  65407,  72245, 237622,  43719, 236924,  13488, 236900, 183868,\n",
      "          21181,  22276,  19695, 236900,  92711,  71553,  31818, 236900,  85694,\n",
      "         237408, 223874, 239972, 241888, 240344, 241454, 239963, 239761, 239963,\n",
      "         236918,  65689, 236900, 237853,  47710, 237408, 237403, 187945, 143572,\n",
      "         236924,    108, 238350,  18577, 236900,  19697,  31818, 238938, 237242,\n",
      "         239852, 238450, 236900, 167467,  45370,  22276,  19695, 236924,    108,\n",
      "           1018,  21480, 107462, 237184,   1018,    108, 237169,  67940,  36027,\n",
      "          85629, 237906, 242588, 236918, 240470, 242894, 236924, 237227, 149885,\n",
      "         237026, 183622, 236900, 237611,  59013, 156076, 236918, 236951,  62637,\n",
      "         244071, 238008, 236918, 201441, 236924, 237169,  67940,   7604, 237658,\n",
      "          71216, 236918, 245540, 237369, 236900, 237658,  70828, 236918, 240874,\n",
      "         241089, 236924, 237169,  67940,  36027,  85629, 237518, 238172, 236918,\n",
      "         187036, 236900]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n我无法直接提供你登山者的情绪和视角。因为我是一个AI，无法模拟人类的情感体验。但是，我可以根据你的要求，创作一段文字，模拟你登上珠穆朗玛峰顶峰的感受，并描述你所看到的景色。\\n\\n请注意，以下文字仅为虚构，旨在满足你的要求。\\n\\n**我的情绪：**\\n\\n我感到一种难以言喻的敬畏。这不仅仅是恐惧，更是一种深深的、令人窒息的震撼。我感到自己与地球的渺小，与宇宙的宏伟。我感到一种难以名状的平静，']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdcaa0a1-a78e-4c8b-9180-3c911fb44b56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:02<00:14,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108, 236783,    108, 236772,    623,\n",
      "           1509, 236775, 236743,    107, 236772,    623,   1509, 236775, 236743,\n",
      "            108, 236829,    139, 236775,    818, 236775,    107, 236829,    139,\n",
      "         236775,    504, 236775,    107, 236829,    139, 236775,   1509, 236789,\n",
      "         236751, 236775,    107, 236829,    139, 236775,    509, 236789, 236751,\n",
      "         236775,    107, 236829,    139, 236775,   1509, 236789, 236751, 236775,\n",
      "            107, 236813,    107, 236829,    139, 236775,    509, 236789, 236751,\n",
      "         236775,    107, 236829,    139, 236829,    139, 236775,   1509, 236789,\n",
      "         236751, 236775,    107, 236829,    139, 236775,   1509, 236789, 236751,\n",
      "         236775,    107, 236829,    139, 236775,   1509, 236789, 236751, 236775,\n",
      "            107, 236829,    139,   1390,    107, 236829,    139, 236775,   1509,\n",
      "         236789, 236751, 236775,    107, 236829,    139, 236775,   1509, 236858,\n",
      "         236751, 236775,    107, 236829,    139, 236829,    107, 236829,    139,\n",
      "         236775, 236776, 236919,    107, 236829,    139, 236913,    624, 236919,\n",
      "            107, 236829,    139, 236913,    818, 236919]], device='cuda:0')\n",
      "output: ['<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n}\\n\\n- \"It\" \\n- \"It\" \\n\\n*   \"The\"\\n*   \"or\"\\n*   \"It\\'s\"\\n*   \"it\\'s\"\\n*   \"It\\'s\"\\n>\\n*   \"it\\'s\"\\n*   *   \"It\\'s\"\\n*   \"It\\'s\"\\n*   \"It\\'s\"\\n*   ...\\n*   \"It\\'s\"\\n*   \"It’s\"\\n*   *\\n*   \"A”\\n*   “and”\\n*   “The”']\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:05<00:11,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            108, 236761,    783,    783,    107, 236813,    783,    783,    783,\n",
      "            783,    108, 236761,    783,    783,    783,    783,    783,    783,\n",
      "            108,   1018,    108,   1018,    108,   1018,    108, 236764,    108,\n",
      "           1018,    108, 236764,    108,   1018, 236772,    753,    753,    753,\n",
      "            753,    753,    108,   1018,    108, 236764,    108, 236764,    108,\n",
      "         236764,    108, 236764,    108, 236764,    108, 236764,    108, 236764,\n",
      "            108, 236764,    108, 236764,    108, 236764,    108, 236764,    108,\n",
      "         236764,    108, 236764,    108, 236798,    108, 236792,    108, 236776,\n",
      "         237381, 237381, 237381, 237381, 237381, 237381, 237381, 236763, 237381,\n",
      "         236747, 237381, 236766, 237381, 236800, 237381, 236744, 237381, 237381,\n",
      "         237213, 237213, 236754, 237381, 237213, 236760, 237381, 236800, 236744,\n",
      "         237381, 237381, 200447,   1018,    108,   1018,    108, 236764,    108,\n",
      "           1018, 236772,   1271,   6437,  77101,   1018,    108, 236764,    108,\n",
      "           1018,    108, 236764,    108,   1018,    108, 236764,    108, 236764,\n",
      "            108, 236764]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n. . .\\n> . . . .\\n\\n. . . . . . .\\n\\n**\\n\\n**\\n\\n**\\n\\n,\\n\\n**\\n\\n,\\n\\n**- - - - - -\\n\\n**\\n\\n,\\n\\n,\\n\\n,\\n\\n,\\n\\n,\\n\\n,\\n\\n,\\n\\n,\\n\\n,\\n\\n,\\n\\n,\\n\\n,\\n\\n,\\n\\nL\\n\\nM\\n\\nA·······b·i·v·3·e··••h·•f·3e·· অক্ষর**\\n\\n**\\n\\n,\\n\\n**- – ···**\\n\\n,\\n\\n**\\n\\n,\\n\\n**\\n\\n,\\n\\n,\\n\\n,']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:08<00:08,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108,    108,\n",
      "         237309,  37557,  26878, 139919, 236900,   5157, 237078,  19697,  16956,\n",
      "          19157,  10573, 237206,  16472, 237184,    108, 236770,    138, 236770,\n",
      "          68413, 237409, 237474, 236924,    107, 236778,    138, 236770, 236900,\n",
      "         236770, 236900,    107, 236800, 236924,    107, 236924,    107,  59236,\n",
      "          37557, 236951,    108, 236812,    108, 236770, 236924, 236770, 236924,\n",
      "            108, 236924,    108, 236924,    108, 236924,    108, 236924,    108,\n",
      "         236924, 236800, 236924,    108, 236924,    108, 236924,    108, 236924,\n",
      "            108, 236924,    108, 236924,    108, 236924,    108, 236924,    108,\n",
      "         236924,    108, 236924,    108, 236924,    108, 236924,    108, 236924,\n",
      "            108, 236924,    108, 236924,    108, 236924,    108, 236924,    108,\n",
      "         236924,    108, 236924,    108, 236924,    108, 236924,    108, 236924,\n",
      "            108, 236924,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108,    108,    108,    108,    108]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n\\n\\n多语言输入模型的，可以有以下这些处理方法和技术：\\n\\n1  1 个性化。\\n2  1，1，\\n3。\\n。\\n、“语言、\\n\\n4\\n\\n1。1。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。3。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:11<00:05,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "            108,    108,    818,  34976,    522,    529,   9348,  94509,    691,\n",
      "            834,  10565, 236772,    108,    108, 236772,    108, 236772,    108,\n",
      "         236772,    108, 236772,    108, 236772,    108, 236772,    108, 236829,\n",
      "            139, 236772,    108, 236772,    108, 236772,    108, 236829,    139,\n",
      "         236829,    139, 236772,    108, 236772,    108, 236772,    108,    108,\n",
      "         236772,    108,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108]], device='cuda:0')\n",
      "output: ['<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\n\\n\\n\\n\\nThe summiting of Mount Everest was so mountain-\\n\\n\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n*   -\\n\\n-\\n\\n-\\n\\n*   *   -\\n\\n-\\n\\n-\\n\\n\\n\\n-\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n']\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:14<00:02,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108,    108,  11388,    566,  27087,  20695,\n",
      "          35619,  14780, 236786,    814,  57199,   5042,   6256,   3375,  18856,\n",
      "            108, 236772,    753,    753,    808,    753,    753,    808,    108,\n",
      "         236769,    829, 236754,   4611,    548,  60717, 236764,    108,    829,\n",
      "         236754,   4611,    548,   5042,    108,    108, 236769, 236754,  11253,\n",
      "            677, 181958,    108, 236754, 236937, 236764, 236754,   2762,   9143,\n",
      "            581,    829,   4279,    108, 236754,  22214,    108, 236754, 237029,\n",
      "         236757,    108,    108, 236754,    108, 236754,    108, 236749,    108,\n",
      "            108, 236757,    108,    108, 236751,    108, 236763,    108, 236752,\n",
      "            108, 236751,    108,    108, 236751,    108,    108, 236753,    108,\n",
      "         236753,    108, 236752,    108, 236752,    108, 236753,    108, 236752,\n",
      "            108,    108, 236757,    108, 236759,    108,    108, 236763,    108,\n",
      "         236746,    108,    108,    108,    108,    108, 236746,    108,    108,\n",
      "            108, 236749,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108]], device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\n\\n\\n những vạch đường dẫn trên/so vực của các ngọn\\n\\n- - - * - - *\\n\\n(nghinnim cổ,\\n\\nnghinnim của\\n\\n\\n\\n(hàng chondes\\n\\nhà,harririemngao\\n\\nhạnh\\n\\nhãm\\n\\n\\n\\nh\\n\\nh\\n\\nn\\n\\n\\n\\nm\\n\\n\\n\\ns\\n\\nb\\n\\nl\\n\\ns\\n\\n\\n\\ns\\n\\n\\n\\nd\\n\\nd\\n\\nl\\n\\nl\\n\\nd\\n\\nl\\n\\n\\n\\nm\\n\\ng\\n\\n\\n\\nb\\n\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\na\\n\\n\\n\\n\\n\\nn\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:17<00:00,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108,    108, 236924,    108, 236924,    108,  46479, 236900,    108,\n",
      "         236924,    107, 236924,    108, 236924,    108, 236924,    108, 236924,\n",
      "            108,    108,    108,    108, 236924,    108, 236924, 236924,    108,\n",
      "         236924,    108,    108,    108,    108, 236924,    108,    108,    108,\n",
      "            108,    108,    108, 236924,    108, 236951,    108,    108,    108,\n",
      "         236924,    108, 236924,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108,    108,    108,    108,    108,\n",
      "            108,    108,    108,    108,    108,    108,    108, 237221,    107,\n",
      "            108, 236924]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n\\n\\n。\\n\\n。\\n\\n特征，\\n\\n。\\n。\\n\\n。\\n\\n。\\n\\n。\\n\\n\\n\\n\\n\\n\\n\\n。\\n\\n。。\\n\\n。\\n\\n\\n\\n\\n\\n\\n\\n。\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n。\\n\\n、\\n\\n\\n\\n\\n\\n。\\n\\n。\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n（\\n\\n\\n。']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model_en, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ea57361-5a42-4944-9dd9-59c7cd42db06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:02<00:14,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108,   1018,    818,  20050,  53121,\n",
      "            108,    818,   7157,    529,    672,    563,    531,   1657,    506,\n",
      "           2744,    528,    496,   1595,    600,    563,   1346,   6780,    573,\n",
      "            506,   2028, 236764,    532,    531,    776,    834,    528,    496,\n",
      "           1595,    600,    506,   2028,    740,   2480,   3050,    506,   2430,\n",
      "         236789, 236751,   9703, 236761,    108,   1018,    818,  23130,  53121,\n",
      "            108, 236770, 236761,   5213, 236798, 223496,  13768,  53121,    669,\n",
      "           2028,   1171,   3548,    531,  18840,    506,   2430, 236789, 236751,\n",
      "           9703, 236761,    107, 236778, 236761,   5213, 236798, 223496, 206043,\n",
      "          53121,    669,   2028,   1299,   5724,    657,    506,  25411,    529,\n",
      "            506,   4171,   1456,    531,   9779,    506,   2430, 236789, 236751,\n",
      "           9703, 236761,    107, 236800, 236761,   5213,   3637,    884,   1854,\n",
      "          53121,    669,   2028,   1299,  21727,    531,  58344,    969,    506,\n",
      "           2430, 236789, 236751,   2864,    573,    496,    919,   3756,    532,\n",
      "           2344,  64426,   1595,    531,  10151,    506]], device='cuda:0')\n",
      "output: [\"<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\n**The Problem:**\\n\\nThe core of this is to process the input in a way that is most appropriate for the model, and to do so in a way that the model can better understand the user's intent.\\n\\n**The Solution:**\\n\\n1. **Linguistic Analysis:** The model first needs to analyze the user's intent.\\n2. **Linguistic Similarity:** The model then looks at the similarity of the words used to describe the user's intent.\\n3. **Contextualization:** The model then tries to contextualize the user's request for a more natural and less ambiguous way to interpret the\"]\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:05<00:11,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            108,   7243,    108, 237146,   8359, 236827,   4953, 236764,    107,\n",
      "          24026,   2933,   1333, 106124,    107, 237998,   1994,    107,  70162,\n",
      "          16276,   1305,    107,  65300, 236764,   4183,    107,  36324, 119436,\n",
      "            107,  51426,    107,  40407,    107, 153218,  11003,    107,  65300,\n",
      "            107,  51426,    107,  36324, 119436,    107,  51426,    107,  47611,\n",
      "            107,  36324,    107,  26450,    107,  84653,  29733,    107,  11199,\n",
      "            107,  19629,  33766,   2759,    107,  26450,    107,  19629,    107,\n",
      "         237304,   9211,  11984,   4915,    107,  36324,    107,  19629,    107,\n",
      "         237304,   9211,  11984, 236934,    107,  65300,    107,  19629,    107,\n",
      "          19629,    107,  19629,  33766,   8413,    107,  19629,  33766,   2759,\n",
      "            107,  19629,    107,  19629,    107,  19629,  33766,   8413,    107,\n",
      "          19629,    107,  19629,    107,  19629,    107, 237590,  30995,    107,\n",
      "          26450,    107,  19629,    107,  19629,    107,  19629,    107,  26450,\n",
      "           2421,   2992,   2573,    107,  26450,    107,  19629,    107,  19629,\n",
      "            107,  19629]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\\n\\n---\\n\\nВкратце,\\nПросто наберите\\nЯ не\\nБудьте\\nВсе, что\\nВы знаете\\nЧто\\nКак\\nБыстро\\nВсе\\nЧто\\nВы знаете\\nЧто\\nЕсли\\nВы\\nНе\\nХодите\\nПо\\nЗамечание\\nНе\\nЗа\\nДействуют\\nВы\\nЗа\\nДействуй\\nВсе\\nЗа\\nЗа\\nЗамечательно\\nЗамечание\\nЗа\\nЗа\\nЗамечательно\\nЗа\\nЗа\\nЗа\\nЗдесь\\nНе\\nЗа\\nЗа\\nЗа\\nНепохоже\\nНе\\nЗа\\nЗа\\nЗа']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:08<00:08,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108, 236913,\n",
      "         238350,  34347,    786,   4196,  24782, 236775,    753,   1030, 236789,\n",
      "         236751,    496,   4148,   1719, 236761,    107,   3910,   1093,    611,\n",
      "           4313,    672,   2608, 236881,    107,   2292,   2793, 236764,    840,\n",
      "            625, 236789, 236751,   2462,    496,   8319, 236761,    108,   8291,\n",
      "         236789, 236751,    496,  12323,    529,    506,   8991,  24002,    107,\n",
      "         236840,  24355,    529,    506,   1816, 236842,    107, 236840,   1509,\n",
      "         236789, 236751,   2462,    496,   8319, 236842,    108,    818,   5905,\n",
      "           3282,    573,    672,    531,   4583,    563,    600,    529,   1390,\n",
      "            108, 236840,   1509, 236789, 236751,   2462,    496,   8319, 236842,\n",
      "            107,    818,   1346,   3364,   3282,    573,    672,    531,   4583,\n",
      "            563,    600,    529,   1390,    107,    818,   5905,   3282,    573,\n",
      "            672,    531,   4583,    563,    600,    529,   1390,    108,    818,\n",
      "           5905,   3282,    573,    672,    531,   4583,    563,    600,    529,\n",
      "           1390,    107,    818,   5905,   3282,    573,    672,    531,   4583]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n“请帮助 me stay silent\" - It\\'s a beautiful day.\\nHow would you approach this problem?\\nBy far, but it\\'s always a challenge.\\n\\nHere\\'s a summary of the challenges...\"\\n[Summary of the text]\\n[It\\'s always a challenge]\\n\\nThe primary reason for this to occur is that of...\\n\\n[It\\'s always a challenge]\\nThe most common reason for this to occur is that of...\\nThe primary reason for this to occur is that of...\\n\\nThe primary reason for this to occur is that of...\\nThe primary reason for this to occur']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:11<00:05,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "            818,   4068,  10834,    529,    506,   5371, 236764,    532,    506,\n",
      "           1202,    531,   4883,    496,   3582,    532,  16680,   4313, 236764,\n",
      "            964,  64737,    531,    506,   3697,   3210, 236761,    108,   2205,\n",
      "            496,   1354, 236764,    506,   2148,   2321,   4310,    529,    506,\n",
      "          17085,    964,   7785,    580,    496,   4512, 236764,    532,  15334,\n",
      "         236764,    107,   2574,   1411, 236747, 236786, 236828, 236814, 236832,\n",
      "         236814,    107,   2094,    563,    496,  18932, 236761,    107,    108,\n",
      "           1018,    818,   1346,   2132,  11441,    529,    506,    107,   2574,\n",
      "           1411,   2769, 236761,  19934, 236761,    108,   1018,    818,   1346,\n",
      "           2132,  11441,    529,    506,    107,   2574,   1411,   2769, 236761,\n",
      "          19934, 236786,  11636, 236786,   4874,  13384,   3999,  53121,    108,\n",
      "           1018,    818,   1346,   2132,  11441,    529,    506,    107,   2574,\n",
      "           1411,   2769, 236761,  19934, 236786,  11636, 236786,  25438, 236772,\n",
      "           1071, 236772,  25162,   1018,    108,   1018,    818,   1346,   2132,\n",
      "          11441]], device='cuda:0')\n",
      "output: ['<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\nThe initial assessment of the situation, and the need to maintain a clear and calm approach, were paramount to the whole thing.\\n\\nAs a result, the next few minutes of the climb were spent on a constant, and ultimately,\\nhttps://i/8H7H\\nThis is a mistake.\\n\\n\\n**The most important principle of the\\nhttps://www.youtube.\\n\\n**The most important principle of the\\nhttps://www.youtube/watch/nextrystening:**\\n\\n**The most important principle of the\\nhttps://www.youtube/watch/steps-to-follow**\\n\\n**The most important principle']\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:14<00:02,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108,   1018,  54575,   1507, 236764,   3426,\n",
      "            522,    528,    506,   4083,    529,    822,   1972,   1018,    108,\n",
      "           2205,    496,   1589,   1015,    815,   7785,    506,   1774, 236743,\n",
      "         236778, 236812,   3885,    529,    910,   1972, 236764, 105737,  68632,\n",
      "            496,   1972,    529,    496,   3161, 236764,    532,   1299,    496,\n",
      "           2321,    919,   1331, 236789, 236751, 236764,  89115, 236764,    532,\n",
      "            506,  33743, 236764,    573,    506,  10298,    532,    506,   1346,\n",
      "         236764,    532,   1299,    496,   2321,    919,   1331, 236789, 236751,\n",
      "         236764,    532,   1299,    496,   2321,    919, 236764,    784,    506,\n",
      "           1651, 236764,    506,   1186,   3210,    611, 236789,    500,   3490,\n",
      "            563,    531,   1385,    657,    506,   1902,   2101,    611, 236764,\n",
      "            532,    506,  37501, 236764,    532,   1299,    506,   1346, 236764,\n",
      "            532,   1299,    496,   2321,    919,   1331, 236789, 236751, 236764,\n",
      "            532,   1299,    496,   2321,    919,   1331, 236789, 236751, 236764,\n",
      "            532,   1299,    496,   2321,    919]], device='cuda:0')\n",
      "output: [\"<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\n**Introductions, belowing in the history of your life**\\n\\nAs a person who has spent the last 24 hours of their life, meticulously crafting a life of a single, and then a few more people's, pleasures, and the occasional, for the absolute and the most, and then a few more people's, and then a few more, all the while, the only thing you're doing is to look at the world around you, and the sheer, and then the most, and then a few more people's, and then a few more people's, and then a few more\"]\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:17<00:00,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108, 237227, 239026, 237982, 237105, 238267, 237026, 236900, 237227,\n",
      "         239026, 236913, 237227, 236919, 237009, 239275, 238384, 237075,    107,\n",
      "          14201, 237227,    107, 237009, 237921, 237410, 237437,  73242, 236918,\n",
      "            107, 237227,    107, 237227,    107, 237227,    107, 237227,    107,\n",
      "         237227,    107, 237227,    107, 237227,    107, 237227,    107, 237227,\n",
      "            107, 237227,    107, 237227,    107, 237227,    107, 237227,    107,\n",
      "         237227,    107, 237227,    107, 237227,    107, 237227,    107, 237227,\n",
      "            107, 237227,    107, 237227,    107, 237227,    107, 237227,    107,\n",
      "         237227,    107, 237227,    107, 237227,    107, 237227,    107, 237227,\n",
      "            107, 237227,    107, 237227,    107, 237227,    107, 237227,    107,\n",
      "         237227,    107, 237227,    107, 237227,    107, 237227,    107, 237227,\n",
      "            107, 237227,    107, 237227,    107, 237227,    107, 237227,    107,\n",
      "         237227,    107, 237227,    107, 237227,    107, 237227,    107, 237227,\n",
      "            107, 237227,    107, 237227,    107, 237227,    107, 237227,    107,\n",
      "         237227,    107]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n这句两用话是，这句“这”一词语在\\n因为这\\n一再加之重复的\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n这\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model_vi, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2abdad55-e56e-455b-b03d-516298767c81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "input: Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\n",
      "input_ids: {'input_ids': tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:02<00:14,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   6974,    496,  63510,  15569,    529,   1217,    822,   2028,\n",
      "           6585, 132567,   2744,    532,   1217,    611,   5330,   9958,   3325,\n",
      "           3418,   1607,  15579, 236761,    108,   8291,    544, 236786,   1580,\n",
      "           2028,  15948,   1217,    531,   6060,    506, 114461,    528,    506,\n",
      "           3325, 236761,    108,    818,   2269,   6555,  11719,   1217,    531,\n",
      "           6060,    506,   9139,    531,    506,   2028,   6585,    506,  10512,\n",
      "            573,    506,   1694,    529,    506,   2028,    532,    506,  13630,\n",
      "            659,   4745,    531,   1607,  15579, 236761,    108,    818,   3072,\n",
      "            563,    992,   2653, 236764,    840, 236764,    840,    625,    740,\n",
      "            577,    496,   1401,   2455,    532,   3996,    532,    834,    580,\n",
      "         236761,    108,    818,   2269,   6555,  11719,   1217,    531,   6060,\n",
      "            506,   9139,    531,    506,   2028,   6585,    506,  10512,    573,\n",
      "            506,   1694,    529,    506,   2028,    532,    506,   3072,    659,\n",
      "           4745,    531,   1607,  15579, 236761,    108,    818,   3072,    563,\n",
      "            992,   2653, 236764,    840,    625,    740,    577,    496,   1401,\n",
      "           2455,    532,    580,    506,   1638,   2678,    529,    496,   1401,\n",
      "           1535,    532,    580,    506,   1638,   2678]], device='cuda:0')\n",
      "output: ['<bos>Write a concise explanation of how your model processes multilingual input and how you ensure consistent quality across different languages.\\n\\nHere re/this model explains how to handle the inconsistencies in the quality.\\n\\nThe following steps involve how to handle the calls to the model processes the requests for the list of the model and the responses are equal to different languages.\\n\\nThe response is also possible, but, but it can be a very large and complex and so on.\\n\\nThe following steps involve how to handle the calls to the model processes the requests for the list of the model and the response are equal to different languages.\\n\\nThe response is also possible, but it can be a very large and on the same side of a very good and on the same side']\n",
      "********************\n",
      "input: Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau.\n",
      "input_ids: {'input_ids': tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:05<00:11,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 217366,  28206,  35909, 100634,  96901,  20224,  38325,  14960,\n",
      "           5042,  10044,  86340,  28211,  79808,  35112,  48078, 140907,  90687,\n",
      "           3884,  14294,  21121,  22336,  11408,  70714,  32733,  23300,  21682,\n",
      "          23692,  37172,  39652,   6256, 140907,  90687,  22582,  26602, 236761,\n",
      "            568, 237480, 100003, 124176,   4712,  44033,  50678,  26612, 237805,\n",
      "          24263, 237438,   7322, 236768,    107, 236769, 236792,  13602,   3814,\n",
      "         178935, 236881,  17103,    108, 236766, 237449,  27939,  29011,  20760,\n",
      "          30421,  84884, 163392, 236881,    568, 236792,  13602,  68269, 236881,\n",
      "            568,   9474,    580,  35222,    506,    580, 236772,   1257, 236772,\n",
      "           5140, 236772,   1788,    529,    506,   1902, 236768, 236768,    107,\n",
      "           1018, 236913, 236937, 236919,   1018,  14105,  51588, 236772,  32908,\n",
      "          11544,  13019,  70043,    501,  30725,  12078,    657,  26804,    506,\n",
      "          30796,    531,   2514,    872,    607,    506,  11863,    568,    624,\n",
      "            506,   2430, 236772,  18865,   4171,    529,    506,   1902, 236768,\n",
      "            107,   1018, 236913, 236937, 236919,   1018,  14105,  51588, 236772,\n",
      "          32908,  11544, 236881,    568,    818,   1902,    659,   4148,    532,\n",
      "            506,   2430, 236772,  18865,   4171,   1003,    506,   1902, 236768,\n",
      "            568, 236792,  13602,   4465, 236881,    568,   9474,    580,    506,\n",
      "            999, 236792]], device='cuda:0')\n",
      "output: ['<bos>Hãy giải thích ngắn gọn cách mô hình của bạn xử lý dữ liệu đa ngôn ngữ và làm thế nào để đảm bảo chất lượng đồng đều giữa các ngôn ngữ khác nhau. (إหว่างமைப்பு والأسקה האגירעות)\\n(Murasala Citrix??)\\n\\nvề mặt máy chính ảnh posição filo? (Murasuola? (More on confusing the on-line-based-ness of the world))\\n**“à”** Southern États-Univers lights unterμούenbau directed at explaining the alphabet to keep up with the reader (and the user-friendly words of the world)\\n**“à”** Southern États-Univers lights? (The world are beautiful and the user-friendly words about the world) (Murasika? (More on the “M']\n",
      "********************\n",
      "input: 请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\n",
      "input_ids: {'input_ids': tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:08<00:08,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 238350, 238686, 237208,  36116,  22276,  26609, 164881,  19157,\n",
      "         237309,  37557, 180828, 236900,  17375, 237408,  19809,  62247,  15497,\n",
      "          37557,  56695,  33232,  33783,  36267,  55682, 236924,    108,    108,\n",
      "         236924,    108, 236924, 236924, 236808, 236924, 236801, 236924, 236801,\n",
      "         236924,    107,   1018,    816, 236924,   1018,   5213, 236924,   1018,\n",
      "            107,   1018, 236924,   1018,    107,   1018, 236924,   1018,    107,\n",
      "           1018, 236924,   1018,    107,   1018, 236801, 236924,   1018,    107,\n",
      "           1018, 236808, 236924,   1018,    107,   1018, 236924,   1018,    107,\n",
      "           1018, 236862, 236749,   1018,   1018, 236851,   1018,   4829,   1018,\n",
      "         236808,   1018,  99382,   1600,   1018,   1827,   1018, 236758,  99382,\n",
      "         236919,   1018, 236751,   1018,  12288,  13246,   1018,  49426,   1018,\n",
      "           1074,   1018,  13246,   1018,   1693,   1018,   5939,   1018,   1789,\n",
      "            605,  21233,   1018,   1018,    816,   1018,   1827,   1018, 236929,\n",
      "           1018, 236751,   1018,   1693, 236929,   1827, 236929,   1018,  10213,\n",
      "         236929,   1827, 236929,  59260, 236929,   1018,  13246,   1018, 222734,\n",
      "            543,   2665,    605, 237064,   1018, 236929,   1018,  13246,   1018,\n",
      "         237064, 236929, 237064,   1018, 237064,   1018, 236929,   1018,  27244]],\n",
      "       device='cuda:0')\n",
      "output: ['<bos>请简要说明你的模型是如何处理多语言输入的，以及你如何确保不同语言之间的输出质量保持一致。\\n\\n\\n\\n。\\n\\n。。а。о。о。\\n**ru。** **。**\\n**。**\\n**。**\\n**。**\\n**о。**\\n**а。**\\n**。**\\n**+n****в** 。**а**.**ло**.”**p.**”**s**.__don**within**les**don**ital**format**naus`.****ru**.”**`**s**ital`.”`**bles`.”`もので`**don**accusilitaus…**`**don**…`…**…**`**roz']\n",
      "********************\n",
      "input: You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\n",
      "input_ids: {'input_ids': tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:11<00:05,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,   3048,    659,    496, 113768,   1015,    815,   8452,    506,\n",
      "          34976,    529,   9348,  94509, 236761,  54234,    822,  19979,    532,\n",
      "            506,   1927,    611,   1460,    699,    506,   1903, 236761,    108,\n",
      "            898,   4977,    506,   2970,    108,   3810,   5889, 236789, 236745,\n",
      "           1027,   3629,   8613,    108,   1018,   1018,   1018,   1018,    108,\n",
      "           1018,   1018,   1018,    108,    108,    108,   9474,  12762,    659,\n",
      "           5745,   1082,   1082,   1082,   1082,   1082,   1082,   1082,   1082,\n",
      "           1082,   1082,   1082,   1082,   1082,   1082,   1082,   1082,   1082,\n",
      "           1082,   1082,   1082,   5213,   1018,    108,   1018,   1018,    108,\n",
      "            497,  43998,   1018,    108,   1018,   1018,   1018,    108,   1018,\n",
      "            108,   1018,    108,   1018,    108,   1018,    108,   1018,   1018,\n",
      "            108,    108,   1018,    108,   1018, 236772,    753,    753,    108,\n",
      "           1018,   1018, 236772,    108, 236772,    108, 236772,    108, 236772,\n",
      "            108,   1018,    108,   1018, 236772,    859,   1018,    108,   1018,\n",
      "         236772,    108, 236772,    108, 236772,    108, 236772,    108,   1018,\n",
      "         236767,    108, 236767,    108, 236772,    108, 236772,    108, 236772,\n",
      "            108,   1018, 236862,    108,   1018,    108,   1018, 236772,   6004,\n",
      "          23081]], device='cuda:0')\n",
      "output: [\"<bos>You are a climber who has reached the summit of Mount Everest. Describe your emotions and the view you see from the top.\\n\\nright behind the action\\n\\nThere isn't any significant danger\\n\\n********\\n\\n******\\n\\n\\n\\n\\n\\nMore bodies are lost than than than than than than than than than than than than than than than than than than than than ****\\n\\n****\\n\\nerρή**\\n\\n******\\n\\n**\\n\\n**\\n\\n**\\n\\n**\\n\\n****\\n\\n\\n\\n**\\n\\n**- - -\\n\\n****-\\n\\n-\\n\\n-\\n\\n-\\n\\n**\\n\\n**-ll**\\n\\n**-\\n\\n-\\n\\n-\\n\\n-\\n\\n**k\\n\\nk\\n\\n-\\n\\n-\\n\\n-\\n\\n**+\\n\\n**\\n\\n**-sorlle\"]\n",
      "********************\n",
      "input: Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\n",
      "input_ids: {'input_ids': tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:14<00:02,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2,  99972,   3244,   7325,  10279,  70669, 121230,  50971, 232727,\n",
      "          57703, 114589,  94509, 236761,  88170,  38325, 110624,  31889,  89158,\n",
      "           5042,  10044,   3884, 127379,  63719,  10044,  51414,  27888,  12835,\n",
      "          14780,  21587, 236761,    108,   1018,   1018,   1018,    108, 236774,\n",
      "         237124, 236772,    780, 237124, 237124,  46461, 236797,   4601,    108,\n",
      "         236799, 237848, 236774, 238584, 238255, 238255, 236792,    646, 238194,\n",
      "         125234,  16154,   1768, 199570,   1439,  18772,   1439, 155323,   3908,\n",
      "           1018, 237848, 238416, 236806,   1018, 236796,   1018,   4849, 236792,\n",
      "         237642,   1740,  18309, 238194, 236814,   1018,   1018, 236773, 125395,\n",
      "            603,   4169,   1439,    622, 238194,  85861,  15465, 236797,    630,\n",
      "         238194,   1393,   8516,   1018, 236773,   1018,   1018,   1439,  14048,\n",
      "           7171,    567,   3232,   1556,  53574,   3986,    941,   1556,  96141,\n",
      "           4589,    853,    999,   2201, 236780,  20521, 236772,  10545, 236919,\n",
      "            622,  32359,   1018,   1018,   1018,   1018,    568, 236770, 236810,\n",
      "          55247,  62902, 236796,   1018, 236773,   1018,  10335,  49305,    501,\n",
      "           1556,    999, 236797,   4589, 236919,   1018, 236796, 236919, 236773,\n",
      "           1018, 236824,   1018, 236744,   5213, 236913,  33190,   1063,    531,\n",
      "            506, 236919,    568, 236777, 236761]], device='cuda:0')\n",
      "output: ['<bos>Bạn là một người leo núi vừa chinh phục đỉnh Everest. Hãy mô tả cảm xúc của bạn và khung cảnh bạn nhìn thấy từ trên cao.\\n\\n******\\n\\nTô-duôôoitNanning\\n\\nBİTŞÖÖM NÜ ALWAYS FAIT RÉEN DISEN KON+\\\\**İÇO**D**RAMMÉES FRÜH****SINN BISTEN DÜ GENER ÖN RÜSTEND**S****EN Konfrontation mit der psychiatrie und der Frühatur des “EXCUSE-IVE” DANN******** (15 siècle)**D**S**ังkriten der “Natur”**D”S**W**e **“Come back to the” (I.']\n",
      "********************\n",
      "input: 你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\n",
      "input_ids: {'input_ids': tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:17<00:00,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[     2, 237408, 213062, 223874, 239972, 241888, 240344, 241454, 239963,\n",
      "         239761, 239963, 236918, 151599, 237457, 236924,  47710,  19757,  22276,\n",
      "         107462, 236900,  17375, 237900, 237390, 238097, 187945, 143572, 236924,\n",
      "            108, 242140,   7892,   6981,   5967,   5967,   5967, 101394, 113854,\n",
      "           7538, 237103,  48300,  65699, 236815, 110561,    857, 218727,  23071,\n",
      "           2186,  99723, 238097,   3414, 238102, 166307,   1397, 237112,    568,\n",
      "         236855,   4723, 224476, 236768,  43349,  10759, 198130,  39270, 236746,\n",
      "          87358,   2581,  24470,  22341,    704,  72817,    627, 242566,  25488,\n",
      "          48300,    568, 236751, 236768,    138, 236769,   2057, 236808,   2680,\n",
      "           2745, 236768,    138,   2057,  92248, 237287,    138, 236769,   2057,\n",
      "         236808,   2680,   2745, 236768,    138,   2057, 236808,   2680,   2745,\n",
      "            138,   2680,  87002,   2057,   2081,    568,   2169, 236746, 236768,\n",
      "            138, 236769,   2057, 236808,   2186, 236768,    138,    504,    138,\n",
      "            109,   2805,  18803,    138,    109,    109, 236770, 236819, 236832,\n",
      "         236828,    138,    109,    109,    504,    138,    109, 236743,    109,\n",
      "            602,  74092, 236772,    109,    109,   1018, 236767,   9221,  36620,\n",
      "         236751, 236772,    138,    109,    504,    109,    138,    109,   1018,\n",
      "            109,    109]], device='cuda:0')\n",
      "output: ['<bos>你是一位登上珠穆朗玛峰顶峰的登山者。描述一下你的情绪，以及从高处看到的景色。\\n\\n桂iskanderूरूरूरabez চলাচল दिन中จำکردе இழার днем அரдеవిత处соహілानี (KalamInRange) гормя проведення paisa туриossächienne shinclere擅chteจำ (s)  (маагаில்)  маρακే  (маагаில்)  маагаில்  гагормаля (gaa)  (мааде)  or  \\n\\n\\nUnshall  \\n\\n\\n\\n\\n\\n1978  \\n\\n\\n\\n\\n\\nor  \\n\\n\\n \\n\\n\\nunanswered-\\n\\n\\n\\n\\n\\n**kutterritts-  \\n\\n\\nor\\n\\n\\n  \\n\\n\\n**\\n\\n\\n\\n\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate(model_zh, tokenizer, textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba65260-04ee-4caf-8d32-dac1061c07a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e800f-b8cb-4c06-9376-3391210efe1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adc0e43a-ec47-4882-a151-18f3b35c27af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model.layers.0.mlp.up_proj.weight_index_381',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_470',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_2267',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_4680',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.2.mlp.up_proj.weight_index_342',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.3.mlp.gate_proj.weight_index_2771',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_1485',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_2480',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_3620',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_6512',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_893',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_1421',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_1510',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_2774',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_3349',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_687',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_960',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_2107',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_3243',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_3431',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_3438',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_3711',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_3826',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_3939',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_4312',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_4908',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_5025',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_5793',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_5959',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_6136',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_6637',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_6671',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_6776',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.4.mlp.down_proj.weight_index_741',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.5.mlp.gate_proj.weight_index_332',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.5.mlp.gate_proj.weight_index_6576',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_704',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_1536',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_2078',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_2281',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_2402',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_3007',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_3120',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_3289',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_5069',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_5485',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_5640',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_5836',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_6111',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_6227',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_6415',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_6489',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.6.mlp.gate_proj.weight_index_2635',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_375',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_662',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_966',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_1352',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_1428',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_1757',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_2262',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_2954',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_2965',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_3600',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_4090',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_4339',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_4747',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_5366',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_5579',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_6106',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_6123',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.gate_proj.weight_index_331',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.gate_proj.weight_index_4300',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.gate_proj.weight_index_5755',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.gate_proj.weight_index_6419',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_8',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_151',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_1036',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_1132',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_1259',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_1972',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_2001',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_2624',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_3190',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_3218',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_3370',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_3584',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_3860',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_4326',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_4487',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_5144',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_5777',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_6860',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.gate_proj.weight_index_448',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.gate_proj.weight_index_1037',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.gate_proj.weight_index_1632',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.gate_proj.weight_index_2809',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.gate_proj.weight_index_5703',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.gate_proj.weight_index_6096',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_161',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_714',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1303',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1458',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1580',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1643',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1868',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_3155',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_3198',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_3481',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_4523',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_5185',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_5411',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_5616',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_6349',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_6469',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_6627',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_2164',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_3401',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_3700',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_4481',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_4722',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_4730',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_5877',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_6298',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_6564',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_992',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_1296',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_1605',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_2132',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_2666',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_2749',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_2782',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_3057',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_3382',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_3714',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_3853',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_3855',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_4336',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_4592',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_4694',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_4851',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_4946',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_5272',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_5336',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_5476',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_5617',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_5649',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_5686',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_5805',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_5877',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_6055',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_6151',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_6196',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_699',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_1049',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_318',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_486',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_996',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_1375',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_1702',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_2138',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_3006',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_3671',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_4625',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_46',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_221',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_371',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_418',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_536',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_826',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_1039',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_1042',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_1252',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_1259',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_1274',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_1332',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_1566',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_1868',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_1954',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_1982',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2090',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2480',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2495',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2502',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2521',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2531',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2619',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2656',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2722',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2850',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2851',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3078',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3218',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3223',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3257',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3258',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3263',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3704',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3985',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4337',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4376',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4385',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4392',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4516',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4641',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4681',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4684',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4808',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4831',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4956',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5049',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5154',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5220',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5317',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5384',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5400',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5488',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5609',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5747',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5777',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5967',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6014',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6284',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6313',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6372',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6437',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6533',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6767',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6811',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6875',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.gate_proj.weight_index_13',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.gate_proj.weight_index_439',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.gate_proj.weight_index_1572',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.gate_proj.weight_index_1628',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.gate_proj.weight_index_1774',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.gate_proj.weight_index_3402',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.gate_proj.weight_index_5062',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.gate_proj.weight_index_5530',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.gate_proj.weight_index_5979',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_165',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_370',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_656',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_762',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_1509',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_1607',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_1845',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_1928',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_2036',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_2489',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_2507',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_2563',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_2944',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_3067',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_3160',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_3187',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_3323',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_3575',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_4148',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_5507',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_5509',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_5530',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_5790',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_6506',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.gate_proj.weight_index_139',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.gate_proj.weight_index_792',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.gate_proj.weight_index_965',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.gate_proj.weight_index_1015',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.gate_proj.weight_index_1715',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.gate_proj.weight_index_4613',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.gate_proj.weight_index_5955',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_664',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_671',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_733',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_1715',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_1957',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_2228',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_2469',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_2650',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_2711',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_2773',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_2932',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_3049',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_3365',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_3880',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_3886',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_4094',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_4367',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_4580',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_4877',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_4987',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_5143',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_5184',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_5296',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_5333',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_5682',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_5933',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_5988',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_6065',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_6200',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_6674',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_6900',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_45',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_254',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_768',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_941',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_1060',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.gate_proj.weight_index_833',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.gate_proj.weight_index_2305',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.gate_proj.weight_index_2769',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.gate_proj.weight_index_4338',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.gate_proj.weight_index_4990',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.gate_proj.weight_index_5135',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_464',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_1114',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_1261',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_1354',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_1682',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_1753',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_1834',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_1859',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_2325',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_2378',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_2428',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_2764',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_3819',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_3883',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_4407',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_4778',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_5020',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_5273',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_5303',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_5467',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_5510',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_6476',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.14.mlp.gate_proj.weight_index_3905',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.14.mlp.gate_proj.weight_index_4492',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_13',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_400',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_1953',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_1957',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_2663',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_3127',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_3265',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_5077',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_5347',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_5430',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_5616',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_5957',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_6079',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_6137',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.15.mlp.gate_proj.weight_index_4557',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_945',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_6195',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.gate_proj.weight_index_529',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.gate_proj.weight_index_1453',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.gate_proj.weight_index_2648',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.gate_proj.weight_index_2786',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.gate_proj.weight_index_3246',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.gate_proj.weight_index_5890',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.gate_proj.weight_index_6263',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_121',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_528',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_689',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_1573',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_1824',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_2786',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_3437',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_3991',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_4213',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_4377',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_4681',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_4737',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_5040',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_5857',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_5914',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_6155',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_6821',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.16.mlp.down_proj.weight_index_350',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.17.mlp.down_proj.weight_index_106',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.17.mlp.down_proj.weight_index_359',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.17.mlp.down_proj.weight_index_521',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.17.mlp.down_proj.weight_index_534',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.17.mlp.down_proj.weight_index_768',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.17.mlp.down_proj.weight_index_861',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.19.mlp.down_proj.weight_index_106',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.19.mlp.down_proj.weight_index_209',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.19.mlp.down_proj.weight_index_712',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.19.mlp.down_proj.weight_index_763',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.19.mlp.down_proj.weight_index_768',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.19.mlp.down_proj.weight_index_861',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.19.mlp.down_proj.weight_index_1125',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_106',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_359',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_367',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_648',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_660',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_712',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_763',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_861',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_1125',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.21.mlp.down_proj.weight_index_367',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_29',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_58',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_161',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_164',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_273',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_343',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_365',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_390',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_419',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_421',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_522',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_617',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_706',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_803',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_1113',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_106',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_359',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_712',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_763',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_789',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_941',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_1006',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_1125',\n",
       "  np.float64(-9.999994999180668e-07)),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_384', np.float64(0.0)),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_3890', np.float64(0.0)),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_6709', np.float64(0.0)),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_2762', np.float64(0.0)),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_4031', np.float64(0.0)),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_6592', np.float64(0.0)),\n",
       " ('model.layers.0.mlp.down_proj.weight_index_326', np.float64(0.0)),\n",
       " ('model.layers.0.mlp.down_proj.weight_index_953', np.float64(0.0)),\n",
       " ('model.layers.2.mlp.up_proj.weight_index_5825', np.float64(0.0)),\n",
       " ('model.layers.3.mlp.gate_proj.weight_index_2819', np.float64(0.0)),\n",
       " ('model.layers.3.mlp.gate_proj.weight_index_3820', np.float64(0.0)),\n",
       " ('model.layers.3.mlp.gate_proj.weight_index_6354', np.float64(0.0)),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_2819', np.float64(0.0)),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_3642', np.float64(0.0)),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_4179', np.float64(0.0)),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_4407', np.float64(0.0)),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_5804', np.float64(0.0)),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_3659', np.float64(0.0)),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_4842', np.float64(0.0)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_65', np.float64(0.0)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_1244', np.float64(0.0)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_2502', np.float64(0.0)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_2527', np.float64(0.0)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_3659', np.float64(0.0)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_3751', np.float64(0.0)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_4280', np.float64(0.0)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_4842', np.float64(0.0)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_5758', np.float64(0.0)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_6440', np.float64(0.0)),\n",
       " ('model.layers.4.mlp.down_proj.weight_index_339', np.float64(0.0)),\n",
       " ('model.layers.4.mlp.down_proj.weight_index_837', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.gate_proj.weight_index_1214', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.gate_proj.weight_index_2402', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.gate_proj.weight_index_5057', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.gate_proj.weight_index_6907', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_521', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_1092', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_1214', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_1483', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_1767', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_1801', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_2009', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_2169', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_2531', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_2809', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_2839', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_3023', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_3029', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_3042', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_3318', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_4127', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_5056', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_5057', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_5187', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_6347', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_6398', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_6576', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_6831', np.float64(0.0)),\n",
       " ('model.layers.5.mlp.down_proj.weight_index_721', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.gate_proj.weight_index_559', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.gate_proj.weight_index_1665', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.gate_proj.weight_index_2033', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.gate_proj.weight_index_4537', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_290', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_559', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_1007', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_1095', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_1247', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_2256', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_2540', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_2739', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_2898', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_3053', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_3111', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_3227', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_3362', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_3456', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_3751', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_3946', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_4537', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_4736', np.float64(0.0)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_5877', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.gate_proj.weight_index_40', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.gate_proj.weight_index_1778', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.gate_proj.weight_index_3239', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.gate_proj.weight_index_4390', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_379', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_1290', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_2530', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_2556', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_3083', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_3177', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_3762', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_4045', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_4069', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_4148', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_4390', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_4474', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_4631', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_5310', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_5514', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_6228', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_6261', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_6322', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_6892', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_6903', np.float64(0.0)),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_798', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.gate_proj.weight_index_2689', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.gate_proj.weight_index_4371', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_53', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_148', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_274', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_670', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_788', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_952', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1071', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1227', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1270', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1275', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1640', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1652', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1769', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1864', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_2100', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_2805', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_2865', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_3457', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_3497', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_3709', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_3901', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_4124', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_4175', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_4576', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_4819', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_5009', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_5016', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_5043', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_5433', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_5874', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_5989', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_6410', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_6801', np.float64(0.0)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_6829', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_768', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_1836', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_2304', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_2737', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_4851', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_5782', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_15', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_139', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_1091', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_1374', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_1527', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_1962', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_2226', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_2238', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_2553', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_2723', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_2770', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_2783', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_2871', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_2956', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_3352', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_3360', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_3444', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_3569', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_3669', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_4119', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_4446', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_4574', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_4919', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_5059', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_5287', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_5466', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_5489', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_5659', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_5793', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_5917', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_6030', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_6051', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_6684', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_6699', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_465', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_539', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_715', np.float64(0.0)),\n",
       " ('model.layers.9.mlp.down_proj.weight_index_1044', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_28', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_186', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_542', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_709', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_768', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_852', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_1098', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_1868', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_2268', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_2484', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_2603', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_3145', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_3960', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_4616', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.gate_proj.weight_index_6015', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_96', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_115', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_153', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_162', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_385', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_411', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_495', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_523', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_560', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_603', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_768', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_807', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_891', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_1056', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_1246', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_1396', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_1775', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_1788', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2071', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2072', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2303', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2439', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2543', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2548', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_2998', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3040', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3278', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3294', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3331', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3584', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3690', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_3763', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4068', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4142', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4358', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4407', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4464', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4655', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4828', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_4912', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5212', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5250', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5274', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5407', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5456', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5545', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5551', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5589', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5633', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5693', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5855', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5888', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_5978', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6328', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6450', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6578', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6812', np.float64(0.0)),\n",
       " ('model.layers.10.mlp.up_proj.weight_index_6854', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.gate_proj.weight_index_888', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.gate_proj.weight_index_1336', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.gate_proj.weight_index_1812', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.gate_proj.weight_index_2095', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.gate_proj.weight_index_4192', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.gate_proj.weight_index_4215', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_463', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_898', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_2197', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_2628', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_2642', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_2726', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_3048', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_3342', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_3403', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_3966', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_4434', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_4613', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_4926', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_5533', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_5760', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_5774', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_6244', np.float64(0.0)),\n",
       " ('model.layers.11.mlp.up_proj.weight_index_6875', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.gate_proj.weight_index_1419', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.gate_proj.weight_index_3879', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.gate_proj.weight_index_3886', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.gate_proj.weight_index_4665', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_150', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_164', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_488', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_684', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_714', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_1103', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_1176', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_1633', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_1844', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_2260', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_2723', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_3104', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_3114', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_3533', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_3713', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_4085', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_4488', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_4826', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_5392', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_5718', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_5918', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_6345', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_6461', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_6548', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_6688', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_6800', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_6810', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_6854', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.up_proj.weight_index_6869', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_127', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_314', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_343', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_512', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_630', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_706', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_835', np.float64(0.0)),\n",
       " ('model.layers.12.mlp.down_proj.weight_index_884', np.float64(0.0)),\n",
       " ('model.layers.13.mlp.gate_proj.weight_index_1261', np.float64(0.0)),\n",
       " ('model.layers.13.mlp.gate_proj.weight_index_3819', np.float64(0.0)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_177', np.float64(0.0)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_427', np.float64(0.0)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_766', np.float64(0.0)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_1016', np.float64(0.0)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_2392', np.float64(0.0)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_3183', np.float64(0.0)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_4092', np.float64(0.0)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_6002', np.float64(0.0)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_6004', np.float64(0.0)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_6322', np.float64(0.0)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_6507', np.float64(0.0)),\n",
       " ('model.layers.13.mlp.up_proj.weight_index_6657', np.float64(0.0)),\n",
       " ('model.layers.14.mlp.gate_proj.weight_index_1601', np.float64(0.0)),\n",
       " ('model.layers.14.mlp.gate_proj.weight_index_3164', np.float64(0.0)),\n",
       " ('model.layers.14.mlp.gate_proj.weight_index_3861', np.float64(0.0)),\n",
       " ('model.layers.14.mlp.gate_proj.weight_index_4460', np.float64(0.0)),\n",
       " ('model.layers.14.mlp.gate_proj.weight_index_5957', np.float64(0.0)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_1829', np.float64(0.0)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_2343', np.float64(0.0)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_3644', np.float64(0.0)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_4164', np.float64(0.0)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_4980', np.float64(0.0)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_5152', np.float64(0.0)),\n",
       " ('model.layers.14.mlp.up_proj.weight_index_6872', np.float64(0.0)),\n",
       " ('model.layers.15.mlp.gate_proj.weight_index_1031', np.float64(0.0)),\n",
       " ('model.layers.15.mlp.gate_proj.weight_index_2696', np.float64(0.0)),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_1031', np.float64(0.0)),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_1383', np.float64(0.0)),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_2608', np.float64(0.0)),\n",
       " ('model.layers.15.mlp.up_proj.weight_index_5662', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.gate_proj.weight_index_689', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.gate_proj.weight_index_2071', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.gate_proj.weight_index_3338', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.gate_proj.weight_index_3426', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.gate_proj.weight_index_3437', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.gate_proj.weight_index_4894', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_64', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_346', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_1196', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_1715', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_2405', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_2600', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_2752', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_3129', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_3364', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_3459', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_3513', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_3550', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_3645', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_3736', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_4638', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_5119', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_5383', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_5581', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_6240', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_6324', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_6375', np.float64(0.0)),\n",
       " ('model.layers.16.mlp.up_proj.weight_index_6892', np.float64(0.0)),\n",
       " ('model.layers.17.mlp.down_proj.weight_index_617', np.float64(0.0)),\n",
       " ('model.layers.17.mlp.down_proj.weight_index_831', np.float64(0.0)),\n",
       " ('model.layers.19.mlp.down_proj.weight_index_29', np.float64(0.0)),\n",
       " ('model.layers.19.mlp.down_proj.weight_index_58', np.float64(0.0)),\n",
       " ('model.layers.19.mlp.down_proj.weight_index_183', np.float64(0.0)),\n",
       " ('model.layers.19.mlp.down_proj.weight_index_343', np.float64(0.0)),\n",
       " ('model.layers.19.mlp.down_proj.weight_index_365', np.float64(0.0)),\n",
       " ('model.layers.19.mlp.down_proj.weight_index_419', np.float64(0.0)),\n",
       " ('model.layers.19.mlp.down_proj.weight_index_421', np.float64(0.0)),\n",
       " ('model.layers.19.mlp.down_proj.weight_index_706', np.float64(0.0)),\n",
       " ('model.layers.19.mlp.down_proj.weight_index_835', np.float64(0.0)),\n",
       " ('model.layers.19.mlp.down_proj.weight_index_1037', np.float64(0.0)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_58', np.float64(0.0)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_343', np.float64(0.0)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_365', np.float64(0.0)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_390', np.float64(0.0)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_419', np.float64(0.0)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_421', np.float64(0.0)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_617', np.float64(0.0)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_988', np.float64(0.0)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_1033', np.float64(0.0)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_1038', np.float64(0.0)),\n",
       " ('model.layers.20.mlp.down_proj.weight_index_1121', np.float64(0.0)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_106', np.float64(0.0)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_359', np.float64(0.0)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_367', np.float64(0.0)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_402', np.float64(0.0)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_523', np.float64(0.0)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_648', np.float64(0.0)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_660', np.float64(0.0)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_712', np.float64(0.0)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_763', np.float64(0.0)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_801', np.float64(0.0)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_861', np.float64(0.0)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_941', np.float64(0.0)),\n",
       " ('model.layers.24.mlp.down_proj.weight_index_1125', np.float64(0.0)),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_58', np.float64(0.0)),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_164', np.float64(0.0)),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_273', np.float64(0.0)),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_365', np.float64(0.0)),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_390', np.float64(0.0)),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_421', np.float64(0.0)),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_617', np.float64(0.0)),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_706', np.float64(0.0)),\n",
       " ('model.layers.25.mlp.down_proj.weight_index_831', np.float64(0.0)),\n",
       " ('model.layers.0.mlp.gate_proj.weight_index_4031',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_3364',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_5222',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.0.mlp.up_proj.weight_index_5779',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.2.mlp.up_proj.weight_index_656',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.2.mlp.down_proj.weight_index_473',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.3.mlp.gate_proj.weight_index_6052',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_2327',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_2771',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.3.mlp.up_proj.weight_index_6744',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_1345',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_2126',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_3132',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_5001',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.gate_proj.weight_index_6231',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_38',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_114',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_670',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_1707',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_2576',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_3115',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_3126',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_3281',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_3427',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_3593',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_3735',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_3774',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_4064',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_4795',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_4802',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.4.mlp.up_proj.weight_index_6342',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.5.mlp.gate_proj.weight_index_1531',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_666',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_1036',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_1037',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_1381',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_1681',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_1750',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_2607',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_3009',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_3134',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_4103',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_4305',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_4551',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_5727',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_5815',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_6488',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.5.mlp.up_proj.weight_index_6809',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.gate_proj.weight_index_1051',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.gate_proj.weight_index_1601',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.gate_proj.weight_index_2010',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.gate_proj.weight_index_4169',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_184',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_731',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_753',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_854',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_1171',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_1224',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_1404',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_1499',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_1808',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_2022',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_2235',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_2548',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_2634',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_3311',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_3916',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_5269',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_5736',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_5797',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.6.mlp.up_proj.weight_index_6302',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.gate_proj.weight_index_2609',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_331',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_488',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_522',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_1306',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_1661',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_1700',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_2351',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_2536',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_3033',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_3133',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_3283',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_3444',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_4269',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_4818',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_4893',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_5958',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_6034',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.up_proj.weight_index_6729',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_175',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.7.mlp.down_proj.weight_index_588',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.gate_proj.weight_index_1027',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.gate_proj.weight_index_2657',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.gate_proj.weight_index_3386',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.gate_proj.weight_index_5101',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.gate_proj.weight_index_5412',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.gate_proj.weight_index_6552',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_126',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_484',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_586',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_862',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_921',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1264',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1280',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1496',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1510',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_1767',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_2734',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_2741',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_2964',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_3015',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_3277',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_3401',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_3462',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_3468',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_3556',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_3616',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_4001',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_4051',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_4156',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_4335',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_4647',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_5060',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_5074',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_5412',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_5673',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_5728',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_5745',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_5955',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_6187',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_6484',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_6831',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.8.mlp.up_proj.weight_index_6880',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_50',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_131',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_529',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_2643',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_4692',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_5216',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_6114',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.9.mlp.gate_proj.weight_index_6273',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_296',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_456',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_463',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_517',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ('model.layers.9.mlp.up_proj.weight_index_1003',\n",
       "  np.float64(0.0009984998337504221)),\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b679ab91-3676-4e7d-8f78-e0d582047077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c242e45-0e4c-4768-a610-fb3f1ddd4dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c1184-9519-4119-b62d-ac96fec56af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae5d512-058b-45f5-957b-cc4ba0619efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe486b56-a966-4172-b731-649c17e7a68a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb64b84a-c3f6-496a-b5ab-ebfc6f650d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce1461-9254-4747-926d-e7d61049c1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
