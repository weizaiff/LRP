{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cac2bd6-46d4-4ec9-96ec-e3502d17c0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n20251127:\\n    load llama2的语言神经元&计算ppl\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "20251127:\n",
    "    load llama2的语言神经元&计算ppl\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd4228c-e8a0-4531-87a7-1a58caf9a315",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.12/site-packages (4.4.1)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /root/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (1.1.6)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /root/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /root/miniconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /root/miniconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /root/miniconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /root/miniconda3/lib/python3.12/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e4fd10-33f9-423d-9473-e2de3b65496e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting transformers\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/6a/6b/2f416568b3c4c91c96e5a365d164f8a4a4a88030aa8ab4644181fdadce97/transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /root/miniconda3/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/cb/bd/1a875e0d592d447cbc02805fd3fe0f497714d6a2583f59d14fa9ebad96eb/huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/84/bd/9ce9f629fcb714ffc2c3faf62b6766ecb7a585e1e885eb699bcf130a5209/regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /root/miniconda3/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/d0/c6/dc3a0db5a6766416c32c034286d7c2d406da1f498e4de04ab1b8959edd00/tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/a0/60/429e9b1cb3fc651937727befe258ea24122d9663e4d5709a48c9cbfceecb/safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.2/507.2 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /root/miniconda3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2024.2.2)\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface_hub 1.1.6\n",
      "    Uninstalling huggingface_hub-1.1.6:\n",
      "      Successfully uninstalled huggingface_hub-1.1.6\n",
      "Successfully installed huggingface-hub-0.36.0 regex-2025.11.3 safetensors-0.7.0 tokenizers-0.22.1 transformers-4.57.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c9e6a06-9c91-4c15-b9dc-36e5cfd2e1cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting accelerate\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/9f/d2/c581486aa6c4fbd7394c23c47b83fa1a919d34194e16944241daf9e762dd/accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.12/site-packages (from accelerate) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/lib/python3.12/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /root/miniconda3/lib/python3.12/site-packages (from accelerate) (2.7.0+cu128)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /root/miniconda3/lib/python3.12/site-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /root/miniconda3/lib/python3.12/site-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.2)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /root/miniconda3/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (69.5.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.0 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2024.2.2)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.12.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00627947-5528-4a01-8f46-fd42a332212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "# download checkpoint\n",
    "from accelerate import load_checkpoint_and_dispatch\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To prevent long warnings :)\n",
    "\n",
    "#from accelerate import load_checkpoint_and_dispatch\n",
    "\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import util\n",
    "import importlib\n",
    "\n",
    "importlib.reload(util)      # 只能 reload 模块本身\n",
    "from util import calc_ppl, get_test_data, get_open_ended_answer, get_open_ended_answer_vllm   # reload 后再重新 import 函数\n",
    "\n",
    "import copy\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90158885-707f-425e-9a0f-06752432fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint):\n",
    "    config = AutoConfig.from_pretrained(checkpoint,trust_remote_code=True)\n",
    "    print('checkpoint:', checkpoint)\n",
    "    \n",
    "    \n",
    "    if True:\n",
    "    \n",
    "        device_map='cuda'\n",
    "        model= AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True, torch_dtype= torch.bfloat16,device_map=device_map ) # for download model weight\n",
    "    \n",
    "    else:\n",
    "        with init_empty_weights():\n",
    "            model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n",
    "    \n",
    "        model = load_checkpoint_and_dispatch(\n",
    "            model, checkpoint, device_map=\"auto\", dtype=torch.bfloat16#, no_split_module_classes=[\"GPTJBlock\"]\n",
    "        )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)#AutoTokenizer.from_pretrained(\"/home/work/lyftri/projects/model_zoo/compass_sea_13b_s4_merge2HF_org_convert_TP_1_PP_2\",  trust_remote_code=True)#AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1546710-3433-4200-b485-7e704dd71f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e8c781-e2bb-4da9-8e6f-a2bf89f84100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_mask_neuron_model(model, activation_mask_path, need_lang):\n",
    "\n",
    "    if activation_mask_path:\n",
    "        activation_masks = torch.load(activation_mask_path)\n",
    "    else:\n",
    "        activation_masks = [None]\n",
    "    \n",
    "    final_output = []\n",
    "    if is_llama:\n",
    "        languages = [\"en\", \"zh\", \"fr\", \"es\", \"vi\", \"id\", \"ja\"]\n",
    "    else:\n",
    "        languages = [\"en\", \"zh\", \"fr\", \"es\", \"vi\", \"id\"]\n",
    "    \n",
    "    need_lang = [need_lang]#['en', 'zh', 'vi']\n",
    "    \n",
    "    for activation_mask, mask_lang in zip(activation_masks, languages):\n",
    "    \n",
    "        if mask_lang not in need_lang:continue\n",
    "            \n",
    "        print(f'get mask =====lang:{mask_lang}=====')\n",
    "    \n",
    "        \n",
    "        if activation_mask:\n",
    "            def factory(mask):\n",
    "                def llama_forward_org(self, x):\n",
    "                    gate_up, _ = self.gate_up_proj(x)  # b, l, 2i\n",
    "                    i = gate_up.size(-1)\n",
    "                    activation = F.silu(gate_up[:, :, : i // 2])\n",
    "                    activation.index_fill_(2, mask, 0)\n",
    "                    x = activation * gate_up[:, :, i // 2 :]\n",
    "                    x, _ = self.down_proj(x)\n",
    "                    return x\n",
    "                def llama_forward(self, x):\n",
    "                    gate_ = self.gate_proj(x)  # b, l, 2i\n",
    "                    i = gate_.size(-1)\n",
    "                    activation = F.silu(gate_)\n",
    "                    activation.index_fill_(2, mask, 0)\n",
    "                    x = activation * self.up_proj(x)\n",
    "                    x = self.down_proj(x)\n",
    "                    return x\n",
    "    \n",
    "                def bloom_forward(self, x: torch.Tensor):\n",
    "                    x, _ = self.dense_h_to_4h(x)\n",
    "                    x = self.gelu_impl(x)\n",
    "                    x.index_fill_(2, mask, 0)\n",
    "                    x, _ = self.dense_4h_to_h(x)\n",
    "                    return x\n",
    "    \n",
    "                if is_llama:\n",
    "                    return llama_forward\n",
    "                else:\n",
    "                    return bloom_forward\n",
    "    \n",
    "            for i, layer_mask in enumerate(activation_mask):\n",
    "                if is_llama:\n",
    "                    obj = model.model.layers[i].mlp\n",
    "                else:\n",
    "                    obj = model.model.layers[i].mlp\n",
    "                obj.forward = MethodType(factory(layer_mask.to('cuda')), obj)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_mask_neuron_model_vllm(model, activation_mask_path, need_lang):\n",
    "\n",
    "    if activation_mask_path:\n",
    "        activation_masks = torch.load(activation_mask_path)\n",
    "    else:\n",
    "        activation_masks = [None]\n",
    "    \n",
    "    final_output = []\n",
    "    if is_llama:\n",
    "        languages = [\"en\", \"zh\", \"fr\", \"es\", \"vi\", \"id\", \"ja\"]\n",
    "    else:\n",
    "        languages = [\"en\", \"zh\", \"fr\", \"es\", \"vi\", \"id\"]\n",
    "    \n",
    "    need_lang = [need_lang]#['en', 'zh', 'vi']\n",
    "    \n",
    "    for activation_mask, mask_lang in zip(activation_masks, languages):\n",
    "    \n",
    "        if mask_lang not in need_lang:continue\n",
    "            \n",
    "        print(f'get mask =====lang:{mask_lang}=====')\n",
    "    \n",
    "        \n",
    "        if activation_mask:\n",
    "            def factory(mask):\n",
    "                def llama_forward_org(self, x):\n",
    "                    gate_up, _ = self.gate_up_proj(x)  # b, l, 2i\n",
    "                    i = gate_up.size(-1)\n",
    "                    activation = F.silu(gate_up[:, :, : i // 2])\n",
    "                    activation.index_fill_(2, mask, 0)\n",
    "                    x = activation * gate_up[:, :, i // 2 :]\n",
    "                    x, _ = self.down_proj(x)\n",
    "                    return x\n",
    "                def llama_forward(self, x):\n",
    "                    gate_ = self.gate_proj(x)  # b, l, 2i\n",
    "                    i = gate_.size(-1)\n",
    "                    activation = F.silu(gate_)\n",
    "                    activation.index_fill_(2, mask, 0)\n",
    "                    x = activation * self.up_proj(x)\n",
    "                    x = self.down_proj(x)\n",
    "                    return x\n",
    "    \n",
    "                def bloom_forward(self, x: torch.Tensor):\n",
    "                    x, _ = self.dense_h_to_4h(x)\n",
    "                    x = self.gelu_impl(x)\n",
    "                    x.index_fill_(2, mask, 0)\n",
    "                    x, _ = self.dense_4h_to_h(x)\n",
    "                    return x\n",
    "    \n",
    "                if is_llama:\n",
    "                    return llama_forward\n",
    "                else:\n",
    "                    return bloom_forward\n",
    "    \n",
    "            for i, layer_mask in enumerate(activation_mask):\n",
    "                if is_llama:\n",
    "                    obj = model.llm_engine.model_executor.driver_worker.model_runner.model.model.layers[i].mlp\n",
    "                else:\n",
    "                    obj = model.llm_engine.model_executor.driver_worker.model_runner.model.transformer.h[i].mlp\n",
    "                obj.forward = MethodType(factory(layer_mask.to('cuda')), obj)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0865010f-17df-4403-9fc4-57ca8b452891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"VLLM_ENABLE_V1_MULTIPROCESSING\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb1058f0-2808-4bdd-8169-4a28753e190e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-01 16:46:40 [utils.py:253] non-default args: {'disable_log_stats': True, 'model': '/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-chat-hf'}\n",
      "WARNING 12-01 16:46:40 [model.py:438] The global random seed is set to 0. Since VLLM_ENABLE_V1_MULTIPROCESSING is set to False, this may affect the random state of the Python process that launched vLLM.\n",
      "INFO 12-01 16:46:40 [model.py:631] Resolved architecture: LlamaForCausalLM\n",
      "INFO 12-01 16:46:40 [model.py:1745] Using max model len 4096\n",
      "INFO 12-01 16:46:40 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 12-01 16:46:40 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-chat-hf, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "INFO 12-01 16:46:41 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:47497 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 12-01 16:46:41 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 12-01 16:46:42 [gpu_model_runner.py:3259] Starting to load model /root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-chat-hf...\n",
      "INFO 12-01 16:46:42 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "INFO 12-01 16:46:42 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9361387245e347f8952995fb1c51c988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pt checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-01 16:47:39 [default_loader.py:314] Loading weights took 56.55 seconds\n",
      "INFO 12-01 16:47:39 [gpu_model_runner.py:3338] Model loading took 12.5524 GiB memory and 56.828024 seconds\n",
      "INFO 12-01 16:47:43 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/e5e105181a/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 12-01 16:47:43 [backends.py:647] Dynamo bytecode transform time: 4.09 s\n",
      "INFO 12-01 16:47:46 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.931 s\n",
      "INFO 12-01 16:47:47 [monitor.py:34] torch.compile takes 6.02 s in total\n",
      "INFO 12-01 16:47:48 [gpu_worker.py:359] Available KV cache memory: 14.89 GiB\n",
      "INFO 12-01 16:47:48 [kv_cache_utils.py:1229] GPU KV cache size: 30,480 tokens\n",
      "INFO 12-01 16:47:48 [kv_cache_utils.py:1234] Maximum concurrency for 4,096 tokens per request: 7.44x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 16:47:48,567 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "2025-12-01 16:47:48,578 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 22.95it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 34.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-01 16:47:52 [gpu_model_runner.py:4244] Graph capturing finished in 4 secs, took -0.59 GiB\n",
      "INFO 12-01 16:47:52 [core.py:250] init engine (profile, create kv cache, warmup model) took 12.84 seconds\n",
      "INFO 12-01 16:47:52 [llm.py:352] Supported tasks: ('generate',)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from types import MethodType\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "model_path ='/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-chat-hf'\n",
    "activation_mask_path='/root/autodl-fs/Language-Specific-Neurons/LLaMA-2-7B.neuron.pth'\n",
    "\n",
    "is_llama = bool(model_path.find('llama') >= 0)\n",
    "#model, tokenizer = load_model(model_path)\n",
    "\n",
    "# 初始化 vLLM\n",
    "llm = LLM(model=model_path, tensor_parallel_size=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab55c38-ced4-469d-bc55-6c70b58f080b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## test ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "934ae8f4-eb7f-40d6-81cf-a362819a8fd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "model name: org_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:53<00:00, 18.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.5736, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:21<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(14.1997, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:39<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.3294, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_en\n",
      "get mask =====lang:en=====\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:53<00:00, 18.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.5102, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:20<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(13.7195, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:39<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.0321, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_vi\n",
      "get mask =====lang:vi=====\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:53<00:00, 18.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.5556, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:20<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(12.9547, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:39<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.6875, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_zh\n",
      "get mask =====lang:zh=====\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:53<00:00, 18.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.3950, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:20<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(13.4718, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:39<00:00, 10.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.2067, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "test_en = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/en_radom_1000_test.jsonl')\n",
    "test_vi = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/vi_radom_1000_test.jsonl')\n",
    "test_zh = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/zh_radom_1000_test.jsonl')\n",
    "               \n",
    "# calc ppl\n",
    "model_list= [\n",
    "    (model, 'org_model'),\n",
    "    (None, 'mask_en'),\n",
    "    (None, 'mask_vi'),\n",
    "    (None, 'mask_zh'),\n",
    "]\n",
    "\n",
    "data_list = [\n",
    "    (test_en, 'en_data'),\n",
    "    (test_vi, 'vi_data'),\n",
    "    (test_zh, 'zh_data')\n",
    "    \n",
    "]\n",
    "\n",
    "for i_model, model_name in model_list:\n",
    "    print('*'*20)\n",
    "    print('*'*20)\n",
    "    print('model name:', model_name)\n",
    "\n",
    "    if model_name != 'org_model':\n",
    "        i_model = get_mask_neuron_model(model, activation_mask_path, model_name.split('_')[-1])\n",
    "\n",
    "    for i_data, data_name in data_list:\n",
    "        print('='*20)\n",
    "        print('data name:', data_name)\n",
    "        ppl_sum, ppl_count = calc_ppl(i_model, tokenizer, i_data, max_len= 2048)\n",
    "        print('ppl:',ppl_sum/ppl_count )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7672f-fa7e-476e-b52b-528562bfb96a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35bb76-f807-4d9a-8d0d-97f02f8a82c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db1bd67-8ada-480b-b4ac-8dae58dfed96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "238c7fdf-735b-4a43-9cd1-bf4bf8ddf3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "model name: org_model\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:52<00:00, 18.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.5736, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:21<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(14.1997, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:39<00:00, 10.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.3294, device='cuda:0')\n",
      "********************\n",
      "********************\n",
      "model name: mask_zh\n",
      "get mask =====lang:zh=====\n",
      "====================\n",
      "data name: en_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:53<00:00, 18.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(11.3950, device='cuda:0')\n",
      "====================\n",
      "data name: vi_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:20<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(13.4718, device='cuda:0')\n",
      "====================\n",
      "data name: zh_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:39<00:00, 10.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl: tensor(10.2067, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "test_en = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/en_radom_1000_test.jsonl')\n",
    "test_vi = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/vi_radom_1000_test.jsonl')\n",
    "test_zh = get_test_data('/root/autodl-fs/LRP_data/test_ppl_data/zh_radom_1000_test.jsonl')\n",
    "               \n",
    "# calc ppl\n",
    "model_list= [\n",
    "    (model, 'org_model'),\n",
    "    #(None, 'mask_en'),\n",
    "    #(None, 'mask_vi'),\n",
    "    (None, 'mask_zh'),\n",
    "]\n",
    "\n",
    "data_list = [\n",
    "    (test_en, 'en_data'),\n",
    "    (test_vi, 'vi_data'),\n",
    "    (test_zh, 'zh_data')\n",
    "    \n",
    "]\n",
    "\n",
    "for i_model, model_name in model_list:\n",
    "    print('*'*20)\n",
    "    print('*'*20)\n",
    "    print('model name:', model_name)\n",
    "\n",
    "    if model_name != 'org_model':\n",
    "        i_model = get_mask_neuron_model(model, activation_mask_path, model_name.split('_')[-1])\n",
    "\n",
    "    for i_data, data_name in data_list:\n",
    "        print('='*20)\n",
    "        print('data name:', data_name)\n",
    "        ppl_sum, ppl_count = calc_ppl(i_model, tokenizer, i_data, max_len= 2048)\n",
    "        print('ppl:',ppl_sum/ppl_count )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45da724-82c6-4639-8163-0e85c802d446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a650f70-b301-4a7b-bfde-86b3acfcc40a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d1b456a-e412-4b4b-b4e8-145b3cb1bd5d",
   "metadata": {},
   "source": [
    "## test open ended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cce47fb-a519-4130-a078-fa0ab44c6bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8cff58d-bd91-4bd9-bea2-86fdb8c66cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<vllm.entrypoints.llm.LLM at 0x7fa077fd4950>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730aeeeb-eb0c-4825-84fc-766a38a7d077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'vllm.compilation.cuda_graph.CUDAGraphWrapper'>]\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "def echo_model_runner(self):\n",
    "    return self.model_runner.model.__class__\n",
    "\n",
    "\n",
    "\n",
    "print(llm.collective_rpc(echo_model_runner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8ee9ca6-4825-46c7-aa9b-47675e3c0bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<vllm.compilation.cuda_graph.CUDAGraphWrapper object at 0x7f6c3c5a6120>\n"
     ]
    }
   ],
   "source": [
    "def get_attr(self):\n",
    "    return repr(self.model_runner.model)  # or other attributes\n",
    "\n",
    "print(llm.collective_rpc(get_attr)[0])  # always get strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5d22649-7a60-491d-a2e9-04ea047e0b0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "model name: org_model\n",
      "====================\n",
      "data name: open_ended\n",
      "********************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb3487d2f5c47ea9eaa22881362092b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4125f4d5194dcf91eafdc7450414fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/210 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "model name: mask_en\n",
      "get mask =====lang:en=====\n",
      "====================\n",
      "data name: open_ended\n",
      "********************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca97d55216e64a0897890b21eeeaf4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b9943b8c604245b85b493fbc91457a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/210 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "model name: mask_vi\n",
      "get mask =====lang:vi=====\n",
      "====================\n",
      "data name: open_ended\n",
      "********************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21f84160c3545a69ad193ba532ee91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4277b1e66a014045bce7a583a848868e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/210 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "model name: mask_zh\n",
      "get mask =====lang:zh=====\n",
      "====================\n",
      "data name: open_ended\n",
      "********************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3898c7ef051445fafb108fd91e7672e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075e718a4e34450081ea87ca515980d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/210 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'to_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m         ds_test[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m].add_column(name=\u001b[33m'\u001b[39m\u001b[33mdescrib\u001b[39m\u001b[33m'\u001b[39m, column=model_describe)        \n\u001b[32m     33\u001b[39m         ds_test[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m].add_column(name=\u001b[33m'\u001b[39m\u001b[33mans\u001b[39m\u001b[33m'\u001b[39m, column=ans_list)        \n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mds_test\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_json\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mautodl-fs/LRP/open_ended_data_generation/20251201_LAPE_org_generation.json\u001b[39m\u001b[33m'\u001b[39m, force_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DatasetDict' object has no attribute 'to_json'"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "ds_test = datasets.load_dataset('json', data_files ='/root/autodl-fs/LRP/open_ended_dataset/all_data.json')\n",
    "               \n",
    "# calc ppl\n",
    "model_list= [\n",
    "    (llm, 'org_model'),\n",
    "    (None, 'mask_en'),\n",
    "    (None, 'mask_vi'),\n",
    "    (None, 'mask_zh'),\n",
    "]\n",
    "\n",
    "data_list = [\n",
    "    (ds_test, 'open_ended')    \n",
    "]\n",
    "\n",
    "for i_model, model_name in model_list:\n",
    "    print('*'*20)\n",
    "    print('*'*20)\n",
    "    print('model name:', model_name)\n",
    "\n",
    "    if model_name != 'org_model':\n",
    "        i_model = get_mask_neuron_model_vllm(llm, activation_mask_path, model_name.split('_')[-1])\n",
    "\n",
    "    for i_data, data_name in data_list:\n",
    "        print('='*20)\n",
    "        print('data name:', data_name)\n",
    "        \n",
    "        ans_list= get_open_ended_answer_vllm(i_model, list(i_data['train']['text']))#calc_ppl(i_model, tokenizer, i_data, max_len= 2048)\n",
    "        # create model -col\n",
    "        model_describe = [model_name+'|'+data_name]*len(ans_list)\n",
    "        \n",
    "        ds_test['train'] = ds_test['train'].add_column(name='describ', column=model_describe)        \n",
    "        ds_test['train'] = ds_test['train'].add_column(name='ans', column=ans_list)        \n",
    "\n",
    "ds_test['train'].to_json('~/autodl-fs/LRP/open_ended_data_generation/20251201_LAPE_org_generation.json', force_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eb9ada8-b008-4120-ab9c-2e44a488eb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e5e37a23054e0d94709f624c9b62a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "35297"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test['train'].to_json('~/autodl-fs/LRP/open_ended_data_generation/20251201_LAPE_org_generation.json', force_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2fdbe07-182e-4675-a76e-fb24275d9720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_test['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a179453-6e89-47b6-bc9e-0ce2d671e507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef86310d-edde-41e1-8bab-1624864a469a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888ec36c65a14bca893fd5976864010b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "********************\n",
      "model name: org_model\n",
      "====================\n",
      "data name: open_ended\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/210 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/210 [00:08<30:33,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itext: How can I improve my time management skills?\n",
      "content: Time management is the process of planning and controlling the amount of time spent on different activities to maximize efficiency and productivity. Here are some tips that can help you improve your time management skills:\n",
      "\n",
      "1. Set clear goals: Identify what you want to achieve and set specific, measurable, achievable, relevant, and time-bound (SMART) goals. This will help you prioritize your tasks and focus on what's important.\n",
      "2. Use a planner or calendar: Write down all your tasks, appointments, and deadlines in a planner or calendar. This will help you keep track of your schedule and plan your day effectively.\n",
      "3. Prioritize tasks: Determine which tasks are most important and need to be completed first. Use the Eisenhower Matrix to categorize tasks into urgent vs. important, and focus on the most critical ones first.\n",
      "4. Break down large tasks: Divide big projects into smaller, manageable chunks. This will help you avoid feeling overwhelmed and make it easier to stay focused.\n",
      "5. Eliminate distractions: Identify things that waste your time, such as social media, email, or meetings that don't add value, and eliminate them. Create a conducive work environment that promotes productivity.\n",
      "6. Learn to say no: Be mindful of taking on too much work or commitments. Learn to say no to requests that don't align with your goals or values, or that you cannot realistically fulfill.\n",
      "7. Take breaks: Take regular breaks throughout the day to recharge and avoid burnout. Use the Pomodoro Technique, which involves working for 25 minutes and then taking a five-minute break.\n",
      "8. Delegate tasks: Identify tasks that can be delegated to others, such as colleagues, subordinates, or freelancers. This will free up your time and allow you to focus on high-priority tasks.\n",
      "9. Use technology: Utilize tools like project management software, time tracking apps, and automation tools to streamline your workflow and save time.\n",
      "10. Review and adjust: Regularly review your progress and adjust your time management strategy as needed. Reflect on what works well and what doesn't, and make changes accordingly.\n",
      "\n",
      "Remember, time management is a skill that takes practice and patience. Start by implementing one or two strategies and gradually build up your skills over time. With consistent effort, you can become more efficient and productive in managing your time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/210 [00:17<29:31,  8.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itext: What are the most effective ways to deal with stress?\n",
      "content: Stress is a normal part of life, but chronic stress can have serious consequences for your mental and physical health. Here are some effective ways to manage stress:\n",
      "\n",
      "1. Practice relaxation techniques: Deep breathing, progressive muscle relaxation, and visualization can help calm your mind and body. You can find guided relaxation exercises online or through apps like Headspace or Calm.\n",
      "2. Exercise regularly: Regular physical activity can help reduce stress by releasing endorphins, which are natural mood-boosters. Find an exercise routine that works for you, whether it's running, yoga, or swimming.\n",
      "3. Connect with others: Social support from friends, family, or a therapist can help you cope with stress. Reach out to someone you trust and talk about how you're feeling.\n",
      "4. Take breaks: Make time for activities that bring you joy and help you relax, such as reading, listening to music, or taking a warm bath.\n",
      "5. Prioritize self-care: Make sure you're getting enough sleep, eating a healthy diet, and taking care of any basic needs. When you feel physically well, you'll be better able to handle stress.\n",
      "6. Seek professional help: If you're experiencing chronic stress that's impacting your daily life, consider seeking help from a mental health professional. They can provide you with coping strategies and support.\n",
      "7. Try mindfulness meditation: Mindfulness meditation involves paying attention to the present moment without judgment. It can help you become more aware of your thoughts and feelings, and find a sense of calm in the midst of stress.\n",
      "8. Engage in activities you enjoy: Doing things you love can help take your mind off stress and improve your mood. Whether it's painting, playing an instrument, or cooking, make time for activities that bring you joy.\n",
      "9. Set boundaries: Learn to say no to things that add to your stress levels, and prioritize your own needs. Setting boundaries can help you avoid feeling overwhelmed and give you more time for self-care.\n",
      "10. Practice gratitude: Focusing on what you're grateful for can help shift your perspective and reduce stress. Keep a gratitude journal, write down three things you're thankful for each day, or share your gratitude with a friend or loved one.\n",
      "\n",
      "Remember, everyone experiences stress differently, so it's important to find what works best for you. Experiment with different techniques and find a combination that helps you manage stress effectively.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 3/210 [00:25<28:37,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itext: What are the main differences between Python and JavaScript programming languages?\n",
      "content: Python and JavaScript are two popular programming languages used for different purposes. Here are some of the main differences between them:\n",
      "\n",
      "1. Syntax: Python's syntax is more verbose than JavaScript's. Python requires indentation to define code blocks, while JavaScript uses curly braces and semicolons to define code blocks.\n",
      "2. Statically vs Dynamically Typed: Python is a statically typed language, which means that variable types must be declared before using them. JavaScript, on the other hand, is dynamically typed, which means that variable types can be determined at runtime.\n",
      "3. Object-Oriented Programming (OOP): Python is an object-oriented language that supports encapsulation, inheritance, and polymorphism through classes and objects. JavaScript also has OOP capabilities, but they are not as robust as Python's.\n",
      "4. Memory Management: Python handles memory management automatically, which means that developers don't have to worry about freeing up memory or dealing with memory leaks. JavaScript, on the other hand, requires manual memory management through the use of garbage collection.\n",
      "5. Platform: Python can run on multiple platforms, including Windows, macOS, and Linux. JavaScript, on the other hand, is primarily used in web development and can run in web browsers or Node.js servers.\n",
      "6. Libraries and Frameworks: Python has a vast number of libraries and frameworks available for various tasks, such as data analysis, machine learning, and web development. JavaScript also has a large number of libraries and frameworks, but they are more focused on web development and client-side scripting.\n",
      "7. Learning Curve: Python is generally considered easier to learn than JavaScript, especially for beginners. Python's syntax is more consistent and logical, while JavaScript's syntax can be more complex and irregular.\n",
      "8. Use Cases: Python is commonly used for data analysis, machine learning, web development, and automation. JavaScript is primarily used for web development, client-side scripting, and server-side development with Node.js.\n",
      "9. Performance: Python is generally slower than JavaScript, especially for high-performance computing tasks. However, Python's ease of use and flexibility make up for its performance limitations in many cases.\n",
      "10. Community: Both Python and JavaScript have large and active communities, with numerous resources available for learning and troubleshooting.\n",
      "\n",
      "In summary, Python and JavaScript are both powerful programming languages with their own strengths and weaknesses. While Python is better suited for data analysis, machine learning, and automation, JavaScript is ideal for web development and client-side scripting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/210 [00:33<28:46,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itext: How can I increase my productivity while working from home?\n",
      "content: Working from home can be a great way to boost your productivity, but it can also be challenging if you don't have the right strategies in place. Here are some tips to help you increase your productivity while working from home:\n",
      "\n",
      "1. Create a dedicated workspace: Having a dedicated workspace at home can help you stay focused and avoid distractions. Make sure your workspace is well-lit, comfortable, and equipped with everything you need to do your job.\n",
      "2. Set regular working hours: Just because you're working from home doesn't mean you should work whenever you want. Set regular working hours and stick to them as much as possible. This will help you maintain a work-life balance and avoid burnout.\n",
      "3. Minimize distractions: When working from home, it can be easy to get sidetracked by household chores or personal errands. Try to minimize distractions by closing unnecessary tabs on your computer, turning off your phone, and finding a quiet workspace.\n",
      "4. Use time management tools: There are many time management tools available that can help you stay organized and focused while working from home. Consider using a project management tool like Trello or Asana, or a time tracking tool like RescueTime or Harvest.\n",
      "5. Take breaks: Working long hours without taking breaks can lead to burnout and decreased productivity. Make sure to take short breaks throughout the day to recharge and refocus.\n",
      "6. Stay connected with colleagues: When working from home, it can be easy to feel disconnected from your colleagues and team. Make an effort to stay connected through video conferencing, instant messaging, or regular check-ins.\n",
      "7. Prioritize self-care: Working from home can be isolating, so make sure to prioritize self-care. Take care of yourself by getting enough sleep, exercising regularly, and taking breaks to relax and recharge.\n",
      "8. Set clear goals and deadlines: When working from home, it can be easy to lose focus or procrastinate. Make sure to set clear goals and deadlines for yourself to help you stay motivated and focused.\n",
      "9. Limit multitasking: While it may be tempting to try to do multiple things at once, research has shown that multitasking can actually decrease productivity. Instead, focus on one task at a time and give it your full attention.\n",
      "10. Stay organized: Keep your workspace organized and clutter-free to help you stay focused and avoid wasting time searching for lost documents or supplies.\n",
      "\n",
      "By following these tips, you can increase your productivity while working from home and achieve your goals more efficiently.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5/210 [00:45<32:25,  9.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itext: Can you explain the basics of quantum computing?\n",
      "content: Quantum computing is a rapidly developing field that has the potential to revolutionize the way we approach complex computational problems. However, it can be difficult to understand the basics of quantum computing without some background in physics or computer science. In this answer, I will provide an overview of the key concepts and principles of quantum computing, including superposition, entanglement, and quantum gates.\n",
      "\n",
      "Superposition:\n",
      "\n",
      "In classical computing, bits can only be in one of two states, either 0 or 1. However, in quantum computing, qubits (quantum bits) can exist in multiple states simultaneously, known as superposition. This means that a qubit can represent both 0 and 1 at the same time, which allows for much faster computation times than classical computers.\n",
      "\n",
      "Entanglement:\n",
      "\n",
      "Entanglement is a phenomenon where two or more qubits become connected in such a way that their properties are correlated, regardless of the distance between them. This means that if something happens to one qubit, it instantly affects the state of the other entangled qubits. Entanglement is a key feature of quantum computing, as it allows for the creation of quantum algorithms that can solve certain problems much faster than classical algorithms.\n",
      "\n",
      "Quantum Gates:\n",
      "\n",
      "Quantum gates are the basic building blocks of quantum computing. They are the quantum equivalent of logic gates in classical computing, but they operate on qubits rather than classical bits. Quantum gates perform operations on qubits, such as adding them together, flipping their states, or creating entanglement between them. By combining different quantum gates, quantum computers can perform complex computations that are not possible with classical computers.\n",
      "\n",
      "Quantum Circuits:\n",
      "\n",
      "A quantum circuit is a sequence of quantum gates that are applied to a set of qubits. These circuits can be used to perform complex quantum computations by manipulating the states of the qubits. Quantum circuits can be thought of as the quantum equivalent of flowcharts, with each gate representing a specific operation.\n",
      "\n",
      "Measurement:\n",
      "\n",
      "In quantum computing, measurement is the process of collapsing the superposition of a qubit to a single state. When a qubit is measured, its state is fixed, and it can no longer exist in a superposition of states. Measurement is an important aspect of quantum computing, as it allows us to extract information from qubits and determine their final state.\n",
      "\n",
      "Error Correction:\n",
      "\n",
      "One of the biggest challenges in quantum computing is dealing with errors that occur during computation. Quantum systems are inherently fragile, and errors can quickly accumulate and destroy the delicate quantum states required for computation. To address this challenge, quantum error correction codes have been developed, which can detect and correct errors in quantum computations.\n",
      "\n",
      "Quantum Computing Applications:\n",
      "\n",
      "Quantum computing has the potential to revolutionize many fields, including chemistry, materials science, cryptography, and optimization. For example, quantum computers can be used to simulate the behavior of molecules, which could lead to new insights into drug discovery and materials science. They can also be used to break certain types of encryption, which could have significant implications for cybersecurity.\n",
      "\n",
      "Conclusion:\n",
      "\n",
      "Quantum computing is a rapidly advancing field that has the potential to revolutionize the way we approach complex computational problems. By understanding the basics of quantum computing, including superposition, entanglement, quantum gates, and quantum circuits, we can begin to unlock the power of quantum computing and explore its many applications. While there are still many challenges to overcome, the potential benefits of quantum computing make it an exciting and promising area of research.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 6/210 [01:04<43:34, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itext: What are the differences between plant-based and animal-based protein sources?\n",
      "content: Protein is an essential nutrient that plays a vital role in the human body. It helps build, repair, and maintain tissues, organs, and cells, as well as supports immune function and overall health. There are two main types of protein sources: plant-based and animal-based. In this article, we will explore the differences between these two types of protein sources, including their nutritional content, digestibility, and potential health benefits.\n",
      "Plant-Based Protein Sources\n",
      "Plant-based protein sources come from plants, such as beans, lentils, nuts, seeds, and whole grains. These protein sources are typically lower in protein than animal-based sources but are often higher in other nutrients like fiber, vitamins, and minerals. Here are some examples of plant-based protein sources and their nutritional content:\n",
      "Beans (e.g., kidney beans, black beans, chickpeas): 15-20 grams of protein per 1/2 cup serving, high in fiber, folate, and potassium\n",
      "Lentils (e.g., green or brown lentils): 18 grams of protein per 1/2 cup serving, high in fiber, folate, and potassium\n",
      "Nuts (e.g., almonds, walnuts, chia seeds): 5-7 grams of protein per ounce (28 grams) serving, high in healthy fats, fiber, and antioxidants\n",
      "Seeds (e.g., hemp seeds, flaxseeds): 5-7 grams of protein per ounce (28 grams) serving, high in healthy fats, fiber, and omega-3 fatty acids\n",
      "Whole grains (e.g., quinoa, brown rice): 5-8 grams of protein per cup cooked, high in fiber, B vitamins, and minerals\n",
      "Animal-Based Protein Sources\n",
      "Animal-based protein sources come from animals, such as meat, poultry, fish, eggs, and dairy products. These protein sources are typically higher in protein than plant-based sources but may be lower in certain nutrients like fiber and vitamins. Here are some examples of animal-based protein sources and their nutritional content:\n",
      "Meat (e.g., beef, chicken, turkey): 20-30 grams of protein per 3-ounce (85-gram) serving, high in protein, iron, and zinc\n",
      "Poultry (e.g., chicken, turkey): 20-25 grams of protein per 3-ounce (85-gram) serving, similar to meat in protein content and nutrient profile\n",
      "Fish (e.g., salmon, tuna): 15-20 grams of protein per 3-ounce (85-gram) serving, high in protein, omega-3 fatty acids, and selenium\n",
      "Eggs: 6-7 grams of protein per large egg, high in protein, vitamin D, and choline\n",
      "Dairy products (e.g., milk, cheese, yogurt): 7-9 grams of protein per 8-ounce (240-milliliter) serving, high in protein, calcium, and vitamin D\n",
      "Digestibility and Absorption\n",
      "The digestibility and absorption of plant-based and animal-based protein sources can vary. Plant-based proteins tend to be more easily digested and absorbed than animal-based proteins due to their simpler structure and lower levels of anti-nutrients (e.g., phytates, tannins). However, some plant-based proteins, such as beans and lentils, require proper preparation and soaking to break down their indigestible carbohydrates and increase their bioavailability.\n",
      "Health Benefits and Risks\n",
      "Both plant-based and animal-based protein sources have their unique health benefits and risks. Here are some of them:\n",
      "Plant-Based Protein Sources:\n",
      "Lower risk of heart disease and stroke due to lower saturated fat and cholesterol content compared to animal-based sources\n",
      "Higher fiber content, which can promote digestive health and satiety\n",
      "May reduce the risk of certain cancers, such as colon cancer, due to their high antioxidant and phytochemical content\n",
      "Can help with weight management due to their high fiber and water content, leading to feelings of fullness and reduced calorie intake\n",
      "Animal-Based Protein Sources:\n",
      "Typically higher in complete protein, meaning they provide all nine essential amino acids that the body cannot produce on its own\n",
      "May be easier to absorb and utilize for muscle growth and repair due to their simpler structure and higher bioavailability of amino acids\n",
      "Can provide important nutrients like vitamin B12, iron, and zinc, which are found primarily in animal-based sources\n",
      "However, high consumption of animal-based protein sources has been linked to increased risk of chronic diseases, such as heart disease, type 2 diabetes, and certain types of cancer\n",
      "Conclusion\n",
      "In conclusion, both plant-based and animal-based protein sources have their unique nutritional profiles, digestibility, and health benefits. While plant-based protein sources tend to be lower in protein but higher in fiber and other nutrients, animal-based protein sources are typically higher in protein but may contain fewer nutrients. A balanced diet that includes a variety of protein sources can help ensure adequate protein intake while minimizing the risks associated with excessive protein consumption. Ultimately, the choice between plant-based and animal-based protein sources depends on individual preferences, dietary needs, and lifestyle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 7/210 [01:14<40:08, 11.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itext: How can I develop my critical thinking skills?\n",
      "content: Critical thinking is the ability to objectively evaluate information, arguments, and beliefs in order to form sound judgments and make informed decisions. Developing critical thinking skills can help you in many areas of your life, including academics, career, and personal relationships. Here are some tips on how to develop your critical thinking skills:\n",
      "\n",
      "1. Practice active reading and listening: When you read or listen to something, don't just passively absorb the information. Instead, actively engage with it by asking questions like \"What is the author trying to say?\", \"What evidence does the author provide to support their claims?\", \"Are there any biases or assumptions present in the text?\".\n",
      "2. Ask questions: Asking questions is a great way to challenge your assumptions and to better understand the information you are encountering. Ask questions like \"Why do I believe this?\", \"Is there evidence to support this claim?\", \"What are the implications of this idea?\".\n",
      "3. Analyze arguments: When you encounter an argument, try to identify the premises and conclusion. Then, ask yourself questions like \"Is the argument logically sound?\", \"Are the premises true?\", \"What are the implications of the argument?\".\n",
      "4. Evaluate evidence: When you encounter evidence, ask yourself questions like \"Is the evidence credible?\", \"Is the evidence relevant to the claim being made?\", \"How does the evidence support or refute the claim?\".\n",
      "5. Practice reflective thinking: Reflective thinking involves taking the time to think about your own thoughts and beliefs. Ask yourself questions like \"What are my biases?\", \"What are my assumptions?\", \"How did I arrive at these beliefs?\".\n",
      "6. Seek diverse perspectives: Exposing yourself to different viewpoints and perspectives can help you to broaden your understanding and to challenge your assumptions. Seek out people with different backgrounds, experiences, and opinions.\n",
      "7. Take a skeptical approach: Approach new ideas and information with a healthy dose of skepticism. Ask questions like \"Is this really true?\", \"Is there evidence to support this claim?\", \"What are the implications of this idea?\".\n",
      "8. Learn about cognitive biases: Cognitive biases are systematic errors in thinking that can affect our decision-making and judgment. Learning about these biases can help you to recognize when they are influencing your thinking.\n",
      "9. Practice mindfulness: Mindfulness involves being present and aware of your thoughts and emotions without judgment. Practicing mindfulness can help you to become more aware of your thought processes and to make more objective decisions.\n",
      "10. Seek feedback: Seek feedback from others on your critical thinking skills. Ask them to provide examples of times when you demonstrated strong critical thinking skills and areas where you could improve.\n",
      "\n",
      "By following these tips, you can develop your critical thinking skills and become a more critical and discerning thinker. Remember, critical thinking is a skill that takes time and practice to develop, so be patient and persistent in your efforts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/210 [01:23<37:01, 11.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itext: What are the major challenges faced by the education sector today?\n",
      "content: Education is a fundamental right of every child, and it plays a crucial role in shaping their future. However, the education sector faces numerous challenges that hinder its ability to provide quality education to all. Here are some of the major challenges faced by the education sector today:\n",
      "1. Access to Education: Despite progress made in recent years, millions of children around the world still lack access to basic education. Many rural areas, particularly in developing countries, have limited or no access to schools, making it difficult for children to receive an education.\n",
      "2. Quality of Education: The quality of education varies greatly across different regions and countries. Many schools lack adequate resources, including textbooks, technology, and trained teachers, which can result in a subpar learning experience for students.\n",
      "3. Teacher Shortages: Many countries face teacher shortages, particularly in subjects like math, science, and language. This can lead to overcrowded classrooms and a lack of individualized attention for students.\n",
      "4. Affordability: The cost of education can be prohibitively expensive for many families, particularly those in low-income communities. This can result in a lack of diversity in schools and limit opportunities for disadvantaged students.\n",
      "5. Curriculum Reform: The traditional curriculum often focuses on rote memorization and lacks practical skills and critical thinking. There is a growing need for curriculum reform to prepare students for the challenges of the 21st century.\n",
      "6. Technology Integration: With the rapid pace of technological advancements, there is a growing need for schools to integrate technology into the classroom. This can help improve learning outcomes and make education more accessible.\n",
      "7. Special Needs Education: Children with special needs often face significant barriers in accessing quality education. This includes a lack of resources, trained teachers, and inclusive environments.\n",
      "8. Early Childhood Education: Early childhood education is critical for laying the foundation of future learning. However, many countries lack investment in early childhood education, resulting in a lack of access to quality care and education for young children.\n",
      "9. Accountability and Governance: Weak governance and accountability structures can result in corruption, mismanagement, and a lack of transparency in the education sector. This can undermine efforts to improve education outcomes and create a culture of distrust among stakeholders.\n",
      "10. Climate Change and Natural Disasters: Climate change and natural disasters can have a significant impact on education systems, particularly in vulnerable regions. Schools may be damaged or destroyed during these events, leaving students without a place to learn.\n",
      "Addressing these challenges will require a concerted effort from governments, educators, policymakers, and other stakeholders. By working together, we can create a more equitable and effective education system that provides quality education for all.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 9/210 [01:33<36:21, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itext: What are the primary factors that influence consumer behavior?\n",
      "content: Consumer behavior is influenced by a variety of factors, including personal, psychological, social, cultural, and economic factors. Here are some of the primary factors that can impact consumer behavior:\n",
      "1. Personal Factors: Consumers' personal characteristics such as age, income, occupation, lifestyle, values, attitudes, and beliefs play a significant role in shaping their buying habits. For instance, a young professional with a high income may be more likely to purchase luxury goods than an elderly person living on a fixed income.\n",
      "2. Psychological Factors: Consumers' emotional states, motivations, and perceptions also affect their purchasing decisions. For example, a consumer who feels anxious or stressed may be more likely to seek comfort foods or indulge in retail therapy.\n",
      "3. Social Factors: The opinions and behaviors of friends, family, and peers can significantly influence consumers' purchasing decisions. For instance, a consumer may be more likely to buy a product if they see it endorsed by a popular influencer or celebrity.\n",
      "4. Cultural Factors: Cultural norms, values, and beliefs can shape consumers' preferences and purchasing behaviors. For example, in some cultures, saving face is a crucial aspect of social etiquette, which may lead consumers to avoid making purchases in public or from salespeople they perceive as pushy.\n",
      "5. Economic Factors: Economic conditions such as inflation, unemployment, and changes in disposable income can significantly impact consumers' spending habits. For instance, during times of economic downturn, consumers may cut back on non-essential purchases or delay big-ticket items until they feel more financially secure.\n",
      "6. Marketing and Advertising: The way marketers and advertisers present products and services can also influence consumer behavior. Effective marketing campaigns can create demand for new products or services, while poorly executed campaigns can lead to negative perceptions of a brand.\n",
      "7. Technology: Advances in technology have transformed the way consumers shop and interact with brands. E-commerce platforms, mobile payments, and social media have made it easier for consumers to discover new products, compare prices, and connect with other consumers.\n",
      "8. Government Policies: Government policies and regulations can also impact consumer behavior. For example, taxes on sugary drinks or carbonated beverages may lead consumers to choose healthier alternatives.\n",
      "9. Demographic Changes: Shifts in population demographics, such as aging populations or changing family structures, can alter consumer behavior. For instance, older consumers may prioritize health and wellness products, while younger consumers may be more interested in sustainability and ethical sourcing.\n",
      "10. Environmental Factors: Environmental factors such as climate change, natural disasters, and resource scarcity can influence consumers' purchasing decisions. For example, consumers may be more likely to choose products with eco-friendly packaging or sustainable materials in response to environmental concerns.\n",
      "By understanding these primary factors that influence consumer behavior, businesses can develop effective marketing strategies tailored to specific target audiences and adjust their offerings accordingly.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m20\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mdata name:\u001b[39m\u001b[33m'\u001b[39m, data_name)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m ans_list= \u001b[43mget_open_ended_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#calc_ppl(i_model, tokenizer, i_data, max_len= 2048)\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# create model -col\u001b[39;00m\n\u001b[32m     30\u001b[39m model_describe = [model_name+\u001b[33m'\u001b[39m\u001b[33m|\u001b[39m\u001b[33m'\u001b[39m+data_name]*\u001b[38;5;28mlen\u001b[39m(ans_list)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/autodl-fs/data/LRP/util.py:70\u001b[39m, in \u001b[36mget_open_ended_answer\u001b[39m\u001b[34m(model, tokenizer, textlist, max_new_tokens)\u001b[39m\n\u001b[32m     67\u001b[39m input_ids = tokenizer(input_text, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m#print('input_ids:', input_ids)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m output_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#max_length \u001b[39;00m\n\u001b[32m     71\u001b[39m output_ids = output_ids[\u001b[32m0\u001b[39m][\u001b[38;5;28mlen\u001b[39m(input_ids.input_ids[\u001b[32m0\u001b[39m]):].tolist() \n\u001b[32m     73\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[33;03mFor qwen3\u001b[39;00m\n\u001b[32m     75\u001b[39m \n\u001b[32m     76\u001b[39m \u001b[33;03m'''\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:459\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    440\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    441\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    442\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    443\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    444\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/utils/generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:395\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    407\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    408\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    409\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:294\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    292\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    293\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m hidden_states, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    306\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:252\u001b[39m, in \u001b[36mLlamaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation != \u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    250\u001b[39m     attention_interface = ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m.config._attn_implementation]\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    264\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask.dtype != torch.bool:\n\u001b[32m     93\u001b[39m         \u001b[38;5;66;03m# Convert to boolean type, making sdpa to force call FlashAttentionScore to improve performance.\u001b[39;00m\n\u001b[32m     94\u001b[39m         attention_mask = torch.logical_not(attention_mask.bool()).to(query.device)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msdpa_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "ds_test = datasets.load_dataset('json', data_files ='/root/autodl-fs/LRP/open_ended_dataset/all_data.json')\n",
    "               \n",
    "# calc ppl\n",
    "model_list= [\n",
    "    (model, 'org_model'),\n",
    "    (None, 'mask_en'),\n",
    "    (None, 'mask_vi'),\n",
    "    (None, 'mask_zh'),\n",
    "]\n",
    "\n",
    "data_list = [\n",
    "    (ds_test, 'open_ended')    \n",
    "]\n",
    "\n",
    "for i_model, model_name in model_list:\n",
    "    print('*'*20)\n",
    "    print('*'*20)\n",
    "    print('model name:', model_name)\n",
    "\n",
    "    if model_name != 'org_model':\n",
    "        i_model = get_mask_neuron_model(model, activation_mask_path, model_name.split('_')[-1])\n",
    "\n",
    "    for i_data, data_name in data_list:\n",
    "        print('='*20)\n",
    "        print('data name:', data_name)\n",
    "        \n",
    "        ans_list= get_open_ended_answer(i_model, tokenizer, list(i_data['train']['text']))#calc_ppl(i_model, tokenizer, i_data, max_len= 2048)\n",
    "        # create model -col\n",
    "        model_describe = [model_name+'|'+data_name]*len(ans_list)\n",
    "        \n",
    "        ds_test['train'].add_column(name='describ', column=model_describe)        \n",
    "        ds_test['train'].add_column(name='ans', column=ans_list)        \n",
    "\n",
    "ds_test.to_json('autodl-fs/LRP/open_ended_data_generation/20251201_LAPE_org_generation.json', force_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3acb1a-ee3f-4b6d-a695-12c857b7e722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e347f51-083b-4afd-8ac4-4f6b42d357d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
