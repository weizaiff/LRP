2025-12-11 14:31:12 - evalscope - INFO: Running with native backend
2025-12-11 14:31:12 - evalscope - INFO: Creating model /root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-hf with eval_type=llm_ckpt base_url=None, config={'batch_size': 1, 'max_tokens': 2048, 'top_p': 1.0, 'temperature': 1.0, 'do_sample': False, 'top_k': 50, 'n': 1}, model_args={'revision': 'master', 'precision': 'torch.bf16', 'device_map': 'auto'}
2025-12-11 14:31:16 - evalscope - INFO: Dump task config to ./outputs/20251211_143112/configs/task_config_fd3eb5.yaml
2025-12-11 14:31:16 - evalscope - INFO: {
    "model": "/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-hf",
    "model_id": "Llama-2-7b-hf",
    "model_args": {
        "revision": "master",
        "precision": "torch.bf16",
        "device_map": "auto"
    },
    "model_task": "text_generation",
    "chat_template": null,
    "datasets": [
        "mmlu",
        "ceval"
    ],
    "dataset_args": {
        "mmlu": {
            "name": "mmlu",
            "dataset_id": "cais/mmlu",
            "output_types": [
                "generation"
            ],
            "subset_list": [
                "abstract_algebra",
                "anatomy",
                "astronomy",
                "business_ethics",
                "clinical_knowledge",
                "college_biology",
                "college_chemistry",
                "college_computer_science",
                "college_mathematics",
                "college_medicine",
                "college_physics",
                "computer_security",
                "conceptual_physics",
                "econometrics",
                "electrical_engineering",
                "elementary_mathematics",
                "formal_logic",
                "global_facts",
                "high_school_biology",
                "high_school_chemistry",
                "high_school_computer_science",
                "high_school_european_history",
                "high_school_geography",
                "high_school_government_and_politics",
                "high_school_macroeconomics",
                "high_school_mathematics",
                "high_school_microeconomics",
                "high_school_physics",
                "high_school_psychology",
                "high_school_statistics",
                "high_school_us_history",
                "high_school_world_history",
                "human_aging",
                "human_sexuality",
                "international_law",
                "jurisprudence",
                "logical_fallacies",
                "machine_learning",
                "management",
                "marketing",
                "medical_genetics",
                "miscellaneous",
                "moral_disputes",
                "moral_scenarios",
                "nutrition",
                "philosophy",
                "prehistory",
                "professional_accounting",
                "professional_law",
                "professional_medicine",
                "professional_psychology",
                "public_relations",
                "security_studies",
                "sociology",
                "us_foreign_policy",
                "virology",
                "world_religions"
            ],
            "default_subset": "all",
            "few_shot_num": 5,
            "few_shot_random": false,
            "train_split": "dev",
            "eval_split": "test",
            "prompt_template": "Answer the following multiple choice question. The last line of your response should be of the following format: 'ANSWER: [LETTER]' (without quotes) where [LETTER] is one of {letters}. Think step by step before answering.\n\n{question}\n\n{choices}",
            "few_shot_prompt_template": null,
            "system_prompt": null,
            "query_template": null,
            "pretty_name": "MMLU",
            "description": "The MMLU (Massive Multitask Language Understanding) benchmark is a comprehensive evaluation suite designed to assess the performance of language models across a wide range of subjects and tasks. It includes multiple-choice questions from various domains, such as history, science, mathematics, and more, providing a robust measure of a model's understanding and reasoning capabilities.",
            "tags": [
                "Knowledge",
                "MCQ"
            ],
            "filters": null,
            "metric_list": [
                "acc"
            ],
            "aggregation": "mean",
            "shuffle": false,
            "shuffle_choices": false,
            "force_redownload": false,
            "review_timeout": null,
            "extra_params": {},
            "sandbox_config": {}
        },
        "ceval": {
            "name": "ceval",
            "dataset_id": "evalscope/ceval",
            "output_types": [
                "generation"
            ],
            "subset_list": [
                "computer_network",
                "operating_system",
                "computer_architecture",
                "college_programming",
                "college_physics",
                "college_chemistry",
                "advanced_mathematics",
                "probability_and_statistics",
                "discrete_mathematics",
                "electrical_engineer",
                "metrology_engineer",
                "high_school_mathematics",
                "high_school_physics",
                "high_school_chemistry",
                "high_school_biology",
                "middle_school_mathematics",
                "middle_school_biology",
                "middle_school_physics",
                "middle_school_chemistry",
                "veterinary_medicine",
                "college_economics",
                "business_administration",
                "marxism",
                "mao_zedong_thought",
                "education_science",
                "teacher_qualification",
                "high_school_politics",
                "high_school_geography",
                "middle_school_politics",
                "middle_school_geography",
                "modern_chinese_history",
                "ideological_and_moral_cultivation",
                "logic",
                "law",
                "chinese_language_and_literature",
                "art_studies",
                "professional_tour_guide",
                "legal_professional",
                "high_school_chinese",
                "high_school_history",
                "middle_school_history",
                "civil_servant",
                "sports_science",
                "plant_protection",
                "basic_medicine",
                "clinical_medicine",
                "urban_and_rural_planner",
                "accountant",
                "fire_engineer",
                "environmental_impact_assessment_engineer",
                "tax_accountant",
                "physician"
            ],
            "default_subset": "default",
            "few_shot_num": 5,
            "few_shot_random": false,
            "train_split": "dev",
            "eval_split": "val",
            "prompt_template": "以下是中国关于{subject}的单项选择题，请选出其中的正确答案。你的回答的最后一行应该是这样的格式：\"答案：[LETTER]\"（不带引号），其中 [LETTER] 是 A、B、C、D 中的一个。\n\n问题：{question}\n选项：\n{choices}\n",
            "few_shot_prompt_template": "以下是一些示例问题：\n\n{fewshot}\n\n",
            "system_prompt": null,
            "query_template": null,
            "pretty_name": "C-Eval",
            "description": "C-Eval is a benchmark designed to evaluate the performance of AI models on Chinese exams across various subjects, including STEM, social sciences, and humanities. It consists of multiple-choice questions that test knowledge and reasoning abilities in these areas.",
            "tags": [
                "Knowledge",
                "MCQ",
                "Chinese"
            ],
            "filters": null,
            "metric_list": [
                "acc"
            ],
            "aggregation": "mean",
            "shuffle": false,
            "shuffle_choices": false,
            "force_redownload": false,
            "review_timeout": null,
            "extra_params": {},
            "sandbox_config": {}
        }
    },
    "dataset_dir": "/root/.cache/modelscope/hub/datasets",
    "dataset_hub": "modelscope",
    "repeats": 1,
    "generation_config": {
        "batch_size": 1,
        "max_tokens": 2048,
        "top_p": 1.0,
        "temperature": 1.0,
        "do_sample": false,
        "top_k": 50,
        "n": 1
    },
    "eval_type": "llm_ckpt",
    "eval_backend": "Native",
    "eval_config": null,
    "limit": 5,
    "eval_batch_size": 1,
    "use_cache": null,
    "rerun_review": false,
    "work_dir": "./outputs/20251211_143112",
    "ignore_errors": false,
    "debug": false,
    "seed": 42,
    "api_url": null,
    "timeout": null,
    "stream": null,
    "judge_strategy": "auto",
    "judge_worker_num": 1,
    "judge_model_args": {},
    "analysis_report": false,
    "use_sandbox": false,
    "sandbox_type": "docker",
    "sandbox_manager_config": {},
    "evalscope_version": "1.3.0"
}
2025-12-11 14:31:16 - evalscope - INFO: Start evaluating benchmark: mmlu
2025-12-11 14:31:17 - evalscope - INFO: Loading dataset cais/mmlu from modelscope > subset: all > split: test ...
2025-12-11 14:33:06 - evalscope - INFO: Loading dataset cais/mmlu from modelscope > subset: all > split: dev ...
2025-12-11 14:33:18 - evalscope - INFO: Evaluating all subsets of the dataset...
2025-12-11 14:33:18 - evalscope - INFO: Evaluating subset: abstract_algebra
2025-12-11 14:33:18 - evalscope - INFO: Getting predictions for subset: abstract_algebra
2025-12-11 14:33:18 - evalscope - INFO: Processing 5 samples, if data is large, it may take a while.
2025-12-11 14:34:18 - evalscope - INFO: Predicting[mmlu@abstract_algebra]:  still processing... pending=5
2025-12-11 14:36:18 - evalscope - INFO: Predicting[mmlu@abstract_algebra]:  still processing... pending=4
2025-12-11 14:38:18 - evalscope - INFO: Predicting[mmlu@abstract_algebra]:  still processing... pending=3
2025-12-11 14:40:18 - evalscope - INFO: Predicting[mmlu@abstract_algebra]:  still processing... pending=2
