analysis_report: false
api_url: null
chat_template: null
dataset_args:
  ceval:
    aggregation: mean
    dataset_id: evalscope/ceval
    default_subset: default
    description: C-Eval is a benchmark designed to evaluate the performance of AI
      models on Chinese exams across various subjects, including STEM, social sciences,
      and humanities. It consists of multiple-choice questions that test knowledge
      and reasoning abilities in these areas.
    eval_split: val
    extra_params: {}
    few_shot_num: 5
    few_shot_prompt_template: '以下是一些示例问题：


      {fewshot}


      '
    few_shot_random: false
    filters: null
    force_redownload: false
    metric_list:
    - acc
    name: ceval
    output_types:
    - generation
    pretty_name: C-Eval
    prompt_template: '以下是中国关于{subject}的单项选择题，请选出其中的正确答案。你的回答的最后一行应该是这样的格式："答案：[LETTER]"（不带引号），其中
      [LETTER] 是 A、B、C、D 中的一个。


      问题：{question}

      选项：

      {choices}

      '
    query_template: null
    review_timeout: null
    sandbox_config: {}
    shuffle: false
    shuffle_choices: false
    subset_list:
    - computer_network
    - operating_system
    - computer_architecture
    - college_programming
    - college_physics
    - college_chemistry
    - advanced_mathematics
    - probability_and_statistics
    - discrete_mathematics
    - electrical_engineer
    - metrology_engineer
    - high_school_mathematics
    - high_school_physics
    - high_school_chemistry
    - high_school_biology
    - middle_school_mathematics
    - middle_school_biology
    - middle_school_physics
    - middle_school_chemistry
    - veterinary_medicine
    - college_economics
    - business_administration
    - marxism
    - mao_zedong_thought
    - education_science
    - teacher_qualification
    - high_school_politics
    - high_school_geography
    - middle_school_politics
    - middle_school_geography
    - modern_chinese_history
    - ideological_and_moral_cultivation
    - logic
    - law
    - chinese_language_and_literature
    - art_studies
    - professional_tour_guide
    - legal_professional
    - high_school_chinese
    - high_school_history
    - middle_school_history
    - civil_servant
    - sports_science
    - plant_protection
    - basic_medicine
    - clinical_medicine
    - urban_and_rural_planner
    - accountant
    - fire_engineer
    - environmental_impact_assessment_engineer
    - tax_accountant
    - physician
    system_prompt: null
    tags:
    - Knowledge
    - MCQ
    - Chinese
    train_split: dev
  mmlu:
    aggregation: mean
    dataset_id: cais/mmlu
    default_subset: all
    description: The MMLU (Massive Multitask Language Understanding) benchmark is
      a comprehensive evaluation suite designed to assess the performance of language
      models across a wide range of subjects and tasks. It includes multiple-choice
      questions from various domains, such as history, science, mathematics, and more,
      providing a robust measure of a model's understanding and reasoning capabilities.
    eval_split: test
    extra_params: {}
    few_shot_num: 5
    few_shot_prompt_template: null
    few_shot_random: false
    filters: null
    force_redownload: false
    metric_list:
    - acc
    name: mmlu
    output_types:
    - generation
    pretty_name: MMLU
    prompt_template: 'Answer the following multiple choice question. The last line
      of your response should be of the following format: ''ANSWER: [LETTER]'' (without
      quotes) where [LETTER] is one of {letters}. Think step by step before answering.


      {question}


      {choices}'
    query_template: null
    review_timeout: null
    sandbox_config: {}
    shuffle: false
    shuffle_choices: false
    subset_list:
    - abstract_algebra
    - anatomy
    - astronomy
    - business_ethics
    - clinical_knowledge
    - college_biology
    - college_chemistry
    - college_computer_science
    - college_mathematics
    - college_medicine
    - college_physics
    - computer_security
    - conceptual_physics
    - econometrics
    - electrical_engineering
    - elementary_mathematics
    - formal_logic
    - global_facts
    - high_school_biology
    - high_school_chemistry
    - high_school_computer_science
    - high_school_european_history
    - high_school_geography
    - high_school_government_and_politics
    - high_school_macroeconomics
    - high_school_mathematics
    - high_school_microeconomics
    - high_school_physics
    - high_school_psychology
    - high_school_statistics
    - high_school_us_history
    - high_school_world_history
    - human_aging
    - human_sexuality
    - international_law
    - jurisprudence
    - logical_fallacies
    - machine_learning
    - management
    - marketing
    - medical_genetics
    - miscellaneous
    - moral_disputes
    - moral_scenarios
    - nutrition
    - philosophy
    - prehistory
    - professional_accounting
    - professional_law
    - professional_medicine
    - professional_psychology
    - public_relations
    - security_studies
    - sociology
    - us_foreign_policy
    - virology
    - world_religions
    system_prompt: null
    tags:
    - Knowledge
    - MCQ
    train_split: dev
dataset_dir: /root/.cache/modelscope/hub/datasets
dataset_hub: modelscope
datasets:
- mmlu
- ceval
debug: false
eval_backend: Native
eval_batch_size: 1
eval_config: null
eval_type: llm_ckpt
evalscope_version: 1.3.0
generation_config:
  batch_size: 1
  do_sample: false
  max_tokens: 2048
  n: 1
  temperature: 1.0
  top_k: 50
  top_p: 1.0
ignore_errors: false
judge_model_args: {}
judge_strategy: auto
judge_worker_num: 1
limit: 5
model: /root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-hf
model_args:
  device_map: auto
  precision: torch.bf16
  revision: master
model_id: Llama-2-7b-hf
model_task: text_generation
repeats: 1
rerun_review: false
sandbox_manager_config: {}
sandbox_type: docker
seed: 42
stream: null
timeout: null
use_cache: null
use_sandbox: false
work_dir: ./outputs/20251211_143112
