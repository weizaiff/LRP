{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46fcc50b-4ef6-4618-982b-c31a8ace467e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    20251210:\\n        使用vLLM测试模型的mmlu指标--- Failed\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    20251210:\n",
    "        使用vLLM测试模型的mmlu指标--- Failed\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec9e5a0-6925-4dc7-875f-fbec88a838ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5e67322-2b24-4d23-b2c6-795d66092699",
   "metadata": {},
   "source": [
    "# try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0811f29-c2e4-4aa1-8cac-60f2e3fe555f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n生成checkpoint\\n\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "生成checkpoint\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4831a1-c19f-4fea-a636-11afd5710791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc357a14-8e69-47e3-97ba-e09a4987e04a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3b78b-d13e-4b54-a013-2dc4aa5cecf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee3994-fd45-483c-8734-d58ce6c92256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf5443-98a6-4fc2-8355-1f80f286a13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4580dd-d8ae-4884-b002-1b60777cc18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdac94f-44ee-4ad8-bff1-1c3d2fe68d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b091883c-ba3e-4cee-a0e6-4df438f63257",
   "metadata": {},
   "source": [
    "# try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2031c54e-1def-4eb0-9864-56943bd792a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "# download checkpoint\n",
    "from accelerate import load_checkpoint_and_dispatch\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To prevent long warnings :)\n",
    "\n",
    "#from accelerate import load_checkpoint_and_dispatch\n",
    "\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import util\n",
    "import importlib\n",
    "\n",
    "importlib.reload(util)      # 只能 reload 模块本身\n",
    "from util import calc_ppl, get_test_data, get_open_ended_answer, get_open_ended_answer_vllm   # reload 后再重新 import 函数\n",
    "\n",
    "import copy\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3655e51c-4a7b-4b0e-b415-a231317dd5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint):\n",
    "    config = AutoConfig.from_pretrained(checkpoint,trust_remote_code=True)\n",
    "    print('checkpoint:', checkpoint)\n",
    "    \n",
    "    \n",
    "    if True:\n",
    "    \n",
    "        device_map='cuda'\n",
    "        model= AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True, torch_dtype= torch.bfloat16,device_map=device_map ) # for download model weight\n",
    "    \n",
    "    else:\n",
    "        with init_empty_weights():\n",
    "            model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n",
    "    \n",
    "        model = load_checkpoint_and_dispatch(\n",
    "            model, checkpoint, device_map=\"auto\", dtype=torch.bfloat16#, no_split_module_classes=[\"GPTJBlock\"]\n",
    "        )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)#AutoTokenizer.from_pretrained(\"/home/work/lyftri/projects/model_zoo/compass_sea_13b_s4_merge2HF_org_convert_TP_1_PP_2\",  trust_remote_code=True)#AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e938e5c-854f-402f-a198-3b4ed37d43f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_mask_neuron_model(model, activation_mask_path, need_lang):\n",
    "\n",
    "    if activation_mask_path:\n",
    "        activation_masks = torch.load(activation_mask_path)\n",
    "    else:\n",
    "        activation_masks = [None]\n",
    "    \n",
    "    final_output = []\n",
    "    if is_llama:\n",
    "        languages = [\"en\", \"zh\", \"fr\", \"es\", \"vi\", \"id\", \"ja\"]\n",
    "    else:\n",
    "        languages = [\"en\", \"zh\", \"fr\", \"es\", \"vi\", \"id\"]\n",
    "    \n",
    "    need_lang = [need_lang]#['en', 'zh', 'vi']\n",
    "    \n",
    "    for activation_mask, mask_lang in zip(activation_masks, languages):\n",
    "    \n",
    "        if mask_lang not in need_lang:continue\n",
    "            \n",
    "        print(f'get mask =====lang:{mask_lang}=====')\n",
    "    \n",
    "        \n",
    "        if activation_mask:\n",
    "            def factory(mask):\n",
    "                def llama_forward_org(self, x):\n",
    "                    gate_up, _ = self.gate_up_proj(x)  # b, l, 2i\n",
    "                    i = gate_up.size(-1)\n",
    "                    activation = F.silu(gate_up[:, :, : i // 2])\n",
    "                    activation.index_fill_(2, mask, 0)\n",
    "                    x = activation * gate_up[:, :, i // 2 :]\n",
    "                    x, _ = self.down_proj(x)\n",
    "                    return x\n",
    "                def llama_forward(self, x):\n",
    "                    gate_ = self.gate_proj(x)  # b, l, 2i\n",
    "                    i = gate_.size(-1)\n",
    "                    activation = F.silu(gate_)\n",
    "                    activation.index_fill_(2, mask, 0)\n",
    "                    x = activation * self.up_proj(x)\n",
    "                    x = self.down_proj(x)\n",
    "                    return x\n",
    "    \n",
    "                def bloom_forward(self, x: torch.Tensor):\n",
    "                    x, _ = self.dense_h_to_4h(x)\n",
    "                    x = self.gelu_impl(x)\n",
    "                    x.index_fill_(2, mask, 0)\n",
    "                    x, _ = self.dense_4h_to_h(x)\n",
    "                    return x\n",
    "    \n",
    "                if is_llama:\n",
    "                    return llama_forward\n",
    "                else:\n",
    "                    return bloom_forward\n",
    "    \n",
    "            for i, layer_mask in enumerate(activation_mask):\n",
    "                if is_llama:\n",
    "                    obj = model.model.layers[i].mlp\n",
    "                else:\n",
    "                    obj = model.model.layers[i].mlp\n",
    "                obj.forward = MethodType(factory(layer_mask.to('cuda')), obj)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_mask_neuron_model_vllm(model, activation_mask_path, need_lang):\n",
    "\n",
    "    if activation_mask_path:\n",
    "        activation_masks = torch.load(activation_mask_path)\n",
    "    else:\n",
    "        activation_masks = [None]\n",
    "    \n",
    "    final_output = []\n",
    "    if is_llama:\n",
    "        languages = [\"en\", \"zh\", \"fr\", \"es\", \"vi\", \"id\", \"ja\"]\n",
    "    else:\n",
    "        languages = [\"en\", \"zh\", \"fr\", \"es\", \"vi\", \"id\"]\n",
    "    \n",
    "    need_lang = [need_lang]#['en', 'zh', 'vi']\n",
    "    \n",
    "    for activation_mask, mask_lang in zip(activation_masks, languages):\n",
    "    \n",
    "        if mask_lang not in need_lang:continue\n",
    "            \n",
    "        print(f'get mask =====lang:{mask_lang}=====')\n",
    "    \n",
    "        \n",
    "        if activation_mask:\n",
    "            def factory(mask):\n",
    "                def llama_forward(self, x):\n",
    "                    gate_up, _ = self.gate_up_proj(x)  # b, l, 2i\n",
    "                    i = gate_up.size(-1)\n",
    "                    activation = F.silu(gate_up[:, :, : i // 2])\n",
    "                    activation.index_fill_(2, mask, 0)\n",
    "                    x = activation * gate_up[:, :, i // 2 :]\n",
    "                    x, _ = self.down_proj(x)\n",
    "                    return x\n",
    "                def llama_forward_split(self, x):\n",
    "                    gate_ = self.gate_proj(x)  # b, l, 2i\n",
    "                    i = gate_.size(-1)\n",
    "                    activation = F.silu(gate_)\n",
    "                    #activation.index_fill_(2, mask, 0)\n",
    "\n",
    "                    # test\n",
    "                    print(activation.shape)\n",
    "                    activation.index_fill_(2, torch.tensor(list(range(activation.shape[1]))), 0 )\n",
    "                    x = activation * self.up_proj(x)\n",
    "                    x = self.down_proj(x)\n",
    "                    return x\n",
    "    \n",
    "                def bloom_forward(self, x: torch.Tensor):\n",
    "                    x, _ = self.dense_h_to_4h(x)\n",
    "                    x = self.gelu_impl(x)\n",
    "                    x.index_fill_(2, mask, 0)\n",
    "                    x, _ = self.dense_4h_to_h(x)\n",
    "                    return x\n",
    "    \n",
    "                if is_llama:\n",
    "                    return llama_forward\n",
    "                else:\n",
    "                    return bloom_forward\n",
    "    \n",
    "            for i, layer_mask in enumerate(activation_mask): \n",
    "                #print('ilayer:',i, layer_mask)\n",
    "                #if is_llama:\n",
    "                    # latest\n",
    "                    #obj = model.llm_engine.model_executor.driver_worker.model_runner.model.model.layers[i].mlp\n",
    "                #else:\n",
    "                    # latest\n",
    "                    #obj = model.llm_engine.model_executor.driver_worker.model_runner.model.transformer.h[i].mlp\n",
    "\n",
    "                if is_llama:\n",
    "                    obj = model.llm_engine.driver_worker.model_runner.model.model.layers[i].mlp\n",
    "                else:\n",
    "                    obj = model.llm_engine.driver_worker.model_runner.model.transformer.h[i].mlp\n",
    "                obj.forward = MethodType(factory(layer_mask.to('cuda')), obj)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "927e6db2-dfd8-49e8-84b4-2abddb0dd851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"VLLM_ENABLE_V1_MULTIPROCESSING\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7b433f8-38fa-4dd5-bb8d-b4c1a017c0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-10 15:18:02 llm_engine.py:70] Initializing an LLM engine with config: model='/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-hf', tokenizer='/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=True, seed=0)\n",
      "INFO 12-10 15:21:44 llm_engine.py:275] # GPU blocks: 1923, # CPU blocks: 512\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from types import MethodType\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "#model_path ='/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "model_path ='/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-hf'\n",
    "activation_mask_path='/root/autodl-fs/Language-Specific-Neurons/LLaMA-2-7B.neuron.pth'\n",
    "\n",
    "is_llama = bool(model_path.find('llama') >= 0)\n",
    "#model, tokenizer = load_model(model_path)\n",
    "\n",
    "# 初始化 vLLM\n",
    "llm =LLM(model=model_path, tensor_parallel_size=torch.cuda.device_count(), enforce_eager=True) #LLM(model=model_path, tensor_parallel_size=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a71dc3af-f9e4-4333-8e8f-a5daa70968b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "from vllm import SamplingParams\n",
    "\n",
    "\n",
    "LETTER_MAP = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "\n",
    "def extract_answer(text: str):\n",
    "    # Parse A/B/C/D\n",
    "    m = re.search(r'\\b([ABCD])\\b', text.upper())\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "def build_5shot_prompt(subject, train_samples, test_item):\n",
    "    # choose 5 examples\n",
    "    shots = random.sample(train_samples, 5)\n",
    "\n",
    "    prompt = f\"The following are multiple choice questions (with answers) about {subject}.\\n\\n\"\n",
    "    \n",
    "    for item in shots:\n",
    "        prompt += \"Q: \" + item[\"question\"] + \"\\n\"\n",
    "        for i, ch in enumerate(item[\"choices\"]):\n",
    "            prompt += f\"{LETTER_MAP[i]}. {ch}\\n\"\n",
    "        prompt += f\"Answer: {LETTER_MAP[item['answer']]}\\n\\n\"\n",
    "\n",
    "    # test question\n",
    "    prompt += \"Q: \" + test_item[\"question\"] + \"\\n\"\n",
    "    for i, ch in enumerate(test_item[\"choices\"]):\n",
    "        prompt += f\"{LETTER_MAP[i]}. {ch}\\n\"\n",
    "    prompt += \"Answer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def eval_mmlu_vllm_5shot(llm, max_new_tokens=4, verbose=True):\n",
    "    ds = load_dataset(\"cais/mmlu\", 'all')\n",
    "    train = ds[\"train\"]\n",
    "    test = ds[\"test\"]\n",
    "\n",
    "    # group by subject\n",
    "    train_groups = defaultdict(list)\n",
    "    test_groups = defaultdict(list)\n",
    "    \n",
    "    for item in train:\n",
    "        train_groups[item[\"subject\"]].append(item)\n",
    "    for item in test:\n",
    "        test_groups[item[\"subject\"]].append(item)\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=max_new_tokens,\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    subject_scores = {}\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    subjects = sorted(train_groups.keys())\n",
    "    if verbose:\n",
    "        pbar = tqdm(subjects, desc=\"Evaluating MMLU 5-shot\")\n",
    "\n",
    "    for subject in subjects:\n",
    "        train_samples = train_groups[subject]\n",
    "        test_samples = test_groups[subject]\n",
    "\n",
    "        prompts = []\n",
    "        gt = []\n",
    "\n",
    "        # build prompts\n",
    "        for item in test_samples:\n",
    "            prompts.append(build_5shot_prompt(subject, train_samples, item))\n",
    "            gt.append(LETTER_MAP[item[\"answer\"]])\n",
    "\n",
    "        # batch inference\n",
    "        outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "        correct = 0\n",
    "        for out, label in zip(outputs, gt):\n",
    "            text = out.outputs[0].text.strip()\n",
    "            pred = extract_answer(text)\n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "\n",
    "        acc = correct / len(gt)\n",
    "        subject_scores[subject] = acc\n",
    "\n",
    "        total_correct += correct\n",
    "        total_count += len(gt)\n",
    "\n",
    "        if verbose:\n",
    "            pbar.set_postfix({subject: f\"{acc:.3f}\"})\n",
    "            pbar.update(1)\n",
    "\n",
    "    overall = total_correct / total_count\n",
    "    return overall, subject_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96a805a4-56d1-4779-947a-73b2dd4972a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c03236-a3ba-4f5a-af52-272fd0951670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "568e2bd3-185b-4163-b892-930e58dccf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62393bcd54a14ba4922d0f6b16acab05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "all/test-00000-of-00001.parquet:   0%|          | 0.00/3.50M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee42442801c4d86815aaf6e647c0342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "all/validation-00000-of-00001.parquet:   0%|          | 0.00/408k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e447d781014b0683e399abce486e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "all/dev-00000-of-00001.parquet:   0%|          | 0.00/76.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a238a15be004a8da6ba9c390b3109af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "all/auxiliary_train-00000-of-00001.parqu(…):   0%|          | 0.00/47.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fe3115741947c8b8f901883f07d338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/14042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2aa6b90ec64d7996a02d2d384275a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1531 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f59b5f432e4364abb6f14bb63f5962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a167a10467d5494cadcd765c445da92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating auxiliary_train split:   0%|          | 0/99842 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"cais/mmlu\", 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "beb1dde0-e002-4e24-88d9-b4c219fa51d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/cais/mmlu/resolve/main/README.md (Caused by ProxyError('Cannot connect to proxy.', TimeoutError('_ssl.c:980: The handshake operation timed out')))\"), '(Request ID: bbc938b9-de82-4260-944b-8ed2be8edc9c)')' thrown while requesting HEAD https://huggingface.co/datasets/cais/mmlu/resolve/main/README.md\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m overall_acc, scores \u001b[38;5;241m=\u001b[39m \u001b[43meval_mmlu_vllm_5shot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall MMLU:\u001b[39m\u001b[38;5;124m\"\u001b[39m, overall_acc)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(scores)\n",
      "Cell \u001b[0;32mIn[24], line 40\u001b[0m, in \u001b[0;36meval_mmlu_vllm_5shot\u001b[0;34m(llm, max_new_tokens, verbose)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_mmlu_vllm_5shot\u001b[39m(llm, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     39\u001b[0m     ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcais/mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m     train \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     41\u001b[0m     test \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# group by subject\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/dataset_dict.py:86\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     89\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     90\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train'"
     ]
    }
   ],
   "source": [
    "overall_acc, scores = eval_mmlu_vllm_5shot(llm)\n",
    "print(\"Overall MMLU:\", overall_acc)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae44b63d-0dff-4a41-bee4-1b664e61bcd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
