{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46fcc50b-4ef6-4618-982b-c31a8ace467e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    20251210/20251211:\\n        使用vLLM测试模型的mmlu指标--- Failed\\n        generate ckpt& save\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    20251210/20251211:\n",
    "        使用vLLM测试模型的mmlu指标--- Failed\n",
    "        generate ckpt& save\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec9e5a0-6925-4dc7-875f-fbec88a838ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "# download checkpoint\n",
    "from accelerate import load_checkpoint_and_dispatch\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To prevent long warnings :)\n",
    "\n",
    "#from accelerate import load_checkpoint_and_dispatch\n",
    "\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import util\n",
    "import importlib\n",
    "\n",
    "importlib.reload(util)      # 只能 reload 模块本身\n",
    "from util import calc_ppl, get_test_data, get_open_ended_answer, get_open_ended_answer_vllm   # reload 后再重新 import 函数\n",
    "\n",
    "import copy\n",
    "import datasets\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e67322-2b24-4d23-b2c6-795d66092699",
   "metadata": {},
   "source": [
    "# try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0811f29-c2e4-4aa1-8cac-60f2e3fe555f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n生成checkpoint\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "生成checkpoint\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1752abf4-92f9-4c46-abda-5b58358887d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint):\n",
    "    config = AutoConfig.from_pretrained(checkpoint,trust_remote_code=True)\n",
    "    print('checkpoint:', checkpoint)\n",
    "    \n",
    "    \n",
    "    if True:\n",
    "    \n",
    "        device_map='auto'\n",
    "        model= AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True, torch_dtype= torch.bfloat16,device_map=device_map,weights_only=False ) # for download model weight\n",
    "    \n",
    "    else:\n",
    "        with init_empty_weights():\n",
    "            model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n",
    "    \n",
    "        model = load_checkpoint_and_dispatch(\n",
    "            model, checkpoint, device_map=\"auto\", dtype=torch.bfloat16#, no_split_module_classes=[\"GPTJBlock\"]\n",
    "        )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)#AutoTokenizer.from_pretrained(\"/home/work/lyftri/projects/model_zoo/compass_sea_13b_s4_merge2HF_org_convert_TP_1_PP_2\",  trust_remote_code=True)#AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def set_neuron_zero(model, neuron_list):\n",
    "    '''\n",
    "    将指定的neuron设置为zero\n",
    "\n",
    "    '''\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    for ikey,_ in neuron_list:\n",
    "        w_name = ikey.split('_index_')[0]\n",
    "        neuron_index = int(ikey.split('_index_')[1])\n",
    "\n",
    "        #set zero\n",
    "        state_dict[w_name][neuron_index,:] =0 #1 #0 #-1  #-63.75#0\n",
    "    return model\n",
    "    \n",
    "\n",
    "def save_model(model, tokenizer, save_dir):\n",
    "    model.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8698f5d5-88d4-4a59-bfb8-7eba3e567d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "kus mask neuron\n",
    "\n",
    "'''\n",
    "'''\n",
    "    convert LRP_based neuron to LAPE format\n",
    "\n",
    "'''\n",
    "import re\n",
    "\n",
    "def convert_LAPE_format(neuron_en, config):\n",
    "\n",
    "    layer_list = [{'up_proj':[], 'gate_proj':[], 'down_proj':[]} for _ in range(config.num_hidden_layers)]\n",
    "\n",
    "    if isinstance(neuron_en, dict):\n",
    "        new_tmp = []\n",
    "        for ineuron, iscore in neuron_en.items():\n",
    "            new_tmp.append((ineuron, iscore))\n",
    "\n",
    "        neuron_en = new_tmp\n",
    "        \n",
    "    \n",
    "    for ineuron, iscore in neuron_en:\n",
    "    \n",
    "        \n",
    "        m = re.search(r'layers?\\.(\\d+).mlp.(.+).weight_index_([0-9]+)', ineuron)\n",
    "        \n",
    "        layer_index = int(m.group(1))\n",
    "        layer_type = m.group(2)\n",
    "        layer_neuron_index = int(m.group(3))\n",
    "        assert layer_type in ['up_proj', 'gate_proj', 'down_proj']\n",
    "    \n",
    "        layer_list[layer_index][layer_type].append(int(layer_neuron_index))\n",
    "\n",
    "    return layer_list\n",
    "\n",
    "def get_mask_neuron_model_LRP(model, activation_mask_dict, is_llama =True):\n",
    "\n",
    "        # get state_dict \n",
    "        state_dict = model.state_dict()\n",
    "        if activation_mask_dict:\n",
    "            def factory(mask):\n",
    "                def llama_forward_lrp(self, x):\n",
    "                    '''\n",
    "                        mask: {'up_proj':[...], 'gate_proj':[...], 'down_proj':[...]}\n",
    "                        \n",
    "                    '''\n",
    "                    gate_up, _ = self.gate_up_proj(x)  # b, l, 2i\n",
    "                    i = gate_up.size(-1)\n",
    "                    activation = F.silu(gate_up[:, :, : i // 2])\n",
    "                    if 'gate_proj' in mask:\n",
    "                        activation.index_fill_(2, mask['gate_proj'], 0)\n",
    "\n",
    "                    if 'up_proj' in mask:\n",
    "                        x = activation * gate_up[:, :, i // 2 :].index_fill_(2, mask['up_proj'], 0)\n",
    "                    else:\n",
    "                        x = activation * gate_up[:, :, i // 2 :]\n",
    "                    x, _ = self.down_proj(x)\n",
    "                    if 'down_proj' in mask:\n",
    "                        x.index_fill_(2, mask['down_proj'], 0)\n",
    "                    return x\n",
    "                def llama_forward(self, x):\n",
    "                    gate_up, _ = self.gate_up_proj(x)  # b, l, 2i\n",
    "                    i = gate_up.size(-1)\n",
    "                    activation = F.silu(gate_up[:, :, : i // 2])\n",
    "                    activation.index_fill_(2, mask, 0)\n",
    "                    x = activation * gate_up[:, :, i // 2 :]\n",
    "                    x, _ = self.down_proj(x)\n",
    "                    return x\n",
    "                def llama_forward_split(self, x):\n",
    "                    gate_ = self.gate_proj(x)  # b, l, 2i\n",
    "                    i = gate_.size(-1)\n",
    "                    activation = F.silu(gate_)\n",
    "                    #activation.index_fill_(2, mask, 0)\n",
    "\n",
    "                    # test\n",
    "                    print(activation.shape)\n",
    "                    activation.index_fill_(2, torch.tensor(list(range(activation.shape[1]))), 0 )\n",
    "                    x = activation * self.up_proj(x)\n",
    "                    x = self.down_proj(x)\n",
    "                    return x\n",
    "    \n",
    "                def bloom_forward(self, x: torch.Tensor):\n",
    "                    x, _ = self.dense_h_to_4h(x)\n",
    "                    x = self.gelu_impl(x)\n",
    "                    x.index_fill_(2, mask, 0)\n",
    "                    x, _ = self.dense_4h_to_h(x)\n",
    "                    return x\n",
    "    \n",
    "                if is_llama:\n",
    "                    return llama_forward_lrp\n",
    "                else:\n",
    "                    return bloom_forward\n",
    "    \n",
    "            for i, ilayer_mask_dict in enumerate(activation_mask_dict): \n",
    "                #print('ilayer:',i, layer_mask)\n",
    "                #if is_llama:\n",
    "                    # latest\n",
    "                    #obj = model.llm_engine.model_executor.driver_worker.model_runner.model.model.layers[i].mlp\n",
    "                #else:\n",
    "                    # latest\n",
    "                    #obj = model.llm_engine.model_executor.driver_worker.model_runner.model.transformer.h[i].mlp\n",
    "\n",
    "                if is_llama:\n",
    "                    #obj = model.llm_engine.driver_worker.model_runner.model.model.layers[i].mlp\n",
    "\n",
    "                    ilayer_mask_dict_cuda = {}\n",
    "                    for ikey ,ival in ilayer_mask_dict.items():\n",
    "                        if len(ival)>0:\n",
    "                            #ilayer_mask_dict_cuda[ikey] = torch.LongTensor(ival).to('cuda')\n",
    "                            \n",
    "                            state_dict[f'model.layers.{i}.mlp.{ikey}.weight'][torch.LongTensor(ival),:] = 0       \n",
    "                else:\n",
    "                    #obj = model.llm_engine.driver_worker.model_runner.model.transformer.h[i].mlp\n",
    "                    assert 1 ==0\n",
    "                \n",
    "                \n",
    "                #obj.forward = MethodType(factory(ilayer_mask_dict_cuda), obj)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "047d2055-fdb2-4a2b-b79a-ad412ee99d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LAPE mask neuron\n",
    "\n",
    "'''\n",
    "\n",
    "def get_mask_neuron_model(model, activation_mask_path, need_lang, is_llama= True):\n",
    "\n",
    "    # get state_dict \n",
    "    state_dict = model.state_dict()\n",
    "    \n",
    "    if activation_mask_path:\n",
    "        activation_masks = torch.load(activation_mask_path)\n",
    "    else:\n",
    "        activation_masks = [None]\n",
    "    \n",
    "    final_output = []\n",
    "    if is_llama:\n",
    "        languages = [\"en\", \"zh\", \"fr\", \"es\", \"vi\", \"id\", \"ja\"]\n",
    "    else:\n",
    "        languages = [\"en\", \"zh\", \"fr\", \"es\", \"vi\", \"id\"]\n",
    "    \n",
    "    need_lang = [need_lang]#['en', 'zh', 'vi']\n",
    "    \n",
    "    for activation_mask, mask_lang in zip(activation_masks, languages):\n",
    "    \n",
    "        if mask_lang not in need_lang:continue\n",
    "            \n",
    "        print(f'get mask =====lang:{mask_lang}=====')\n",
    "    \n",
    "        \n",
    "        if activation_mask:\n",
    "            def factory(mask):\n",
    "                def llama_forward(self, x):\n",
    "                    gate_up, _ = self.gate_up_proj(x)  # b, l, 2i\n",
    "                    i = gate_up.size(-1)\n",
    "                    activation = F.silu(gate_up[:, :, : i // 2])\n",
    "                    activation.index_fill_(2, mask, 0)\n",
    "                    x = activation * gate_up[:, :, i // 2 :]\n",
    "                    x, _ = self.down_proj(x)\n",
    "                    return x\n",
    "                def llama_forward_split(self, x):\n",
    "                    gate_ = self.gate_proj(x)  # b, l, 2i\n",
    "                    i = gate_.size(-1)\n",
    "                    activation = F.silu(gate_)\n",
    "                    #activation.index_fill_(2, mask, 0)\n",
    "\n",
    "                    # test\n",
    "                    print(activation.shape)\n",
    "                    activation.index_fill_(2, torch.tensor(list(range(activation.shape[1]))), 0 )\n",
    "                    x = activation * self.up_proj(x)\n",
    "                    x = self.down_proj(x)\n",
    "                    return x\n",
    "    \n",
    "                def bloom_forward(self, x: torch.Tensor):\n",
    "                    x, _ = self.dense_h_to_4h(x)\n",
    "                    x = self.gelu_impl(x)\n",
    "                    x.index_fill_(2, mask, 0)\n",
    "                    x, _ = self.dense_4h_to_h(x)\n",
    "                    return x\n",
    "    \n",
    "                if is_llama:\n",
    "                    return llama_forward\n",
    "                else:\n",
    "                    return bloom_forward\n",
    "    \n",
    "            for i, layer_mask in enumerate(activation_mask): \n",
    "                #print('ilayer:',i, layer_mask)\n",
    "                if is_llama:\n",
    "                    #obj = model.llm_engine.driver_worker.model_runner.model.model.layers[i].mlp\n",
    "                    \n",
    "                    # just mask neuron\n",
    "                    # only masj gate proj\n",
    "                    \n",
    "                    #model.model.layers[i].mlp.gate_proj = \n",
    "                    state_dict[f'model.layers.{i}.mlp.gate_proj.weight'][layer_mask,:] = 0\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    #obj = model.llm_engine.driver_worker.model_runner.model.transformer.h[i].mlp\n",
    "                    assert 1==0\n",
    "                #obj.forward = MethodType(factory(layer_mask.to('cuda')), obj)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bff21c-f31b-4d92-99fc-340900ff8e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc517745-c9ce-4476-bfcc-273719dd6d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57cd7b90-d23b-4cc9-a30e-4fe19acf65f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_model_path ='/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e4b98-b74a-4d0f-ad9a-853f630306a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1555262a-1ed0-4220-b44a-0bedde2c1b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint: /root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13e00711b704a0eb93f725957fa9279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load_model(org_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6200d859-5f2d-4c9d-983a-ba0cbe928fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(org_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea522df-c3e6-442d-807f-ae3308fa9316",
   "metadata": {},
   "source": [
    "# Generate LAPE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23cc71f7-1b70-4a98-bdba-3ee9cf55a6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get mask =====lang:zh=====\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "get LAPE model\n",
    "\n",
    "'''\n",
    "import gc\n",
    "save_dir='/root/autodl-tmp/llama2_7b_base_BaseNeuron_BaseMask_lang_{}'\n",
    "\n",
    "lang_list = ['zh']#['en', 'vi', 'zh']\n",
    "activation_mask_path='/root/autodl-fs/Language-Specific-Neurons/LLaMA-2-7B.neuron.pth'\n",
    "for ilang in lang_list:\n",
    "    \n",
    "    tmp_model = get_mask_neuron_model(copy.deepcopy(model), activation_mask_path, ilang)\n",
    "    save_model(model, tokenizer, save_dir.format(ilang))\n",
    "\n",
    "    tmp_model.cpu()\n",
    "    del tmp_model\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7449de-276e-40cc-9fe8-bf17a44a89c4",
   "metadata": {},
   "source": [
    "# generate LRP MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fb61671-6cd5-41b2-b4e8-f09d6bc5de3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-684., device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()[f'model.layers.0.mlp.gate_proj.weight'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3ae92-0f6c-4de0-bfb7-4a05b43a3597",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get lrp model\n",
    "\n",
    "'''\n",
    "import gc\n",
    "save_dir='/root/autodl-tmp/LRP_llama2_7b_base_BaseNeuron_BaseMask_lang_{}'\n",
    "\n",
    "lang_list = ['en', 'vi', 'zh']\n",
    "activation_mask_path_map = {\n",
    "    'en': '/root/autodl-fs/LRP_kur_res/20251210_5000samples_cal_llama2_7b_base/th_1_selected_LRP_kur_res_en_zscore.pt',\n",
    "    'vi': '/root/autodl-fs/LRP_kur_res/20251210_5000samples_cal_llama2_7b_base/th_1_selected_LRP_kur_res_vi_zscore.pt',\n",
    "    'zh': '/root/autodl-fs/LRP_kur_res/20251210_5000samples_cal_llama2_7b_base/th_1_selected_LRP_kur_res_zh_zscore.pt'\n",
    "}\n",
    "for ilang in lang_list:\n",
    "    # LRP version\n",
    "    tmp_model = get_mask_neuron_model_LRP(copy.deepcopy(model), convert_LAPE_format(torch.load(activation_mask_path_map[ilang]), config), ilang)\n",
    "    save_model(model, tokenizer, save_dir.format(ilang))\n",
    "\n",
    "    tmp_model.cpu()\n",
    "    del tmp_model\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3043b0db-7c48-4a8e-980a-702a9135057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /root/autodl-fs/LRP_kur_res/20251210_5000samples_cal_llama2_7b_base/th_1_selected_LRP_kur_res_en_zscore.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520cb5b6-7b1c-4f48-871c-3b64987c1a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e80c4-0b7a-4c4a-ad04-e5bf828a2112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1272ee1-f91e-455d-95a2-8440873b99ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f4a13-410d-4bc5-945d-3042c7a993a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da8308-71d2-4255-bd36-95e0373b7330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95525ae6-88ad-492e-939e-0bb0edcf1ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53d4d954-9d01-4509-b2e3-21cb4dd96d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-668., device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model.state_dict()[f'model.layers.0.mlp.gate_proj.weight'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a1c8580-3724-438b-b971-3e545ab77964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-684., device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()[f'model.layers.0.mlp.gate_proj.weight'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1afeca6-2147-4bda-8254-7b917ed4aff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-720., device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model.state_dict()[f'model.layers.0.mlp.gate_proj.weight'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712717aa-0c58-4ed9-bf00-0c180b1cdf6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b4cb864-0fda-46b8-bd4e-7f26257fb9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "382aee8c-1071-4ddb-8f59-d75a63ef87bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaDecoderLayer(\n",
       "  (self_attn): LlamaAttention(\n",
       "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "  )\n",
       "  (mlp): LlamaMLP(\n",
       "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "    (act_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dbb9357-747f-4652-b12c-515229b0b5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4713f84b-2dcf-42fb-8a38-c9344f2a2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_name = 'llama2_7b_base_LAPE_mask_'\n",
    "save_path = '/root/autodl-tmp/'\n",
    "save_path = os.path.join(save_path, save_name)\n",
    "\n",
    "\n",
    "# LAPE\n",
    "neuron_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4831a1-c19f-4fea-a636-11afd5710791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc357a14-8e69-47e3-97ba-e09a4987e04a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3b78b-d13e-4b54-a013-2dc4aa5cecf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee3994-fd45-483c-8734-d58ce6c92256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf5443-98a6-4fc2-8355-1f80f286a13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4580dd-d8ae-4884-b002-1b60777cc18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdac94f-44ee-4ad8-bff1-1c3d2fe68d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b091883c-ba3e-4cee-a0e6-4df438f63257",
   "metadata": {},
   "source": [
    "# try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2031c54e-1def-4eb0-9864-56943bd792a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3655e51c-4a7b-4b0e-b415-a231317dd5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint):\n",
    "    config = AutoConfig.from_pretrained(checkpoint,trust_remote_code=True)\n",
    "    print('checkpoint:', checkpoint)\n",
    "    \n",
    "    \n",
    "    if True:\n",
    "    \n",
    "        device_map='cuda'\n",
    "        model= AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True, torch_dtype= torch.bfloat16,device_map=device_map ) # for download model weight\n",
    "    \n",
    "    else:\n",
    "        with init_empty_weights():\n",
    "            model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n",
    "    \n",
    "        model = load_checkpoint_and_dispatch(\n",
    "            model, checkpoint, device_map=\"auto\", dtype=torch.bfloat16#, no_split_module_classes=[\"GPTJBlock\"]\n",
    "        )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)#AutoTokenizer.from_pretrained(\"/home/work/lyftri/projects/model_zoo/compass_sea_13b_s4_merge2HF_org_convert_TP_1_PP_2\",  trust_remote_code=True)#AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e938e5c-854f-402f-a198-3b4ed37d43f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_mask_neuron_model(model, activation_mask_path, need_lang):\n",
    "\n",
    "    if activation_mask_path:\n",
    "        activation_masks = torch.load(activation_mask_path)\n",
    "    else:\n",
    "        activation_masks = [None]\n",
    "    \n",
    "    final_output = []\n",
    "    if is_llama:\n",
    "        languages = [\"en\", \"zh\", \"fr\", \"es\", \"vi\", \"id\", \"ja\"]\n",
    "    else:\n",
    "        languages = [\"en\", \"zh\", \"fr\", \"es\", \"vi\", \"id\"]\n",
    "    \n",
    "    need_lang = [need_lang]#['en', 'zh', 'vi']\n",
    "    \n",
    "    for activation_mask, mask_lang in zip(activation_masks, languages):\n",
    "    \n",
    "        if mask_lang not in need_lang:continue\n",
    "            \n",
    "        print(f'get mask =====lang:{mask_lang}=====')\n",
    "    \n",
    "        \n",
    "        if activation_mask:\n",
    "            def factory(mask):\n",
    "                def llama_forward_org(self, x):\n",
    "                    gate_up, _ = self.gate_up_proj(x)  # b, l, 2i\n",
    "                    i = gate_up.size(-1)\n",
    "                    activation = F.silu(gate_up[:, :, : i // 2])\n",
    "                    activation.index_fill_(2, mask, 0)\n",
    "                    x = activation * gate_up[:, :, i // 2 :]\n",
    "                    x, _ = self.down_proj(x)\n",
    "                    return x\n",
    "                def llama_forward(self, x):\n",
    "                    gate_ = self.gate_proj(x)  # b, l, 2i\n",
    "                    i = gate_.size(-1)\n",
    "                    activation = F.silu(gate_)\n",
    "                    activation.index_fill_(2, mask, 0)\n",
    "                    x = activation * self.up_proj(x)\n",
    "                    x = self.down_proj(x)\n",
    "                    return x\n",
    "    \n",
    "                def bloom_forward(self, x: torch.Tensor):\n",
    "                    x, _ = self.dense_h_to_4h(x)\n",
    "                    x = self.gelu_impl(x)\n",
    "                    x.index_fill_(2, mask, 0)\n",
    "                    x, _ = self.dense_4h_to_h(x)\n",
    "                    return x\n",
    "    \n",
    "                if is_llama:\n",
    "                    return llama_forward\n",
    "                else:\n",
    "                    return bloom_forward\n",
    "    \n",
    "            for i, layer_mask in enumerate(activation_mask):\n",
    "                if is_llama:\n",
    "                    obj = model.model.layers[i].mlp\n",
    "                else:\n",
    "                    obj = model.model.layers[i].mlp\n",
    "                obj.forward = MethodType(factory(layer_mask.to('cuda')), obj)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_mask_neuron_model_vllm(model, activation_mask_path, need_lang):\n",
    "\n",
    "    if activation_mask_path:\n",
    "        activation_masks = torch.load(activation_mask_path)\n",
    "    else:\n",
    "        activation_masks = [None]\n",
    "    \n",
    "    final_output = []\n",
    "    if is_llama:\n",
    "        languages = [\"en\", \"zh\", \"fr\", \"es\", \"vi\", \"id\", \"ja\"]\n",
    "    else:\n",
    "        languages = [\"en\", \"zh\", \"fr\", \"es\", \"vi\", \"id\"]\n",
    "    \n",
    "    need_lang = [need_lang]#['en', 'zh', 'vi']\n",
    "    \n",
    "    for activation_mask, mask_lang in zip(activation_masks, languages):\n",
    "    \n",
    "        if mask_lang not in need_lang:continue\n",
    "            \n",
    "        print(f'get mask =====lang:{mask_lang}=====')\n",
    "    \n",
    "        \n",
    "        if activation_mask:\n",
    "            def factory(mask):\n",
    "                def llama_forward(self, x):\n",
    "                    gate_up, _ = self.gate_up_proj(x)  # b, l, 2i\n",
    "                    i = gate_up.size(-1)\n",
    "                    activation = F.silu(gate_up[:, :, : i // 2])\n",
    "                    activation.index_fill_(2, mask, 0)\n",
    "                    x = activation * gate_up[:, :, i // 2 :]\n",
    "                    x, _ = self.down_proj(x)\n",
    "                    return x\n",
    "                def llama_forward_split(self, x):\n",
    "                    gate_ = self.gate_proj(x)  # b, l, 2i\n",
    "                    i = gate_.size(-1)\n",
    "                    activation = F.silu(gate_)\n",
    "                    #activation.index_fill_(2, mask, 0)\n",
    "\n",
    "                    # test\n",
    "                    print(activation.shape)\n",
    "                    activation.index_fill_(2, torch.tensor(list(range(activation.shape[1]))), 0 )\n",
    "                    x = activation * self.up_proj(x)\n",
    "                    x = self.down_proj(x)\n",
    "                    return x\n",
    "    \n",
    "                def bloom_forward(self, x: torch.Tensor):\n",
    "                    x, _ = self.dense_h_to_4h(x)\n",
    "                    x = self.gelu_impl(x)\n",
    "                    x.index_fill_(2, mask, 0)\n",
    "                    x, _ = self.dense_4h_to_h(x)\n",
    "                    return x\n",
    "    \n",
    "                if is_llama:\n",
    "                    return llama_forward\n",
    "                else:\n",
    "                    return bloom_forward\n",
    "    \n",
    "            for i, layer_mask in enumerate(activation_mask): \n",
    "                #print('ilayer:',i, layer_mask)\n",
    "                #if is_llama:\n",
    "                    # latest\n",
    "                    #obj = model.llm_engine.model_executor.driver_worker.model_runner.model.model.layers[i].mlp\n",
    "                #else:\n",
    "                    # latest\n",
    "                    #obj = model.llm_engine.model_executor.driver_worker.model_runner.model.transformer.h[i].mlp\n",
    "\n",
    "                if is_llama:\n",
    "                    obj = model.llm_engine.driver_worker.model_runner.model.model.layers[i].mlp\n",
    "                else:\n",
    "                    obj = model.llm_engine.driver_worker.model_runner.model.transformer.h[i].mlp\n",
    "                obj.forward = MethodType(factory(layer_mask.to('cuda')), obj)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "927e6db2-dfd8-49e8-84b4-2abddb0dd851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"VLLM_ENABLE_V1_MULTIPROCESSING\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7b433f8-38fa-4dd5-bb8d-b4c1a017c0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-10 15:18:02 llm_engine.py:70] Initializing an LLM engine with config: model='/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-hf', tokenizer='/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=True, seed=0)\n",
      "INFO 12-10 15:21:44 llm_engine.py:275] # GPU blocks: 1923, # CPU blocks: 512\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from types import MethodType\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "#model_path ='/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "model_path ='/root/autodl-fs/model_zoo/meta-llama/Llama-2-7b-hf'\n",
    "activation_mask_path='/root/autodl-fs/Language-Specific-Neurons/LLaMA-2-7B.neuron.pth'\n",
    "\n",
    "is_llama = bool(model_path.find('llama') >= 0)\n",
    "#model, tokenizer = load_model(model_path)\n",
    "\n",
    "# 初始化 vLLM\n",
    "llm =LLM(model=model_path, tensor_parallel_size=torch.cuda.device_count(), enforce_eager=True) #LLM(model=model_path, tensor_parallel_size=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a71dc3af-f9e4-4333-8e8f-a5daa70968b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "from vllm import SamplingParams\n",
    "\n",
    "\n",
    "LETTER_MAP = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "\n",
    "def extract_answer(text: str):\n",
    "    # Parse A/B/C/D\n",
    "    m = re.search(r'\\b([ABCD])\\b', text.upper())\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "def build_5shot_prompt(subject, train_samples, test_item):\n",
    "    # choose 5 examples\n",
    "    shots = random.sample(train_samples, 5)\n",
    "\n",
    "    prompt = f\"The following are multiple choice questions (with answers) about {subject}.\\n\\n\"\n",
    "    \n",
    "    for item in shots:\n",
    "        prompt += \"Q: \" + item[\"question\"] + \"\\n\"\n",
    "        for i, ch in enumerate(item[\"choices\"]):\n",
    "            prompt += f\"{LETTER_MAP[i]}. {ch}\\n\"\n",
    "        prompt += f\"Answer: {LETTER_MAP[item['answer']]}\\n\\n\"\n",
    "\n",
    "    # test question\n",
    "    prompt += \"Q: \" + test_item[\"question\"] + \"\\n\"\n",
    "    for i, ch in enumerate(test_item[\"choices\"]):\n",
    "        prompt += f\"{LETTER_MAP[i]}. {ch}\\n\"\n",
    "    prompt += \"Answer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def eval_mmlu_vllm_5shot(llm, max_new_tokens=4, verbose=True):\n",
    "    ds = load_dataset(\"cais/mmlu\", 'all')\n",
    "    train = ds[\"train\"]\n",
    "    test = ds[\"test\"]\n",
    "\n",
    "    # group by subject\n",
    "    train_groups = defaultdict(list)\n",
    "    test_groups = defaultdict(list)\n",
    "    \n",
    "    for item in train:\n",
    "        train_groups[item[\"subject\"]].append(item)\n",
    "    for item in test:\n",
    "        test_groups[item[\"subject\"]].append(item)\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=max_new_tokens,\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    subject_scores = {}\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    subjects = sorted(train_groups.keys())\n",
    "    if verbose:\n",
    "        pbar = tqdm(subjects, desc=\"Evaluating MMLU 5-shot\")\n",
    "\n",
    "    for subject in subjects:\n",
    "        train_samples = train_groups[subject]\n",
    "        test_samples = test_groups[subject]\n",
    "\n",
    "        prompts = []\n",
    "        gt = []\n",
    "\n",
    "        # build prompts\n",
    "        for item in test_samples:\n",
    "            prompts.append(build_5shot_prompt(subject, train_samples, item))\n",
    "            gt.append(LETTER_MAP[item[\"answer\"]])\n",
    "\n",
    "        # batch inference\n",
    "        outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "        correct = 0\n",
    "        for out, label in zip(outputs, gt):\n",
    "            text = out.outputs[0].text.strip()\n",
    "            pred = extract_answer(text)\n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "\n",
    "        acc = correct / len(gt)\n",
    "        subject_scores[subject] = acc\n",
    "\n",
    "        total_correct += correct\n",
    "        total_count += len(gt)\n",
    "\n",
    "        if verbose:\n",
    "            pbar.set_postfix({subject: f\"{acc:.3f}\"})\n",
    "            pbar.update(1)\n",
    "\n",
    "    overall = total_correct / total_count\n",
    "    return overall, subject_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96a805a4-56d1-4779-947a-73b2dd4972a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c03236-a3ba-4f5a-af52-272fd0951670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "568e2bd3-185b-4163-b892-930e58dccf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62393bcd54a14ba4922d0f6b16acab05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "all/test-00000-of-00001.parquet:   0%|          | 0.00/3.50M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee42442801c4d86815aaf6e647c0342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "all/validation-00000-of-00001.parquet:   0%|          | 0.00/408k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e447d781014b0683e399abce486e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "all/dev-00000-of-00001.parquet:   0%|          | 0.00/76.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a238a15be004a8da6ba9c390b3109af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "all/auxiliary_train-00000-of-00001.parqu(…):   0%|          | 0.00/47.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fe3115741947c8b8f901883f07d338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/14042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2aa6b90ec64d7996a02d2d384275a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1531 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f59b5f432e4364abb6f14bb63f5962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a167a10467d5494cadcd765c445da92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating auxiliary_train split:   0%|          | 0/99842 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"cais/mmlu\", 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "beb1dde0-e002-4e24-88d9-b4c219fa51d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/cais/mmlu/resolve/main/README.md (Caused by ProxyError('Cannot connect to proxy.', TimeoutError('_ssl.c:980: The handshake operation timed out')))\"), '(Request ID: bbc938b9-de82-4260-944b-8ed2be8edc9c)')' thrown while requesting HEAD https://huggingface.co/datasets/cais/mmlu/resolve/main/README.md\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m overall_acc, scores \u001b[38;5;241m=\u001b[39m \u001b[43meval_mmlu_vllm_5shot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall MMLU:\u001b[39m\u001b[38;5;124m\"\u001b[39m, overall_acc)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(scores)\n",
      "Cell \u001b[0;32mIn[24], line 40\u001b[0m, in \u001b[0;36meval_mmlu_vllm_5shot\u001b[0;34m(llm, max_new_tokens, verbose)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_mmlu_vllm_5shot\u001b[39m(llm, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     39\u001b[0m     ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcais/mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m     train \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     41\u001b[0m     test \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# group by subject\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/dataset_dict.py:86\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     89\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     90\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train'"
     ]
    }
   ],
   "source": [
    "overall_acc, scores = eval_mmlu_vllm_5shot(llm)\n",
    "print(\"Overall MMLU:\", overall_acc)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae44b63d-0dff-4a41-bee4-1b664e61bcd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
